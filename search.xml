<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql 使用]]></title>
    <url>%2F2021%2F03%2F17%2Fmysql%2F</url>
    <content type="text"><![CDATA[数据库操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 连接数据库mysql -h 地址 -P 端口 -u 用户名 -p 密码# 显示全部数据仓库show databases;# 创建数据仓库create database test_name;# 进入数据仓库use test_name;# 显示全部数据表show tables;SHOW TABLES FROM 表名# 创建数据表create table table_name (column_name column_type, column_name column_type, column_name column_type); # 复制表结构CREATE TABLE 表名 LIKE 要复制的表名# 表中插值insert into table_name values(&quot;学习 mysql&quot;, &quot;教程&quot;, &quot;今天&quot;);insert into table_name (titie,word) values(&quot;学习 mysql&quot;, &quot;教程&quot;,); #指定字段# 显示数据表select * from table_name;# 查找数据select col_name,col_name2 from table_name;select col_name,col_name2 from table_name where col_name3 &gt; 10; # where 增加选择条件select col_name from table_name where col_name3 REGEXP &apos;^st&apos;; #正则表达式 col_name3 以st开头select * from mytable where birthaddr=&apos;河北&apos; order by sex; # 排序# =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||,# in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, notselect * from mytable where sex=1 and birthaddr in (&apos;河北&apos;,&apos;河南&apos;)select sum(col_name) from table_name group by col_name2; #分组统计 count sum max min avgselect birthaddr,count(sex) as cc from mytable group by birthaddr having cc &gt;= 2; # group by having# 联表select a.id, a.author, b.count from runoob_tbl a inner join tcount_tbl b ON a.author = b.author; # 内接select a.id, a.author, b.count from runoob_tbl a left join tcount_tbl b ON a.author = b.author; # 左接select a.id, a.author, b.count from runoob_tbl a right join tcount_tbl b ON a.author = b.author; # 右接# 修改数据update table_name set addr=&quot;河北&quot; where col_name=&quot;小王&quot;; # 修改值alter table table_name rename to new_table_name; # 修改表名alter table table_name drop col_name; # 删除col_name字段alter table table_name add col_name_new VARCHAR(40); #最后一列添加字段alter table table_name add col_name_new VARCHAR(40) first; #第一列添加字段alter table table_name add col_name_new VARCHAR(40) after col_name; #在某一列后面添加字段#删除delete from table_name where runoob_id=3; #删除行数据drop table table_name; # 删除表truncate table_name; # 清空表drop database; #删除库 having 子句与 where 功能、用法相同，执行时机不同。where 在开始时执行检测数据，对原数据进行过滤。having 对筛选出的结果再次进行过滤。having 字段必须是查询出来的，where 字段必须是数据表存在的。 python 操作mysql1234567891011121314151617&gt;pip install mysqlclientimport MySQLdbconn= MySQLdb.connect(host='localhost',user='root',passwd='xxxxxxx', db ='test_db') # 连接数据库cur = conn.cursor()cur.execute("select * from mytable") # 所有的mysql在 cur.execute 中编写data = [ ('Jane', date(2005, 2, 12)), ('Joe', date(2006, 5, 23)), ('John', date(2010, 10, 3)),]stmt = "INSERT INTO employees (first_name, hire_date) VALUES (%s, %s)"cursor.executemany(stmt, data) #插入多条result = cur.fetchall() # 获得返回值，返回值是一个嵌套元祖conn.close() # 关闭连接]]></content>
      <categories>
        <category>database</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[缺失值&异常值处理]]></title>
    <url>%2F2021%2F03%2F15%2Fmissing-data%2F</url>
    <content type="text"><![CDATA[缺失值处理方法一： 删除含有缺失值的行数据方法二：填补，包含单特征填补和多特征填补。单特征填补比较简单，只需要考虑单列数据，一般是用其平均值，中位数，众数填补。123456789import numpy as npfrom sklearn.impute import SimpleImputerimp = SimpleImputer(missing_values=np.nan, strategy='mean')data = [[1, 2, 3], [np.nan, 3, 5], [7, 6, 9]]imp.fit(data)imp.transform(data)&gt;&gt;&gt; array([[1., 2., 3.], [4., 3., 5.], [7., 6., 9.]]) 随机森林填补123456789101112import numpy as npfrom sklearn.experimental import enable_iterative_imputerfrom sklearn.impute import IterativeImputerimp = IterativeImputer(max_iter=10, random_state=0)data = [[1, 2, 3], [3, 6, 7], [4, 8, 12], [np.nan, 3, 1], [7, np.nan, 10]]imp.fit(data)imp.transform(data)&gt;&gt;&gt; array([[ 1. , 2. , 3. ], [ 3. , 6. , 7. ], [ 4. , 8. , 12. ], [ 1.50207882, 3. , 1. ], [ 7. , 13.99140168, 10. ]]) R 包 mice 处理12345678library(mice)md.pattern(airquality) #缺失值可视化&gt; Wind Temp Month Day Solar.R Ozone 111 1 1 1 1 1 1 0 35 1 1 1 1 1 0 1 5 1 1 1 1 0 1 1 2 1 1 1 1 0 0 2 0 0 0 0 7 37 44 左边第一列，为不同缺失情况下的样本数右边第一列，为不同缺失情况下的缺失个数最下面一行，每个列有多少个缺失值12345imputed_Data &lt;- mice(airquality, m=5, maxit = 50, method = 'pmm', seed = 1024)stripplot(imputed_Data, col=c("grey",mdc(2)),pch=c(1,20)) #填充可视化xyplot(imputed_Data , Ozone ~ Solar.R | .imp, pch=20,cex=1.2) #分面板观察填充fit=with(imputed_Data,lm(Ozone ~ Wind + Solar.R + Temp))completeData &lt;- complete(imputed_Data,1) # 用第一种结果填充 异常值处理方法一：单列异常值处理，使用箱线图观察1234import matplotlib.pyplot as pltimport pandas ad pddata.boxplot()plt.show() 方法二：使用 PyOD 进行异常值检测（ML）123456from pyod.models.iforest import IForestimport pandas ad pdclf = IForest()clf.fit(data)clf.predict(data)&gt;&gt;&gt;array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) # 1不正常，0正常]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用 siuba 处理 DataFrame]]></title>
    <url>%2F2021%2F03%2F15%2Fsiuba%2F</url>
    <content type="text"><![CDATA[siuba 是一个语法上类似 dplyr 的 python包，能够用链式语法处理 pandas 的数据结构。1234567891011121314151617181920212223242526272829303132from siuba import *# 选择select("stack.loss","Water.Temp") # 引号取列select(_.bin) # _.取列（缺点：列名不能包含. _）select(-_.bin) # 负向取列select(_["mpg": "hp"]) # 范围取列select(_.contains('d')) # 包含取列 # 过滤filter(_["stack.loss"]&gt;20) # _[]取列filter(_.bin&gt;20)filter(_.hp &gt; _.hp.mean())filter((_["stack.loss"]&gt;20) | (_["stack.loss"]&lt;40)) # 或运算 需要把条件用括号包围filter((_["stack.loss"]&gt;20) &amp; (_["stack.loss"]&lt;40)) # 并预算filter(_["stack.loss"]&gt;20, _["stack.loss"]&lt;40) # 并预算,使用逗号可以不加括号# 新列mutate(hp=_["stack.loss"]-_["Water.Temp"])mutate(hp=_["stack.loss"].mean())# 分组group_by(_["Water.Temp"])group_by(_.cyl, _.gear)group_by(high_hp = _.hp &gt; 300) #定义分组新列 hp大小于300为分组条件# 总结summarize(count=_["Water.Temp"].count()) #一般配合group_by使用# 链式操作符&gt;&gt;file&gt;&gt;group_by(_["Water.Temp"])&gt;&gt;summarize(x=_["Water.Temp"].count())]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mongodb 数据库使用]]></title>
    <url>%2F2021%2F02%2F25%2Fmongodb%2F</url>
    <content type="text"><![CDATA[配置除了 compass（GUI工具） 安装在D盘，其他都默认安装路径。当前 mongodb 版本4.4.4 (2021-2-25) https://www.mongodb.com/download-center/community 安装社区版 mongodb server. https://www.mongodb.com/try/download/compass 安装 mongodb compass (可选). https://www.mongodb.com/try/download/database-tools 安装 mongodb tools. 将安装软件路径加入环境变量后，在 cmd 输入 mongo 即可使用。默认本地 host 路径：mongodb://127.0.0.1:27017/ 使用列出数据库&gt; show dbs创建数据库（在use database之后会自动进入此数据库）&gt; use test_database删除数据库&gt; db.dropDatabase()创建集合db.createCollection(&quot;test1&quot;)删除集合db.test1.drop()插入文档 (插入文档的格式为json)123456789101112131415161718db.test1.insertOne(&#123;title: "this is a test", description: 'MongoDB is a Nosql database', by: 'user', tags: ['user', 'database', 'NoSQL'], likes: 100 &#125;)db.test1.insertMany([&#123;title: "this is a test", description: 'MongoDB is a Nosql database', by: 'user', tags: ['user', 'database', 'NoSQL'], likes: 100 &#125;, &#123;title: "this is a test2", description: 'mysql is a sql database', by: 'user', tags: ['user', 'database', 'SQL'], likes: 99 &#125;]) 删除文档12db.test1.deleteOne(&#123;&quot;title&quot;:&quot;this is a test&quot;&#125;) #删除与过滤器匹配的第一个文档db.test1.deleteMany(&#123;&quot;title&quot;:&quot;this is a test&quot;&#125;) #删除与过滤器匹配的所有文档 更新文档1234567891011db.test1.updateOne(&#123;&quot;likes&quot;:100&#125;,[&#123;$set:&#123;&quot;by&quot;:&quot;seomeone&quot;&#125;&#125;]) # &#123;&quot;likes&quot;:100&#125;为查询条件，如更新范围为全部文档，则留空为&#123;&#125;,&#123;$set:&#123;&quot;by&quot;:&quot;seomeone&quot;&#125;意义为将by字段内容修改为seomeone，如果没有by字段不做修改db.test1.updateMany(&#123;&quot;likes&quot;:100&#125;,[&#123;$set:&#123;&quot;by&quot;:&quot;seomeone&quot;&#125;&#125;])db.test1.updateMany(&#123;&#125;, [&#123;$unset:&quot;by&quot;&#125;&#125;]) #删除by字段db.collection.updateMany( &lt;query&gt;, [ &#123; $set: &#123; status: &quot;Modified&quot;, comments: [ &quot;$misc1&quot;, &quot;$misc2&quot; ] &#125; &#125;, &#123; $unset: [ &quot;misc1&quot;, &quot;misc2&quot; ] &#125; ]) #官方格式 查询文档$lt &lt; $gt &gt; $lte &lt;= $gte &gt;= $ne !=123456789101112db.test1.findOne().pretty()db.test1.find().pretty()db.test1.find(&#123;&quot;likes&quot;:&#123;$lt:50&#125;&#125;)db.test1.find(&#123;title:/^教/&#125;) #正则db.test1.find(&lt;query&gt;, &#123;_id:0, title: 1, by: 1&#125;) # 后面可加输出字段，0不输出，1输出db.test1.find(&#123;&quot;by&quot;:&quot;someone&quot;, &quot;title&quot;:&quot;MongoDB&quot;&#125;) #and条件db.test1.find( &#123; $or: [ &#123;&quot;by&quot;:&quot;someone&quot;, &quot;title&quot;:&quot;MongoDB&quot;&#125; ] &#125; ) #or条件]]></content>
      <categories>
        <category>database</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线性回归]]></title>
    <url>%2F2020%2F12%2F09%2Fregression%2F</url>
    <content type="text"><![CDATA[一般线性回归模型（LM）我们在小学就学过了二元一次方程，其中 y 为因变量，x 为自变量。当 x 发生变化时，y 的值也会按照一定的规律发生改变。一般的我们认为其表达式为: y = β_{0} + β_{1}x在现实生活中我们有成对的（x，y），想构建一条直线能够让这些点均匀的分布在这条直线的周围。所以我们需要对 β 的值（参数）进行估计。统计世界中的公式会有一个小尾巴，用来代表误差，即： y = β_0+β_{1}x+μ损失函数有了成对的（x，y），我们可以任意地在散点间画一条直线，那么哪条直线才是最准确的呢？我们先从残差说起。残差说白了就是真实值和预测值间的差值（也可以理解为差距、距离），用公式表示是: e = y-\hat{y}我们把所有的残差先平方（消除负号），然后再都加起来得到一个值，用这个值来评判这条直线是否是最好的。残差平方和越接近0，那就说明真实值和预测值越相近。残差平方和公式: Q = \sum(y_{i}-\hat{y}_{i})^{2} = \sum(y_{i}-(\hat{β}_{0}+\hat{β}_{1}x_{i}))^{2}还记得微积分知识的话，就知道导数为0时，Q 取最小值，因此我们对两个 β 求偏导并令其为0: \frac{\partial Q}{\partial{β}_{0}} = 2\sum(y_{i}-\hat{β}_{0}-\hat{β}_{1}x_{i})=0\frac{\partial Q}{\partial{β}_{1}} = 2\sum(y_{i}-\hat{β}_{0}-\hat{β}_{1}x_{i})x_{i}=0将所有的（x，y）代入上述式子就得到了一个二元一次方程，求解就可以得到两个参数的值。这个方法叫做最小二乘法。 限制 一般线性模型要求因变量 y 要符合正态分布 误差要符合正态分布 方差齐 （x，y）线性相关 如果是多元线性回归，多个自变量之间需不相关 广义线性模型（GLM）由于一般线性模型的限制较多，无法适应现实中的所有情况，如分类型因变量（动植物是否抗病、药物是否有效），由此提出了广义线性模型。广义线性模型主要包含两种模型：逻辑回归模型和泊松分布模型，前者假设因变量为分类数据（二分类），后者假设因变量为计数类数据。广义线性模型对于一般线性模型来说，主要是多了一个连接函数，连接函数将因变量和线性预测器。 Lasso回归和岭回归我们在用实际数据进行分析时，可能会遇到以下两种问题 过拟合, overfitting 欠拟合, underfitting 而 Lasso 回归和 Ridge 回归是在损失函数中分别加入一个约束项，增加模型的泛化能力。岭回归损失函数如下（第二项为所有参数平方和，即L2范数）: L(x,y) = \sum(y_{i}-βx_{i})^{2}+λ\sumβ^{2}Lasso 回归损失函数如下（第二项为所有参数绝对值之和，即L1范数）: L(x,y) = \sum(y_{i}-βx_{i})^{2}+λ\sum|β|上述两个公式中第二项为添加的约束项(正则项)。 则 求解β 参数可以用下述表示： \hatβ = argmin||y−Xβ||^{2}_{2}+λ||β||^{2}_{2}\hatβ = argmin||y−Xβ||^{2}_{2}+λ||β||_{1}范数L1范数：表示向量x中非零元素的绝对值之和。 ||X||_{1} = \sum|x_{i}|L2范数：表示向量元素的平方和再开方。 ||X||_{2} = \sqrt{\sum x_{i}^{2}}线性混合模型（LMM）混合线性模型也被称作为随机效应模型（random effects model）,它假定要分析的数据来自不同总体的层次，这些总体的差异与该层次有关。其假设正好适用于动植物遗传进化研究，不同的个体很有可能具有亲缘关系。模型通式如下： y = Xβ+Zμ+ε可以看出，其比一般线性模型多了 Zμ 项。其中 β 称作固定效应，μ 称作随机效应，ε 为随机误差。若将上述表达式中的 Zμ+ε 作为一个整体的随机误差项来看，混合模型标准形式就变成传统的线性模型。在求解混合线性模型参数时，不再使用最小二乘法，而是使用最大似然估计（ML）或者约束最大似然估计（REML）。 混合模型方程（MME) 如下: \begin{bmatrix} X^{T}R^{-1}X & X^{T}R^{-1}Z \\ Z^{T}R^{-1}X & Z^{T}R^{-1}Z+G^{-1} \end{bmatrix} \begin{bmatrix}β\\μ\end{bmatrix} = \begin{bmatrix}X^{T}R^{-1}Y\\Z^{T}R^{-1}Y\end{bmatrix}其中： R = Iσ^{2}_{e},G = Iσ^{2}_{μ}混合线性模型依然假设因变量为正态分布，所以无法拟合分类型变量。 模型GWAS应用以下代码均在R语言环境下运行。 LM12345678library(data.table)geno = fread("c.raw")phe = fread("phe.txt")pca = fread("pca.txt")plink = fread("pca_cov.txt",header=F,sep=" ")dd = data.frame(phe = phe$V3,pca1 = pca$V1,pca2 = pca$V1,geno)mymodel = lm(phe ~ pca1+pca2 + X(单个位点名字),data=dd) #pca1+pca2作为模型协变量summary(mymodel) GLM(Logistic回归)12345library(data.table)geno = fread("c.raw",)phe = fread("phe.txt") #表型为 0-1分类dd = data.frame(phe$V3,geno)mymodel = glm(phe$V3 ~ X(单个位点名字) ,family = "binomial",data=dd ) MLM123456789101112131415M &lt;- matrix(rep(0,200*1000),1000,200)for (i in 1:200) &#123;M[,i] &lt;- ifelse(runif(1000)&lt;0.5,-1,1)&#125;colnames(M) &lt;- 1:200geno &lt;- data.frame(marker=1:1000,chrom=rep(1,1000),pos=1:1000,M,check.names=FALSE)QTL &lt;- 100*(1:5) #pick 5 QTLu &lt;- rep(0,1000) #marker effectsu[QTL] &lt;- 1g &lt;- as.vector(crossprod(M,u))h2 &lt;- 0.5y &lt;- g + rnorm(200,mean=0,sd=sqrt((1-h2)/h2*var(g)))pheno &lt;- data.frame(line=1:200,y=y) #表型后几列可以增加固定效应（性别，种群结构）kk &lt;-A.mat(geno)scores &lt;- GWAS(pheno,geno,plot=TRUE,K=kk) #结果第四列为log p值]]></content>
      <categories>
        <category>statistics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[群体遗传学统计量计算]]></title>
    <url>%2F2020%2F09%2F28%2Fpopulation%2F</url>
    <content type="text"><![CDATA[Fst &amp; dxy &amp; pi(single)123python popgenWindows.py -w 100000 -m 50 -s 20000 -g input.geno.gz \-o fpd.csv.gz -f phased -T 6 -p popA A1,A2,A3,A4 -p popB B1,B2,B3,B4# -w :窗口大小 -m :窗口内最少snp数 -s :步长 -T :进程数 -p :群体名和群体样本名 ABBA_BABA1python ABBABABAwindows.py -g input.geno.gz -f phased -P1 -P2 -P3 -O --popsFile pop.txt -w 100000 -m 50 -s 20000 -T 6 -o abba_baba.csv Tajima’D(single)1vcftools --vcf test.vcf --TajimaD 100000 --out td]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用 treemix 进行基因渗入分析 & 使用 hierfstat 计算成对 Fst]]></title>
    <url>%2F2020%2F09%2F07%2Ftreemix%2F</url>
    <content type="text"><![CDATA[treemix 是利用等位基因频率来推断群体间分化和杂合（基因流动或者基因渗入）的软件。 格式转换通常，我们的变异数据储存格式为vcf,首先将其转换为tped格式。1vcftools --vcf test.vcf --plink-tped --out test 转换后的tfam文件如下：第一列为FID,第二列为IID，我们需要修改第一列数据为群体信息。12345wHAIPI052555-68 wHAIPI052555-68 0 0 0 -9wHAIPI052556-77 wHAIPI052556-77 0 0 0 -9wHAIPI052557-76 wHAIPI052557-76 0 0 0 -9wHAIPI052558-63 wHAIPI052558-63 0 0 0 -9wHAIPI052559-71 wHAIPI052559-71 0 0 0 -9 修改好的tfam文件如下：XM和GD是2个群体12345XM wHAIPI052555-68 0 0 0 -9XM wHAIPI052556-77 0 0 0 -9XM wHAIPI052557-76 0 0 0 -9GD wHAIPI052558-63 0 0 0 -9GD wHAIPI052559-71 0 0 0 -9 然后再编辑一个群体信息文件(pop.cov)，格式如下：12345XM wHAIPI052555-68 XMXM wHAIPI052556-77 XMXM wHAIPI052557-76 XMGD wHAIPI052558-63 GDGD wHAIPI052559-71 GD 编辑好之后，计算等位基因组频率：1plink --tfile test --freq --within pop.cov 然后使用treemix 自带的脚本 plink2treemix.py进行格式转换，转换前需要对等位基因频率文件压缩。1python2 plink2treemix.py plink.frq.strat.gz treemix_in.gz 基因渗入分析1treemix -se -bootstrap -k 1000 -m 1 -i treemix_in.gz 或者进行多个m和重复：123456789101112for m in &#123;1..5&#125; do for i in &#123;1..20&#125; do treemix -se -bootstrap \ -i treemix_in.gz \ -o TreeMix.$&#123;i&#125;.$&#123;m&#125; \ -global \ -m $&#123;m&#125; \ -k 1000 done done 其中m参数为 the number of migration edges，中文翻译过来其实就是基因渗入方向的个数，默认为0，可以尝试1-10，并在每个m中重复10次。 最佳迁移边缘个数选择将生成的llik、cov、modelcov 文件放置同一文件夹，使用 R 包OptM进行分析：123library(OptM)linear = optM(&quot;./result&quot;)plot_optM(linear) 生成图中，当Δm值最小时的 migration edges 为最佳迁移边缘个数。 基因渗入作图确定最佳迁移边缘个数之后，我们使用 m=最佳迁移边缘个数的结果文件作图。12source("plotting_funcs.R") #treemix scr文件夹中R脚本plot_tree("TreeMix") #TreeMix为结果文件前缀 pairwise Fst1234567891011121314library(vcfR)library(adegenet)library(hierfstat)kiwipang&lt;-read.vcfR("my.vcf") #读取vcfdf&lt;-vcfR2genind(kiwipang) #vcf2genindploidy(df)&lt;-2 #倍型为2pop.Kiwipang&lt;-read.table("pop.txt",sep="\t",header=F) #读取群体结构文件，第一列为样本名，第二列为群体名，顺序与vcf文件一致all(colnames(kiwipang@gt)[-1] == pop.Kiwipang$V1) #检查文件名pop(df)&lt;-pop.Kiwipang$V2 #指定群体df$pop #查看群体mydf&lt;-genind2hierfstat(df) #genind2hierfstatpopFst1&lt;-genet.dist(mydf,diploid=F) #计算pairwise FstpopFst&lt;-as.matrix(popFst1) #转为矩阵pheatmap::pheatmap(popFst,cluster_rows = F,cluster_cols = F) #作图]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[全基因组比对策略]]></title>
    <url>%2F2020%2F08%2F31%2Fgenome-alignment%2F</url>
    <content type="text"><![CDATA[两个基因组的比对主要在于大片段序列的比对正确性，这和小片段比对（blast）、多序列比对（mafft）不同。基因组层面的比对方法常用的有 lastz 和 mummer。 lastz 流程axt文件的处理在使用 lastz 比对完两个基因组之后，生成了以 axt 结尾的文件（比对时可以分割染色体）。axt 文件即记录了2个基因组的比对位置和对应比对的序列，如下：其中 chr21 19645 19675 Chr01 318732 318762是比对位置。1231 chr21 19645 19675 Chr01 318732 318762 + 2328CACACAGACATACACACATGCACACACACGCCACACAAACACACACACACACACACACACAC 我们需要把这一行的信息挑选出来。1grep chr xxx.axt |cut -f 2,3,4,5,6,7 -d &quot; &quot; &gt; xxx.pos 得到比对位置之后，我们下一步对结果进行过滤。 片段长度过滤太短的比对无法真实地体现比对位置的正确性，一般比对长度要大于 1000bp1&gt;&gt;&gt;awk &apos;$3-$2&gt;1000&#123;print $0&#125;&apos; xxx.pos &gt;xxx.filter.pos 位置信息过滤理想情况下，应该是一个基因组的染色体比对到另一个基因组的染色体上，而实际情况会有很多碎片散落在其他染色体上，我们挑选出来的位置文件包含了一条染色体比对到另一个基因组所有染色体的信息，所以我们要进行过滤，看看有没有一一对应比对的染色体位置信息。1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; head -5 xxx.filter.pos chr11 1016468 1018558 Chr01 1589564 1591669 chr11 2208239 2209250 Chr01 3172963 3173855 chr11 4940937 4942025 Chr01 6355782 6356928 chr11 6016505 6017738 Chr01 8231646 8232851 chr11 6770249 6771299 Chr01 10322020 10323107&gt;&gt;&gt; cut -f 4 xxx.filter.pos -d &quot; &quot;|uniq -c 142 Chr01 275 Chr02 318 Chr03 141 Chr04 216 Chr05 227 Chr06 167 Chr07 313 Chr08 245 Chr09 212 Chr10 273 Chr11 223 Chr12 193 Chr13 236 Chr14 303 Chr15 170 Chr16 304 Chr17 5130 Chr18 240 Chr19 333 Chr20 209 Chr21 289 Chr22 141 Chr23 142 Chr24&gt;&gt;&gt; grep Chr18 xxx.filter.pos &gt; xxx.result 可以看到 chr11 的绝大部分序列比对到了 Chr18（5130），我们由此得知 chr11-Chr18是一对儿比较好的共线性染色体。 作图我们使用LINKVIEW 进行比对的可视化，输入文件就是上面的 xxx.result。1python LINKVIEW.py xxx.result -o xxx # https://github.com/YangJianshun/LINKVIEW mummer 流程12345678910111213141516171819202122232425262728&gt;&gt;&gt; nucmer genome1.fa genome2.fa&gt;&gt;&gt; delta-filter -i 89 -l 1000 -1 out.delta &gt; out.delta.filter #比对率大于89%，对比长度大于1000&gt;&gt;&gt; show-coords out.delta.filter &gt; out.coords&gt;&gt;&gt; grep -P &quot;chr1\s+&quot; out.delta.coord|awk &apos;&#123;print $12,$13&#125;&apos; |sort |uniq -c 6 chr1 Chr01 6 chr1 Chr02 7 chr1 Chr03 12 chr1 Chr04 6688 chr1 Chr05 4 chr1 Chr06 12 chr1 Chr07 9 chr1 Chr08 4 chr1 Chr09 17 chr1 Chr10 8 chr1 Chr11 4 chr1 Chr12 12 chr1 Chr13 7 chr1 Chr14 23 chr1 Chr15 8 chr1 Chr16 8 chr1 Chr17 7 chr1 Chr18 11 chr1 Chr19 10 chr1 Chr20 4 chr1 Chr21 7 chr1 Chr22 3 chr1 Chr23 6 chr1 Chr24 可以看到 chr1 的绝大部分序列比对到了 Chr05（6688），我们由此得知 chr1-Chr05是一对儿比较好的共线性染色体。 作图我们使用LINKVIEW 进行比对的可视化。123grep -P "chr1\s+Chr05" out.delta.coord|awk '&#123;print $12,$1,$2,$13,$4,$5&#125;' &gt;chr1_chr5.possed -i 's/ /\t/g' chr1_chr5.pospython LINKVIEW.py chr1_chr5.pos -o xxx # https://github.com/YangJianshun/LINKVIEW]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据填补]]></title>
    <url>%2F2020%2F08%2F07%2Fimputation%2F</url>
    <content type="text"><![CDATA[在数据收集的时候，不可避免的会有些样本的表型（特征）会丢失或者无法测量，如果想保证数据表的完整就要涉及数据的填补。对于连续型数据来说，取一列数据的平均值作为填补是最简单的，但是很容易造成数据方差有偏，无法还原真实数据情况。对于分类型舒俱来说，取一列中的众数作为填补是最简单的，但是无法还原真实数据情况。此时，使用机器学习方法对缺失数据进行预测是一种比较好的方法。最常用的方法包括 KNN（K-邻近法）,RF（随机森林）。两种方法的学习不含缺失值的列（回归）来预测缺失列进行填补。在实际测试的时候，有文献说明使用随机森林方法填补缺失值更加准确，随之而来的是数据量变大，程序运行时间增加的更多。 R 包 missForest 的使用1234data(iris)iris.mis &lt;- prodNA(iris, noNA = 0.2) #随机删除数据20%iris.imp &lt;- missForest(iris.mis, xtrue = iris, verbose = TRUE) #填补iris.imp$OOBerror #准确度（NRMSE是连续性数据指标，PFC是分类型数据指标） R 包 missMDA 的使用1234567891011121314data(orange)nb &lt;- estim_ncpPCA(orange,ncp.max=5) #估计维数res.comp &lt;- imputePCA(orange,ncp=nb) #填补连续数据res.comp$completeObs #结果data(vnf)nb &lt;- estim_ncpMCA(vnf,ncp.max=5)res.impute &lt;- imputeMCA(vnf, ncp=nb)#填补分类数据res.impute$completeObs #结果data(ozone)nb &lt;- estim_ncpFAMD(ozone)res.impute &lt;- imputeFAMD(ozone, ncp=nb)#填补混合数据res.impute$completeObs #结果 基因型填补基因型填补相对于普通数据填补要困难，常用的软件有 begle(HMM) 和 linkimputeR（KNN）。12345678910111213141516171819202122232425262728293031java -jar begle.jar gt=test.vcf ne=1000 out=imputation.vcf # ne 有效种群大小&gt;&gt;&gt;cat accuracy.ini[Input]filename = test.vcfsave = filtered.vcf[InputFilters]maf=0.05positionmissing = 0.8exacthw=0.01[Global]depth = 2[Stats]root = ./level = tableeachmasked = yes[Output]control = ./impute.xml[Log]file = log.txtlevel = debug[Accuracy]numbermasked = 1000&gt;&gt;&gt;java -jar LinkImputeR.jar -s accuracy.ini&gt;&gt;&gt;java -jar LinkImputeR.jar impute.xml &apos;Case 1&apos; result.vcf]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python进程池使用]]></title>
    <url>%2F2020%2F07%2F23%2Fpython-Process%2F</url>
    <content type="text"><![CDATA[python由于有全局锁的原因，多线程无法调用多核CPU。所以在计算密集型的程序中使用多进程是比较合适的策略，而进程池可以很方便的构建程序进程管理。 apply_async使用示例123456789101112131415161718192021222324252627import timefrom multiprocessing import Pooldef task(x_y): (x,y)=x_y res = [] for i in range(x): for j in range(y): if i*j%99980000==1: res.append(i) else: pass return resif "__main__" == __name__: start = time.time() results = [] x = [10000,10000,10000,10000] y= [10000,10000,10000,10000] x_y = list(zip(x,y)) pool = Pool(4) for i in x_y: result = pool.apply_async(task, (i, )) results.append(result) for i in results: print(i.get()) end = time.time() t = end - start print(t) #12.45 CPU调用图： 要注意的点： pool = Pool(4) 默认Pool()会调用所有的CPU pool.apply_async 依次传入参数为 1 调用函数，2 参数（必须为元祖形式传入） i.get() 应该在进程结束后再使用 get 获得返回值 x_y = list(zip(x,y))如果函数参数不止一个，应该使用这样的方式把参数合并。 map_async使用示例1234567891011121314151617181920212223import timefrom multiprocessing import Pooldef task(x_y): (x,y)=x_y res = [] for i in range(x): for j in range(y): if i*j%99980000==1: res.append(i) else: pass return resif "__main__" == __name__: start = time.time() x = [10000,10000,10000,10000] y= [10000,10000,10000,10000] x_y = list(zip(x,y)) pool = Pool(4) # 创建进程池对象 result = pool.map_async(task, x_y) print(result.get()) # 进程函数返回值 end = time.time() t = end - start print(t) #12.34 要注意的点： pool.map_async(task, x_y)和 pool.apply_async 传入参数一致，但是不用循环，而是直接传入整体参数x_y 普通方式实现123456789101112131415161718192021import timedef task(x_y): (x,y)=x_y res = [] for i in range(x): for j in range(y): if i*j%99980000==1: res.append(i) else: pass return resif "__main__" == __name__: start = time.time() x = [10000,10000,10000,10000] y= [10000,10000,10000,10000] x_y = list(zip(x,y)) for i in x_y: print(task(i)) end = time.time() t = end - start print(t) #47.97]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[annovar 对 vcf 文件注释]]></title>
    <url>%2F2020%2F05%2F20%2Fannovar%2F</url>
    <content type="text"><![CDATA[123456789101112#gff文件格式转换（文件第一行需要保留 ##gff-version 3）./gff3ToGenePred fish.gff fish_refGene.txt# 生成转录组信息文件perl retrieve_seq_from_fasta.pl --format refGene --seqfile fish.fa fish_refGene.txt -outfile fish_refGeneMrna.fafish_refGene.txt 和 fish_refGeneMrna.fa 这2个文件需要放在同一个目录下，如（fish）#vcf文件格式转换perl convert2annovar.pl -format vcf4old test.vcf -out test.input#开始注释perl annotate_variation.pl -geneanno -dbtype refGene -out result --buildver fish ./test.input fish/]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用vcf文件构建进化树]]></title>
    <url>%2F2020%2F03%2F02%2Fvcf2tree%2F</url>
    <content type="text"><![CDATA[首先，过滤 vcf 文件 12345671.vcftools --vcf final.vcf --min-alleles 2.0 --max-alleles 2.0 --max-missing 0.95 --non-ref-af 0.05 --max-non-ref-af 0.95 --recode -c &gt;filter.vcfjava -jar SnpSift.jar filter "(countHom() &gt; X) &amp; (QUAL &gt;= 30 )" filter.vcf &gt; filtered.vcf #（可选）X可以指定数字，如10，100 意义为纯合变异的样本数2.vcftools --vcf filter.vcf --plink --out filter #转plink ped格式plink --file filter --recode transpose 12 --out filtered #转plink tped格式 然后，构建距离矩阵 1234567891.VCF2Dis -InPut filtered.vcf -OutPut p_dis.mat2.emmax-kin filtered -h -s #-s 构建IBS 亲缘矩阵2.1python hIBS2PHYLIP.py filtered.hIBS.kinf2.2cat filtered.hIBS.kinf|awk -v OFS='\t' '&#123;for(i=1;i&lt;=NF;i++)&#123;$i=sqrt(($i-1)^2)&#125;&#125;&#123;print $0&#125;' &gt;PHYLIP_need.txtcut -f1 -d ' ' filtered.tfam |paste - PHYLIP_need.txt &gt;p_dis.mat #最后在结果第一行加样本数 最后，使用PHYLIPNEW构建进化树 12PHYLIPNEW-3.69.650/bin/fneighbor -datafile p_dis.mat -outfile tree.out.txt -matrixtype s -treetype n -outtreefile tree.out.tre#-treetype n 指构建Neighbor-joining树，可选u 指构建UPGMA树 -outgrno 参数可指定外群的数量 hIBS2PHYLIP.py 12345678910import sysimport numpy as npif (len(sys.argv)==2): with open(sys.argv[1],'r') as IN, open("PHYLIP_need.txt",'w') as OUT: f_np = np.loadtxt(IN,delimiter='\t') result = abs((f_np-1)) #减一取绝对值 np.savetxt(OUT,result,fmt="%.6f",delimiter="\t") print("Done!")else: print("Usage: python hIBS2PHYLIP.py test.hIBS.kinf")]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用 NGSadmix 群体结构分析]]></title>
    <url>%2F2019%2F12%2F19%2FNGSadmix%2F</url>
    <content type="text"><![CDATA[第一步： 将 vcf 文件格式转化为 BEAGLE 格式，要看下自己的 vcf 文件有 GL 字段还是 PL 字段。 12vcftools --vcf input.vcf --out test --BEAGLE-GL --chr 1,2,3,4,5,6vcftools --vcf input.vcf --out test --BEAGLE-PL --chr 1,2,3,4,5,6 第二步： 运行 NGSadmix 生成结果中后缀为 qpot的则是作图所需文件。 12./NGSadmix -likes test.BEAGLE.PL -K 3 -P 4 -o myout -minMaf 0.05#-K 群体数 -P 计算机核心数 第三步： 作图 12data&lt;-read.table("myout.qopt")barplot(t(as.matrix(data)),col=rainbow(3),axes=FALSE,ylab="K=3",font=2,cex.lab=1.4) 第四步： 评估结果 evalAdmix 会生成一个样本残差矩阵，对角线为NA。如果数据与混合模型非常吻合，则其他位置的相关性结果将为 0。 1./evalAdmix -beagle test.BEAGLE.PL -fname myout.fopt.gz -qname myout.qopt -P 4 12345source("NicePlotCorRes.R")pop&lt;-read.table("input.fam")r&lt;-as.matrix(read.table("output.corres.txt"))plotCorRes(cor_mat = r, pop = as.vector(pop[,2]), title="Evaluation of admixture proportions with K=3", max_z=0.1, min_z=-0.1)]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PCA]]></title>
    <url>%2F2019%2F11%2F19%2Fpca%2F</url>
    <content type="text"><![CDATA[PCA实现简单地说，PCA的过程就是求协方差矩阵特征向量的过程。 下面是教程中很普遍的一个例子： 123456789101112131415161718192021222324import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.decomposition import PCAimport matplotlib.pyplot as pltdata = load_iris() #加载鸢尾花数据X = data.data #X为花的各个特征Y = data.target #Y为花的分类标签newData=X-np.mean(X, axis=0) #数据去中心化covMat=np.cov(newData,rowvar=0) #求协方差矩阵featValue,featVec=np.linalg.eig(covMat) #求特征值和特征向量index=np.argsort(featValue) #特征值排序n_index=index[-1:-3:-1] #选择特征值最大的2个数的标签n_featVec=featVec[:, n_index] #选择2个特征值对应特征向量lowData=np.dot(newData,n_featVec) #计算最后结果#highData=np.dot(lowData,n_featVec.T)+meanvalplt.figure(figsize=(8,4))plt.subplot(121)plt.title("my_PCA")plt.scatter(lowData[:, 0], lowData[:, 1], c = Y) ![](https://img.imgdb.cn/item/604f03095aedab222c817a67.png) 群体结构的PCA方法一： 1plink --bfile myfile --pca 10 --out myfile_pca 方法二： 12gcta64 --bfile ./myfile --make-grm --autosome --autosome-num 24 --out ./myfile_grmgcta64 --grm ./myfile_grm_grm --pca 10 --out ./all.genotype_pca 方法三： 1flashpca_x86-64 --bfile myfile 方法四： 1234567891011121314library(vcfR)library(adegenet)library(adegraphics)library(pegas)library(lattice)library(ape)vcf &lt;- read.vcfR("test.vcf.gz")aa.genlight &lt;- vcfR2genlight(vcf, n.cores=1)pca_result &lt;- glPca(aa.genlight, nf=3, n.cores=1) pca_result$eig[1]/sum(pca_result$eig) # pc1 解释的比例pca_result$eig[2]/sum(pca_result$eig) # pc2 解释的比例pca_result$eig[3]/sum(pca_result$eig) # pc3 解释的比例pca_score &lt;- as.data.frame(pca_result$scores) #作图数据 确定显著PCA数量1twstats -t twtable -i pca.eigenval -o eigenvaltw.out 在结果文件查看 p-value列，如果值小于0.05则表示选择这个主成分。]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python 的引用问题]]></title>
    <url>%2F2019%2F11%2F08%2Fpython-Reference%2F</url>
    <content type="text"><![CDATA[先看错误示范： 123456789person = &#123;'name': '', 'ids': 0&#125;team = []for i in range(3): x = person x['ids'] = i team.append(x)print(team)&gt;&gt;&gt;[&#123;'name': '', 'ids': 2&#125;, &#123;'name': '', 'ids': 2&#125;, &#123;'name': '', 'ids': 2&#125;] 可能一眼看不出结果为什么不是自己相象的那样，我们在中间穿插一些print来 debug 下。 12345678910111213141516person = &#123;'name': '', 'ids': 0&#125;team = []for i in range(3): x = person x['ids'] = i print(id(x),x) team.append(x)print(team)&gt;&gt;&gt;33373712 &#123;'name': '', 'ids': 0&#125;33373712 &#123;'name': '', 'ids': 1&#125;33373712 &#123;'name': '', 'ids': 2&#125;[&#123;'name': '', 'ids': 2&#125;, &#123;'name': '', 'ids': 2&#125;, &#123;'name': '', 'ids': 2&#125;] 我们可以看到 x 的 id 在循环中并没有发生变化，但是 x 的值却在循环中因为x[&#39;ids&#39;] = i的赋值发生了变化。这样导致的结果便是 team 列表中3 个 append x 的元素都会变成循环中最后一次的赋值。究其原因就是在循环中 x 一直在引用 person的值。 解决方法1： 1234567891011121314151617181920from copy import deepcopyperson = &#123;'name': '', 'ids': 0&#125;team = []for i in range(3): x = deepcopy(person) #使用deepcopy,改变x的id x['ids'] = i print(id(x),x) team.append(x)print(team)&gt;&gt;&gt;45083456 &#123;'name': '', 'ids': 0&#125;42706336 &#123;'name': '', 'ids': 1&#125;45195840 &#123;'name': '', 'ids': 2&#125;[&#123;'name': '', 'ids': 0&#125;, &#123;'name': '', 'ids': 1&#125;, &#123;'name': '', 'ids': 2&#125;] 解决方法2： 1234567891011121314team = []for i in range(3): person = &#123;'name': '', 'ids': 0&#125; #直接定义变量（不引用还不行?） person['ids']=i print(id(person),person) team.append(person)print(team)&gt;&gt;&gt;37436944 &#123;'name': '', 'ids': 0&#125;37437016 &#123;'name': '', 'ids': 1&#125;37474376 &#123;'name': '', 'ids': 2&#125;[&#123;'name': '', 'ids': 0&#125;, &#123;'name': '', 'ids': 1&#125;, &#123;'name': '', 'ids': 2&#125;]]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ggplot2 theme 详细]]></title>
    <url>%2F2019%2F11%2F04%2Fggplot2-theme%2F</url>
    <content type="text"><![CDATA[在 ggplot2 绘图中，theme() 主要用来自定义图的非数据组成部分（标题、标签、字体、字号、背景、图例、轴等）。在完成绘图的主要部分后，这些调整可以图片使更加美观和符合作图要求。 方法常用可调整对象如下： 参数 意义 参数 意义 axis.title x 轴和 y 轴的标题 axis.line x 轴和 y 轴 axis.title.x x 轴的标题 axis.line.x x 轴 axis.title.y y 轴的标题 axis.line.y y 轴 axis.text x 轴和 y 轴的文本（标尺） legend.position 图例位置 axis.text.x x 轴的文本（标尺） legend.justification 图例位置 axis.text.y y 轴的文本（标尺） legend.key.size 图例大小 axis.ticks x 轴和 y 轴的文本标记 legend.text 图例文本（标尺） axis.ticks.x x 轴的文本标记 legend.title 图例标题 axis.ticks.y y 轴的文本标记 panel.background 绘图面板 axis.ticks.length x 轴和 y 轴文本标记的长度 panel.border 绘图边界 axis.ticks.length.x x 轴文本标记的长度 panel.grid.major 绘图网格 axis.ticks.length.y y 轴文本标记的长度 panel.background 绘图面板 plot.title 图片标题 plot.subtitle 图片副标题 plot.background 图片背景 可调整参数如下： element_blank :取消绘制当前内容（为空） element_rect :调整图片的背景和边框 element_line :调整线条的颜色，粗细，类型 element_text :调整文本的颜色，大小，角度 可调整的大小格式： unit : unit 可设置具体大小,如 unit(1,’cm’) 就是一厘米，unit(1,’mm’) 就是一毫米 rel : rel 是一个相对的调整方法， rel(2) 表示原来的 2 倍大小 示例123library(ggplot2)data&lt;-carsp1&lt;-ggplot(data = data,aes(x=data$speed,y=data$dist))+geom_point() 123library(ggplot2)data&lt;-carsp1+theme(axis.title=element_text(size=rel(2),colour='red')) #更改 x y 轴标题颜色大小 123library(ggplot2)data&lt;-carsp1+theme(panel.background=element_blank()) #取消绘图面板背景]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[统计学常识]]></title>
    <url>%2F2019%2F07%2F10%2Ftongji%2F</url>
    <content type="text"><![CDATA[二项分布二项分布是重复n次的伯努利试验。概率分布： p(x)=C_{n}^{k}p^{k}q^{n-k},(k=0,1,2,3⋅⋅⋅,k)均值： np方差： npq泊松分布当二项分布的 n 很大，但是 p 非常小的时候其分布近似泊松分布。概率分布： p(x=k)=\frac{λ^{k}}{k!}e^{-λ},(k=0,1,2,3⋅⋅⋅,k)均值、方差： λ均匀分布概率分布： f(x)=\frac{1}{b-a},(a]]></content>
      <categories>
        <category>statistics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习实例]]></title>
    <url>%2F2019%2F06%2F26%2FML%2F</url>
    <content type="text"><![CDATA[Scikit-LearnScikit-learn 是开源的 Python 机器学习库，提供了数据预处理、交叉验证、算法与可视化算法等一系列接口。 Basic Example: 基本用例12345678910111213&gt;&gt;&gt; from sklearn import neighbors, datasets, preprocessing&gt;&gt;&gt; from sklearn.cross_validation import train_test_split&gt;&gt;&gt; from sklearn.metrics import accuracy_score&gt;&gt;&gt; iris = datasets.load_iris()&gt;&gt;&gt; X, y = iris.data[:, :2], iris.target&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33)&gt;&gt;&gt; scaler = preprocessing.StandardScaler().fit(X_train)&gt;&gt;&gt; X_train = scaler.transform(X_train)&gt;&gt;&gt; X_test = scaler.transform(X_test)&gt;&gt;&gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5)&gt;&gt;&gt; knn.fit(X_train, y_train)&gt;&gt;&gt; y_pred = knn.predict(X_test)&gt;&gt;&gt; accuracy_score(y_test, y_pred) 数据加载与切分我们一般使用 NumPy 中的数组或者 Pandas 中的 DataFrame 等数据结构来存放数据： 1234&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; X = np.random.random((10,5))&gt;&gt;&gt; y = np.array(['M','M','F','F','M','F','M','M','F','F','F'])&gt;&gt;&gt; X[X &lt; 0.7] = 0 NumPy 还提供了方便的接口帮我们划分训练数据与测试数据： 123&gt;&gt;&gt; from sklearn.cross_validation import train_test_split&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Model: 模型模型创建监督学习Linear Regression12&gt;&gt;&gt; from sklearn.linear_model import LinearRegression&gt;&gt;&gt; lr = LinearRegression(normalize=True) Support Vector Machines12&gt;&gt;&gt; from sklearn.svm import SVC&gt;&gt;&gt; svc = SVC(kernel='linear') Naive Bayes12&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB&gt;&gt;&gt; gnb = GaussianNB() KNN12&gt;&gt;&gt; from sklearn import neighbors&gt;&gt;&gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5) 无监督学习Principal Component Analysis12&gt;&gt;&gt; from sklearn.decomposition import PCA&gt;&gt;&gt; pca = PCA(n_components=0.95) KMeans12&gt;&gt;&gt; from sklearn.cluster import KMeans&gt;&gt;&gt; k_means = KMeans(n_clusters=3, random_state=0) 模型拟合有监督学习123&gt;&gt;&gt; lr.fit(X, y)&gt;&gt;&gt; knn.fit(X_train, y_train)&gt;&gt;&gt; svc.fit(X_train, y_train) 无监督学习12&gt;&gt;&gt; k_means.fit(X_train)&gt;&gt;&gt; pca_model = pca.fit_transform(X_train) 模型预测有监督预测123&gt;&gt;&gt; y_pred = svc.predict(np.random.random((2,5)))&gt;&gt;&gt; y_pred = lr.predict(X_test)&gt;&gt;&gt; y_pred = knn.predict_proba(X_test) 无监督预测1&gt;&gt;&gt; y_pred = k_means.predict(X_test) 模型评估分类度量Accuracy Scope123&gt;&gt;&gt; knn.score(X_test, y_test)&gt;&gt;&gt; from sklearn.metrics import accuracy_score&gt;&gt;&gt; accuracy_score(y_test, y_pred) Classification Report12&gt;&gt;&gt; from sklearn.metrics import classification_report &gt;&gt;&gt; print(classification_report(y_test, y_pred)) Confusion Matrix12&gt;&gt;&gt; from sklearn.metrics import confusion_matrix &gt;&gt;&gt; print(confusion_matrix(y_test, y_pred)) 回归度量Mean Absolute Error123&gt;&gt;&gt; from sklearn.metrics import mean_absolute_error&gt;&gt;&gt; y_true = [3, -0.5, 2]&gt;&gt;&gt; mean_absolute_error(y_true, y_pred) Mean Squared Error12&gt;&gt;&gt; from sklearn.metrics import mean_squared_error&gt;&gt;&gt; mean_squared_error(y_test, y_pred) R2 Score12&gt;&gt;&gt; from sklearn.metrics import r2_score&gt;&gt;&gt; r2_score(y_true, y_pred) 聚类度量Adjusted Rand Index12&gt;&gt;&gt; from sklearn.metrics import adjusted_rand_score&gt;&gt;&gt; adjusted_rand_score(y_true, y_pred) Homogeneity12&gt;&gt;&gt; from sklearn.metrics import homogeneity_score&gt;&gt;&gt; homogeneity_score(y_true, y_pred) V-measure12&gt;&gt;&gt; from sklearn.metrics import v_measure_score&gt;&gt;&gt; metrics.v_measure_score(y_true, y_pred) 交叉验证123&gt;&gt;&gt; from sklearn.cross_validation import cross_val_score&gt;&gt;&gt; print(cross_val_score(knn, X_train, y_train, cv=4))&gt;&gt;&gt; print(cross_val_score(lr, X, y, cv=2)) 数据预处理标准化1234&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler&gt;&gt;&gt; scaler = StandardScaler().fit(X_train)&gt;&gt;&gt; standardized_X = scaler.transform(X_train)&gt;&gt;&gt; standardized_X_test = scaler.transform(X_test) 归一化1234&gt;&gt;&gt; from sklearn.preprocessing import Normalizer&gt;&gt;&gt; scaler = Normalizer().fit(X_train)&gt;&gt;&gt; normalized_X = scaler.transform(X_train)&gt;&gt;&gt; normalized_X_test = scaler.transform(X_test) 二值化123&gt;&gt;&gt; from sklearn.preprocessing import Binarizer&gt;&gt;&gt; binarizer = Binarizer(threshold=0.0).fit(X)&gt;&gt;&gt; binary_X = binarizer.transform(X) 类条件编码123&gt;&gt;&gt; from sklearn.preprocessing import LabelEncoder&gt;&gt;&gt; enc = LabelEncoder()&gt;&gt;&gt; y = enc.fit_transform(y) 缺失值推导123&gt;&gt;&gt; from sklearn.preprocessing import Imputer&gt;&gt;&gt; imp = Imputer(missing_values=0, strategy='mean', axis=0)&gt;&gt;&gt; imp.fit_transform(X_train) 多项式属性生成123&gt;&gt;&gt; from sklearn.preprocessing import PolynomialFeatures&gt;&gt;&gt; poly = PolynomialFeatures(5)&gt;&gt;&gt; poly.fit_transform(X) 模型调优Grid Search1234567&gt;&gt;&gt; from sklearn.grid_search import GridSearchCV&gt;&gt;&gt; params = &#123;"n_neighbors": np.arange(1,3), "metric": ["euclidean", "cityblock"]&#125;&gt;&gt;&gt; grid = GridSearchCV(estimator=knn, param_grid=params)&gt;&gt;&gt; grid.fit(X_train, y_train)&gt;&gt;&gt; print(grid.best_score_)&gt;&gt;&gt; print(grid.best_estimator_.n_neighbors) Randomized Parameter Optimization123456789&gt;&gt;&gt; from sklearn.grid_search import RandomizedSearchCV&gt;&gt;&gt; params = &#123;"n_neighbors": range(1,5), "weights": ["uniform", "distance"]&#125;&gt;&gt;&gt; rsearch = RandomizedSearchCV(estimator=knn, param_distributions=params, cv=4, n_iter=8, random_state=5)&gt;&gt;&gt; rsearch.fit(X_train, y_train)&gt;&gt;&gt; print(rsearch.best_score_) 贝叶斯分类器123456789101112131415161718192021222324252627# 导入基础库from sklearn.datasets import make_classificationfrom sklearn.metrics import roc_auc_scorefrom sklearn.model_selection import train_test_split# 导入朴素贝叶斯分类器from naive_bayes_classifier import NaiveBayesClassifier# 构造二元分类问题X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, random_state=1111, n_classes=2, class_sep=2.5, n_redundant=0)# 划分训练数据集与测试数据集X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)model = NaiveBayesClassifier()# 进行模型拟合训练model.fit(X_train, y_train)# 进行模型预测predictions = model.predict(X_test)[:, 1]# 输出最终的测试结果print('classification accuracy', roc_auc_score(y_test, predictions))]]></content>
      <categories>
        <category>statistics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据预处理与 多重测试矫正]]></title>
    <url>%2F2019%2F06%2F24%2Fqvalue%2F</url>
    <content type="text"><![CDATA[数据归一化归一化后数据分布在（0，1）之间 min-max 归一化1x&apos; = (x - x_min) / (x_max - x_min) 平均 归一化1x&apos; = (x - x_mean) / (x_max - x_min) z-score 标准化标准化后数据方差为 1，均值为 0，数值范围在 0 附近 1x&apos; = (x - x_mean) / x_sd 适用条件（1）如果对输出结果范围有要求，用归一化。（2）如果数据较为稳定，不存在极端的最大最小值，用归一化。（3）如果数据存在异常值和较多噪音，用标准化。 FDR使用 qvlaue 对 GWAS p 值进行多重检验矫正： 123result =qvalue(data$p,pfdr = TRUE,fdr.level=0.05)view(result$qvalues)view(result$significant) 适用 sampleM 推算有效的独立测试数： 1Rscript simpleM_Ex.R threshold = 1 (0.05) / out_number]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shell常用命令]]></title>
    <url>%2F2019%2F06%2F14%2Fbash-tips%2F</url>
    <content type="text"><![CDATA[Ctrl+a 回到命令行首位Ctrl+e 回到命令行尾部Ctrl+xx 在命令行首和光标之间移动Ctrl+u 从光标处删除至命令行首Ctrl+l 清屏 123456789101112131415161718192021awk '!arr[$2]++' test.txt #根据第二列输出file.txt文件中的唯一条目awk '!arr[$0]++' file.txt #可过滤掉重复的行awk '&#123;sum+=$1&#125; END &#123;print sum&#125;' file.txt #将file.txt文件中第一列的值相加awk '&#123;x+=$2&#125;END&#123;print x/NR&#125;' file.txt #计算第二列的平均值 awk -v OFS='\t' #设定输出分隔符awk '&#123;print NR,length($0);&#125;' filename #打印行号和每行字符数awk '&#123;print NF&#125;' filename #打印文件列数sed -n '1~3p' filename #每3行打印一行（1：开始行，3：步长）ls -d /usr/myfile/* #输出myfile文件夹下所有文件的绝对路径cat 1.gz 2.gz 3.gz &gt;all.gz #直接合并压缩文件qstat -u USERNAME|grep 'qw'|cut -f1 -d ' '| xargs qdel #批量删除等待的任务 123456#大文件字符替换时，使用perl代替sed可以提高处理速度。perl -p -i -e 's/ATCG/atcg/' test.txt #-i直接处理perl -p -i -e 'tr/ATCGatcg/TAGCtacg/' test.txt# 快速删除大文件中包含指定字符的行。perl -p -i -e 's/.*test.*//' test.txt #正则匹配perl -p -i -e 's/^\s$//' test.txt #删除空白行 mapfile 命令可以将文件中的文本按行读入到数组中。 1234echo &#123;a..z&#125; | tr " " "\n" &gt;alpha.log mapfile myarr &lt;alpha.log echo $&#123;myarr[@]&#125; a b c d e f g h i j k l m n o p q r s t u v w x y z]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[群体结构作图]]></title>
    <url>%2F2019%2F05%2F31%2FSTRUCTURE%2F</url>
    <content type="text"><![CDATA[在拿到群体结构软件 （STRUCTURE，fastStructure，admixture）分析的 Q 数据之后下一步就是怎么把数据可视化做出堆叠图。 方法一： 1barplot(t(as.matrix(data)),col=rainbow(2),axes=FALSE,ylab="K=2",font=2,cex.lab=1.4) # col=rainbow(2)中的数字为计算时的K值 方法二： 12345#首先在 Q 数据中增加一列样本编号librar(ggplot2)library(tidyr)data2&lt;-gather(data,key='new_one',value ='count',V2，V3，-V1) #使用gather将宽数据转换为长数据ggplot(data = data2, mapping = aes(x =V1, y = V3, fill = V2)) + geom_bar(stat = 'identity', position = 'fill') 方法三： 123python2 distruct.py -K 2 --input=test/all.genotype --output=test/testoutput_simple_distruct.svg# 使用 fastStructure 自带脚本作图# 脚本使用python2编写 将 svg 结果放入 AI 中手动编辑图片，将多余的信息用橡皮擦擦除，点击直接选择工具选中图中某一个颜色后，点击选择-相同-填充颜色，AI 会自动选择图片中所有相同的颜色，点击右上角的颜色图谱（或直接输入 RGB 值）则可进行颜色的替换，填充颜色要和描边颜色一致，或者不要描边。这样你就可以自定义你图片的主题了。当然配色的话看个人需求了。]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[EMMAX GWAS]]></title>
    <url>%2F2019%2F05%2F27%2FEMMAX-GWAS%2F</url>
    <content type="text"><![CDATA[1emmax-kin tped_file -v -h -d 10 #计算K矩阵 1admixture --cv -j4 all.genotype.bed K_number | tee ./log_K_number.out #计算群体结构 1emmax -v -d 10 -t data_pruned -p pheno.txt -k all_genotype_recode12.hBN.kinf -c cov.emmax.txt -o emmax_result #gwas 1#emmax进行 case/control gwas时,表型数据应该为2/1 emmax gwas]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4种方法生成数列时间比较]]></title>
    <url>%2F2019%2F04%2F22%2Fseq-num%2F</url>
    <content type="text"><![CDATA[十万bash1234567891011# num.shdatefor i in &#123;1..100000&#125;doecho $idonedatesh num.sh &gt;num.txt09:23:3109:23:33 # 2秒 seq12345678# work.shdateseq 1 100000datesh work.sh &gt;num.txt09:24:2009:24:20 # 不到1秒 python123456789101112# num.pynum = range(1,100001)for i in num: print(i)# work.shdatepython num.pydatesh work.sh &gt;num.txt09:26:0209:26:03 # 1秒 ruby123456789101112# num.rbfor i in 1..100000 p iend# work.shdateruby num.rbdatesh work.sh &gt;num.txt09:26:5809:27:01 # 3秒 一百万bash123456789101112# num.shdatefor i in &#123;1..1000000&#125;doecho $idonedatesh num.sh &gt;num.txt09:14:2609:14:51 # 25秒 seq12345678# work.shdateseq 1 1000000datesh work.sh &gt;num.txt09:07:2809:07:29 # 1秒 python12345678910111213# num.pynum = range(1,1000001)for i in num: print(i)# work.shdatepython num.pydatesh work.sh &gt;num.txt09:09:3609:09:38 # 2秒 ruby12345678910111213# num.rbfor i in 1..1000000 p iend# work.shdateruby num.rbdatesh work.sh &gt;num.txt09:12:2709:12:46 # 19秒 两百万bash123sh num.sh &gt;num.txt09:37:5109:38:41 # 50秒 seq123sh work.sh &gt;num.txt09:39:4809:39:48 # 不到1秒 python123sh work.sh &gt;num.txt09:36:2409:36:27 # 3秒 ruby1234sh work.sh &gt;num.txt09:34:1509:34:59 # 44秒 五百万bash123sh num.sh &gt;num.txt09:41:5909:44:07 # 2分8秒 seq123sh work.sh &gt;num.txt09:41:1109:41:12 # 1秒 python123sh work.sh &gt;num.txt09:45:39 09:45:43 # 4秒 ruby1234sh work.sh &gt;num.txt09:46:4109:48:18 # 1分37秒 一千万123456789101112bash09:54:3709:59:31 # 4分54秒seq09:51:5509:51:56 # 1秒python09:53:3209:53:43 # 9秒ruby09:55:5609:59:41 # 3分45秒 一亿123456seq09:57:1409:57:16 # 2秒python10:02:4710:04:16 # 1分29秒]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[samtools按区域bam转fastq]]></title>
    <url>%2F2019%2F04%2F19%2Fsamtools%2F</url>
    <content type="text"><![CDATA[第一步12$ samtools index test.bam # 构建索引$ samtools sort -n test.bam # 按名称将bam文件排序 第二步1$ samtools view -h -b test.bam chr1:1000-2000 -o region.bam # 按区域取比对数据 第三步12$ samtools fastq -1 read_1.fq -2 read_2.fq -s singleton.fq -N region.bam # -s 只有单端匹配上的序列，-N 在1 2 序列名称后加/1，/2，-n 参数会使序列名维持原样 软件对比picard12$ java -jar picard.jar SamToFastq I=test.bam F=read1.fq F2=read2.fq R1_TRIM=1 R1_MAX_BASES=100 R2_TRIM=1 R2_MAX_BASES=100# picard 虽然可以进行转换但是无法选择区域，提供的参数TRIM和MAX_BASES是截取序列的长度 bcftools12$ bedtools bamtofastq -i test.bam -fq read1.fq -fq2 test.read2.fq# 同picard，参数更少]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[csvtk处理csv和tsv文件]]></title>
    <url>%2F2019%2F04%2F16%2Fcsvtk%2F</url>
    <content type="text"><![CDATA[处理csv (tsv) 的工具已经见过不少，如csvkit，xsv。而csvtk这个工具的一些子命令却能解决一些前者无法解决的痛点，这些痛点可能需要你自己写脚本。 软件版本：0.18.2 地址：https://github.com/shenwei356/csvtk headers1$ csvtk headers test.txt #显示文件打印标题行，-t参数可设定制表符分割，默认逗号分割 pretty csv2tab tab2csv space2tab transpose csv2md123456$ csvtk pretty test.txt #对csv更友好的展示$ csvtk csv2tab test.txt #转换csv格式到tab$ csvtk tab2csv test.txt #转换tab格式到csv$ csvtk space2tab test.txt #转换空格分割格式到tab$ csvtk transpose test.txt #转置csv/tsv **$ csvtk csv2md test.txt #转csv/tsv为makrdown格式 cut123$ csvtk cut -f 1,3-4,7-8 test.txt #选取1,3到4,7到8列$ csvtk cut -F -f '*.id,name' test.txt # -F开启模糊搜索$ csvtk cut -f -2,-3 test.txt #选取第2,3列之外的列 uniq123$ csvtk uniq -f 1 test.txt #最解决痛点的子命令，指定列进行删除重复数据，会保留第一个发现的数据#linux 自带的uniq会认定整行为一个判断单位，当blast比对中会有很多序列比对到一个序列，而我们只想保留#第一列的一个字段就好了，这时csvtk uniq 可以轻松处理这个问题。 grep12$ csvtk grep -f 1 -p 'name' test.txt #同uniq一样，grep也可以指定列去搜索（精确搜索）$ csvtk grep -f 1 -p 'name' -r test.txt # -r 开启正则（模糊搜索） join123$ csvtk join -f '1;1' test1.csv test2.csv #按照2个文件的第一列合并文件，等于-f '1'$ csvtk join -f '1;2' test1.csv test2.csv #按照2个文件的第一列和第二列合并文件csvtk join -f '1;2' --keep-unmatched test1.csv test2.csv #保留第一个文件中未匹配的行 fliter filter212$ csvtk filter -f '9&gt;1000' test.txt #过滤选择第9列大于1000的行$ csvtk filter2 -f '$7 &gt; $8' test.txt #类似awk过滤 replace12345678910111213141516171819202122$ csvtk replace -f id -p '(\d+)' -r 'ID: $1' #将id列数据(\d+)换为'ID: \d+'$ cat data.csvname idA ID001B ID002C ID004$ cat alias.csv001 Tom002 Bob003 Jim$ csvtk replace -f 2 -p "ID(.+)" -r "N: &#123;nr&#125;, alias: &#123;kv&#125;" -K -k alias.csv data.csvname idA N: 1, alias: TomB N: 2, alias: BobC N: 3, alias: 004#使用alias.csv中的键值对进行替换，&#123;nr&#125;代表计数，&#123;kv&#125;代表值，-K代表当键值对中找不到对应的键时使用原文件中的键。-k代表键值对文件 rename1csvtk rename -f 1,2 -n id1,id2 test.txt #将第1,2列的标题改为id1和id2 mutate21$ csvtk mutate2 -n test -e '$1+$2' test.txt #添加新列，-n 新列标题，-e 处理方式（awk模式） sort123$ csvtk sort -k 2:n test.txt #-k 制定排序规则，2:n意义为按第二列排序，并认为数据为数字$ csvtk sort -k 1:N test.txt #-k 制定排序规则，1:N意义为按第一列排序，并认为数据为字符串与数字混合$ csvtk sort -k 3:n,4:n test.txt #按照第三四列排序 summary123456789101112131415161718$ csvtk summary -f f4:sum,f5:sum -i test.txt #统计第四五列数字总和，-i 忽略非数字列$ csvtk summary -f f4:sum,f5:sum -g f1,f2 -i test.txt #统计第四五列数字总和，按第一二列groupby$ csvtk summary -H -i -f 4:sum,5:sum -g 1,2 #在列表没有 header 时统计第四五列数字总和，按第一二列groupby-f 支持的参数：collapse #以行的形式展示数据count #计数不忽略NAcountn #计数忽略NAfirst #第一个字符last #最后一个字符max #最大的数mean #平均数median #中位数min #最小数stdev #标准差sum #总和uniq #唯一字符variance #方差]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[比对格式问题]]></title>
    <url>%2F2019%2F04%2F11%2Fka-ks%2F</url>
    <content type="text"><![CDATA[Phylip 格式在使 pamlX 的 CodeML 进行 Ka/Ks 计算时，蛋白比对序列需要不含终止密码子且格式为 phylip 格式。如今 phylip 有两种格式：第一种为 Phylip3.2，第二种为 Phylip4。这两种格式具有明显的差别，图示如下： 其中，第一行 Phylip4 是没有 I 标识的，且 Phylip4 格式中是把所有序列的第一行集中起来放置的。在 CodeML 使用中发现只有 Phylip4 可以使用，但是会报错，原因不清楚，但是在 Phylip4 格式文件中第一行也加上 I 标识就可以正常使用了。 格式转换工具序列比对一般有两种方式：在线版和离线版，现在我通常用 mafft，mafft 比对完后会默认输出 CLUSTAL 的格式。好用的是他提供了 Reformat 来进行格式转换，基本包含了常用的所有格式。离线的一个 python 包工具为 seqmagick，可以进行 fasta 和 phylip，fasta 和 fastq 格式的转换，他还可以使用一个已经比对好的蛋白序列来进行蛋白的 DNA 序列的回比对（具体什么专业名字不清楚），也非常方便。]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shinyCircos使用方法]]></title>
    <url>%2F2019%2F03%2F19%2FshinyCircos%2F</url>
    <content type="text"><![CDATA[项目地址：https://github.com/venyao/shinyCircos 用perl写的circos的学习成本太高了，想看看两个基因组之间的差异（装个逼）怎么还得学习一周？现在我们用shinyCircos解决这个烦恼。 安装shinyCircos是个用shiny部署的R应用，所以使用RStudio安装就好了。 首先先把这些包安装： 12345678install.packages("shiny") install.packages("circlize") install.packages("RColorBrewer")install.packages("data.table")install.packages("RLumShiny") # try http:// if https:// URLs are not supported source("https://bioconductor.org/biocLite.R") biocLite("GenomicRanges") 然后进入项目主页，把整个项目下载下来放在自己硬盘的某个文件夹下并解压（解压后文件名为：shinyCircos-master）。 回到RStudio，并输入下面的命令，则会自动打开shinyCircos的程序网页。 12library(shiny)runApp("your_dir/shinyCircos-master", launch.browser = TRUE) 使用文件类型主页给出了所有文件的示例文件，下载下来对比自己的文件就可以知道需要做出什么改变。 主文件主文件包含3列：chr start end 如果是单基因组chr列按照普通染色体命名即可，如果要做多基因组之间的关系，染色体命名要区分开。 trackstracks文件可以有任意多个，包含有散点图，条形图，折线图，热图等等。 散点图、折线图、条形图包含4列：chr start end value1 热图包含：chr start end value1 value2 value3 …… valuen links datalink data旨在描述染色体位点之间的关系。 数据包含6列或者7列：chr start end chr start end （color） 导入数据点击页面的 data upload，在Upload chromosome data下选择本地的染色体文件上传。然后，在Upload data for inner tracks下选择本地的track文件上传，点击后可以选择图的类型（散点，条形等）。最后在Upload data to create links下选择本地的连锁文件上传。点击左下角的Go！即可展示、检查全部上传的文件。 画图点击页面的 Circos viasualization进入画图界面。在Plot options下勾选你所上传的所有文件的名字。选择后会有各种细微的操作调整图画。最后点击左下角的Go！即可展示所做circos图，同时支持PDF,SVG和作图所用R脚本的下载。 具体实践需要理解各种文件的内容和所做出来的图的外观，相信你在不断的操作中会摸索到其中的方法的。毕竟一个可视化的界面比一个脚本界面舒服的多。]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[plink-vcftools]]></title>
    <url>%2F2019%2F03%2F01%2Fplink-vcftools%2F</url>
    <content type="text"><![CDATA[bcftools1234bgzip view.vcf #bgzip压缩bcftools view file.vcf -Oz -o file.vcf.gz #bgzip压缩bcftools index view.vcf.gz #建立索引 12bcftools view view.vcf.gz -s NA00001,NA00002 -o subset.vcf #筛选样本bcftools view view.vcf.gz -s ^NA00001,NA00002 -o subset.vcf # ^去除样本 1bcftools sort view.vcf.gz -o sort.view.vcf #排序vcf 123456bcftools reheader -s trans.file sample.vcf -o new.sample.vcf #更改vcf文件中样本名cat trans.file NA00001 NA1 NA00002 NA2 NA00003 NA3 1bcftools concat chr1.vcf.gz chr2.vcf.gz -o concat.vcf #合并vcf文件 1bcftools merge merge.a.vcf.gz merge.b.vcf.gz -o merge.vcf #合并vcf文件（单样本多vcf文件） 12bcftools stats view.vcf &gt; view.stats #统计vcf文件bcftools filter view.vcf.gz --regions chr16:17052042-17594883 &gt; filter.vcf #选择固定区域SNP vcftools1vcftools --vcf myvcf.vcf --plink --out myplink #vcf2ped 12vcftools --vcf myvcf.vcf --keep keep.txt --recode --out keep_sub #vcf样本选择vcftools --vcf myvcf.vcf --remove remove.txt --recode --out remove_sub #vcf样本删除 1vcftools --vcf final.vcf --min-alleles 2.0 --max-alleles 2.0 --max-missing 0.95 --non-ref-af 0.05 --max-non-ref-af 0.95 --recode -c &gt;filter.vcf #vcf过滤 1vcftools --vcf filter.vcf --thin 1000 --recode -c &gt;thin.vcf #按snp间隔过滤vcf 1vcftools --gzvcf file.vcf.gz --positions specific_position.txt --recode --out specific_position.vcf #选择指定SNP plink1plink --vcf myvcf.vcf --recode --out myplink #vcf2ped 1plink --vcf myvcf.vcf --freq --out frquency #maf统计 1plink --file FILENAME --make-bed --out FILENAME #ped2bed 1plink --bfile FILENAME --recode --out FILENAME --noweb #bed2ped 1plink --tfile FILENAME --recode --out FILENAME #tped2bed 1plink --bfile FILENAME --recode transpose --out FILENAME --noweb #bed2tped 1plink --bfile FILENAME --recode vcf-iid --out FILENAME #bed2vcf 12plink --bfile FILENAME --indep-pairwise 50 5 0.5 #LD-pruningplink --bfile FILENAME --extract plink.prune.in --make-bed --out data_pruned 1plink -file FILENAME --recodeHV --out recode #haploview GATK123456789gatk-4.1.2.0/gatk --java-options "-Xmx10g" HaplotypeCaller -R Chr01.fa --emit-ref-confidence GVCF -I D1906306A.Chr01.realign.bam -O D1906306A.Chr01.realign.g.vcfgatk-4.1.2.0/gatk --java-options "-Xmx10g" CombineGVCFs -R Chr01.fa -V D1905327A.Chr01.realign.g.vcf -V D1905328A.Chr01.realign.g.vcf -O all.raw.snps.indels.g.vcfgatk-4.1.2.0/gatk --java-options "-Xmx10g" GenotypeGVCFs -R Chr01.fa -V all.raw.snps.indels.g.vcf -O Chr01.raw.snps.indels.vcfgatk-4.1.2.0/gatk --java-options "-Xmx10g" SelectVariants --select-type-to-include SNP -R Chr01.fa -V Chr01.raw.snps.indels.vcf -O Chr01.snp.vcfgatk-4.1.2.0/gatk --java-options "-Xmx10g" VariantFiltration -R Chr01.fa --filter-expression "QD &lt; 2.0 || MQ &lt; 40.0 || FS &gt; 60.0 || SOR &gt; 3.0 || MQRankSum &lt; -12.5 || ReadPosRankSum &lt; -8.0" --filter-name LowQualFilter --missing-values-evaluate-as-failing --variant Chr01.snp.vcf -O Chr01.filtered.vcf]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gwas]]></title>
    <url>%2F2019%2F01%2F04%2Fgwas%2F</url>
    <content type="text"><![CDATA[质量控制质量控制是为了去掉在测序过程中低质量的、没有测到的和测序的接头序列。质控软件大同小异，在去除低质量碱基时，最好使用的方法应该是滑动窗口。 因为根据目前的测序原理，read的错误率会明显富集于末端区域，而前半部分的质量都会比较高，这种计算(SOAPnuke按照低质量碱基比例去除序列)比例的方法并不能很好地反映这一现象。 质控同时要注意测序下机数据 fq 文件的质量值格式是Phred33还是Phred64。 数据比对数据比对软件还是最常用的 bwa ，在构建参考序列索引时，is 算法最快(基因组小于1G常用)，而 bwtsw 用于大基因组。 在 bwa mem算法比对时，-R &#39;@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name&#39; -R参数中的@RG信息要给出，如果没有给出，仍然可以使用 picard 中 AddOrReplaceReadGroups 对 sam 或 bam 文件进行加头处理。这个信息对于我们后续对比对数据进行错误率分析和Mark duplicate时非常重要 。 另外，samtools 可以利用参考基因组为 sam 或者 bam 文件加上 header 中的 @SQ 信息。 变异检测变异检测核心是使用 GATK 中的HaplotypeCaller 组件进行 snp calling 。而决定变异检测最终结果的好坏则是Variant quality score recalibration（VQSR），其中VQSR中使用VariantFiltration进行硬过滤时，我的参数为：--filterExpression &quot;QD &lt; 2.0 || MQ &lt; 40.0 || ReadPosRankSum &lt; -8.0 || FS &gt; 60.0|| HaplotypeScore &gt; 13.0 || MQRankSum &lt; -12.5&quot; 在获得变异 vcf 文件后将其转为基因型文件后，常常发现有些个体在某些位点上是没有基因型的，这个时候就需要Beagle软件将这些空缺的位点补齐，形成一份完整的变异文件。 vcf 文件也可以 使用SnpEff， Annovar 这个2个软件进行变异注释，得出每一个变异位点的确切功能。 个性化分析个性化是变异文件内容充分挖掘的过程，是从几百万 snp 位点中挑选出和表型具有显著联系的几个或几十个位点的过程。如果群体结构简单，是单一种群使用简单线性模型(GLM)即可得出良好的结果，如果测序的包含多个群体，或者单群体经过群体结构分析(PCA分析、structrue分析)分群严重，则可以使用混合线性模型(MLM)进行分析，MLM 需要亲缘矩阵(K)和群体结构矩阵(Q)作为协变量建模计算。 建模计算出结果后便可以使用结果中的P值画曼哈顿图和QQ图，QQ图用来检测结果的正态性，曼哈顿图中阈值一直在讨论和研究中，包括且不限于(Bonferroni校正，sampleM，Keff，SLIDE)方法来确定阈值，一般简单的，使用0.05/snp数作为阈值，如果结果有点差，可以做出图后确定阈值后在阈值上留下1-100个点为好。 在选出显著性位点后，查看注释文件确定位点的位置、变异、功能等，处于编码区还是非编码区，是否改变氨基酸等。接下来就是找出这些点所处的基因或基因的上下游什么位置。一般以位点的上下游500Kb搜寻，严格的可以以上下游100Kb。 经过资料、文献的确认接下来就要准确确认基因的信息(所选基因很可能是某个亚家族的一个分支)，方法可以是进化树、domain 分析。 推荐文献质控： Sickle: A sliding-window, adaptive, quality-based trimming tool for FastQ filesCutadapt removes adapter sequences from high-throughput sequencing reads 比对： Fast and accurate short read alignment with Burrows–Wheeler transform 变异检测 Performance benchmarking of GATK3.8 and GATK4 A One-Penny Imputed Genome from Next-Generation Reference Panels Genotype imputation for genome-wide association studies Non-Synonymous and Synonymous Coding SNPs Show Similar Likelihood and Effect Size of Human Disease Association 个性化分析 Mixed linear model approach adapted for genome-wide association studies The importance of cohort studies in the post-GWAS era From genome- wide associations to candidate causal variants by statistical fine- mapping]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux 工具替换]]></title>
    <url>%2F2019%2F01%2F04%2Flinux-tools%2F</url>
    <content type="text"><![CDATA[ack用来代替 linux 中的 grep ，使用方法： https://github.com/beyondgrep/ack2 123456789ack text ./ #在当前目录下所有文件中寻找text存在的行ack text test.py #在test.py 文件中寻找text存在的行ack -i text test.py #忽略大小写ack -v text test.py #寻找没有text存在的行ack -w text test.py #text必须是一个单词ack -H text test.py #同时输出文件名和行号ack -m 2 text test.py #最多输出2个匹配ack -g work #通过正则表达寻找文件ack -c text test.py #报告总匹配行数 ripgrep用来代替 linux 中的 grep ，使用方法： https://github.com/BurntSushi/ripgrep/ 12345678rg text ./ #在当前目录下所有文件中寻找text存在的行rg text *.py #在所有 python 文件中寻找text存在的行rg -i text test.py #忽略大小写rg -v text test.py #寻找没有text存在的行rg -w text test.py #text必须是一个单词rg -N text test.py #不输出行号rg -z text test.gzip #搜索压缩文件rg -c text test.py #报告总匹配行数 nnn用来代替 linux 中的 ls ，使用方法： https://github.com/jarun/nnn 12345678910nnn ./#键盘快捷键q #退出d #显示文件详细. #显示隐藏文件s #按文件大小排序t #按修改时间排序e #在编辑器中打开文件/ #过滤、寻找文件? #快捷键说明 fzffzf 用来进行模糊匹配，超级强大。使用Ctrl+r 反向查看命令历史然后进行模糊搜索快速定位。less |fzf 可以快速查找需要内容。 https://github.com/junegunn/fzf less work1.1.1.py |fzf Ctrl + r progressprogress 可以显示系统绝大多数命令的进程。使用 -w 参数预估进程速度和剩余时间。 https://github.com/Xfennec/progress ccat用来代替系统的 cat 命令（代码高亮） https://github.com/jingweno/ccat ncduncdu 用来代替系统的 du 命令，使用友好的界面展示储存使用情况。 https://github.com/rofl0r/ncdu ncdu tldrtldr 用来代替系统的 man 命令，相比 man 是给出一个命令的详细解释，tldr是给出一个命令的常用示例。 https://github.com/tldr-pages/tldr]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sam文件header全面解析]]></title>
    <url>%2F2018%2F11%2F20%2Fsam-header%2F</url>
    <content type="text"><![CDATA[众说周知，最常用的比对软件 bwa 产生的结果文件是 sam 格式，具体格式可以参考 sam 。 其中文件的主体便是比对的结果，而头文件往往是我们最容易忽视的地方，尤其踩过太多的坑，特来解释和记录其 header。 header 中最常见的有3个：@SQ @RG @PG ，@SQ 用来记录参考序列的信息，@RG 用来记录样本的信息，@PG 用来记录 bwa 程序信息。 @SQ 和 @PG 在 bwa 比对之后就会在 sam 文件中存在。而 @RG 则需要自己在 bwa mem 比对的命令中使用 -R 参数来添加。@RG 这个信息对于我们后续对比对数据进行错误率分析和Mark duplicate时非常重要 。 如果在 bwa 比对期间没有选择 -R 参数，可以 picard 中 AddOrReplaceReadGroups 对 sam 或bam 文件进行加头处理。 12345678910-R '@RG\tID:sample\tLB:sample\tSM:sample\tPL:ILLUMINA' #bwa mem @RG头格式java -jar picard.jar AddOrReplaceReadGroups \ #sam或bam文件（picard ） I = input.bam \ O = output.bam \ RGID = 4 \ RGLB = lib1 \ RGPL = illumina \ RGPU = unit1 \ RGSM = 20 1234567891011$ bwa mem -t 4 genome.fa ./samples/A.fastq &gt;A.sam #非常普通的SE数据bwa比对$ head -n 3 A.sam@SQ SN:I LN:230218@PG ID:bwa PN:bwa VN:0.7.17-r1188 CL:bwa mem -t 4 genome.fa -R SRR800764.1 4 * 0 0 * * 0 0 CATCTTTGGAGTAACTATTATTTCGCCCCTTTTGTTTGCTGCATATCGCCCCGCTCTCTGCATACACGATTGGATAATGACCAAAGCAAGGTTTAATACGC………… #关键看头文件，后面比对信息省略$ samtools view -Sb A.sam &gt; A.bam #转为bam文件$ samtools sort A.bam &gt; A_sort.bam #排序$ samtools view -h A_sort.bam &gt; A_sort.sam #再转回来sam文件，一定要加-h参数，默认是不输出header的。如果没有-h参数，是无法对sam文件进行转换和排序的。如下：[E::sam_parse1] missing SAM header[W::sam_read1] parse error at line 1[main_samview] truncated file. 上面提到在缺少头文件的前提下是无法将 sam 转为 bam 的，更加无进行 bam 文件排序。但是如果我们使用 picard 这个问题就可以解决了。 1java -Xmx5g -Djava.io.tmpdir=./tmp -jar ../picard-tools-1.117/SortSam.jar INPUT=A_sort.sam OUTPUT=A_sort.bam SORT_ORDER=coordinate VALIDATION_STRINGENCY=SILENT picard 的 SortSam.jar 可以直接对没有 header 的 sam 文件排序并转为 bam 文件。 另外，samtools 可以为sam或者bam文件加上 header 中的 @SQ： 1samtools view -Sb -T genome.fa sample_no_header.sam &gt;sample_with_header.bam #使用 -T 参考序列加header（@SQ）]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[水产育种中的基因组选择（翻译）(二)]]></title>
    <url>%2F2018%2F11%2F13%2FGenomic-Selection-in-Aquaculture-Breeding-Programs2%2F</url>
    <content type="text"><![CDATA[一个实现基因组预测的实例如上一节所述，许多方法已被用于基因组预测。这里，将使用R包rrBLUP展示实现基因组预测的示例 (Endelman, 2011)。在演示中，将使用另一个R包BLR中的599个小麦品系的数据集，这些小麦系在1279个DArT标记上进行基因分型。如果你是语言新手，有许多在线资源可用于获取R的基本介绍 (Torfs &amp; Brauer, 2014; Venables, Smith &amp; R Development Core Team, 2016)。 12345678910111213141516171819202122232425262728293031install.packages('rrBLUP')install.packages('BLR')library(BLR)library(rrBLUP)rm(list=ls())set.seed(99) #设置随机数data(wheat) #载入小麦数据############################################### marker-basedM &lt;- 2∗X-1 #将标记转换为&#123;−1，1&#125;pheno &lt;-Y[,1] #取第一列表型数据geno &lt;- M #取基因型数据whichTest&lt;-sample(1:length(pheno),100) #随机取出100个样本表型数据的编号phenoTrain &lt;- pheno[-whichTest] #取出训练集表型数据phenoTest &lt;- pheno[whichTest] #取出测试集表型数据GenoTrain &lt;- as.matrix(geno[-whichTest, ]) #取出训练集基因型数据genoTest &lt;- as.matrix(geno[whichTest, ]) ##取出测试集基因型数据markerEffects &lt;- mixed.solve(y=phenoTrain, Z=GenoTrain)$u #计算位点效应predictedGBV &lt;- genoTest%∗%markerEffects #计算育种值(predictionAcc &lt;- cor(predictedGBV, phenoTest)) #计算育种精确度# 0.5370625############################################### kinship-basedA1 &lt;-A.mat(M) #计算基因组关联矩阵rownames(A1) &lt;- 1:length(pheno) #替换A1矩阵行名yNA=pheno #取出表型yNA[whichTest] &lt;- NA #将测试集表型删除data1 &lt;-data.frame(y=yNA,gid=1:length(pheno)) #构建列表ans1 &lt;- kin.blup(data1,K=A1,geno="gid",pheno="y") #计算育种结果(cor(ans1$g[whichTest],Y[whichTest,1])) #计算育种精确度# 0.5370625 GS的几个重要考虑需要多少动物进行基因分型？GS 的准确性对与育种成功至关重要，它本身就是动植物育种的一个重要研究领域。除了选择统计模型外，GS 的准确性还取决于许多相互关联的因素，即基因组大小，标记密度，标记与 QTL之间的 LD，QTL 效应的数量和大小，有效群体大小，训练集的大小，训练与测试集动物的关系和性状遗传力。为了达到所需的精确度，训练集的动物数量主要取决于性状的遗传力估计和种群的有效规模。 (Ne) (Figure 2)。较低的遗传力估计将需要一个更大的训练集，同时，较大的Ne也需要相应的更大的训练样本。通常，更多的训练样本将为候选样本提供更准确的预测。例如， 荷斯坦奶牛需要有基因型和表型记录的3000-10000头公牛参考种群才能有0.7的基因组选择精确度。 ​ Figure 2 参考种群中所需的动物数量，以便为估计的GBV精确性达到0.7(Goddard &amp; Hayes, 2009) 多少SNP位点才足够？snp位点应该覆盖整个基因组，并且要有足够的密度以确保大多数QTL都在LD中，需要至少有一个标记在LD区间，因此，对标记效应的估计可以捕捉到最大的遗传变异。所需SNP的总数主要取决于基因组大小和基因组的LD范围。通常，相邻SNP标记之间的LD的 r2 &gt;0.2 ，那SNP将会足够(Calus et al., 2008)。 在实践中，50 K SNP 为奶牛品种提供了较好的预测精度。将密度从50 K进一步增加到800 K，只会略微增加预测精度 (Khatkar et al., 2012)。然而，如果训练集和测试集动物是相关的，基因组关联矩阵（GRM）可用于使用 gBLUP 预测 GBV。GRM 可以用一个很小的 SNP 集精确地计算出来。 用中等密度 SNP 芯片进行基因分型的成本(例如，牛的50 K SNP芯片)大约是每一个 DNA 样本40到100美元。基因分型和测序的成本正在迅速下降，使这项技术应用于更多物种，成本效益更高。此外，基因型估算可以有效地提高基因型数据的密度，降低成本。基因型估算包括用高密度 SNP 集对一小部分群体进行基因分型，并用低成本、低密度 SNP 位点预测其他群体的高密度基因型，这些经过计算模拟的基因型虽然不是太准确，但是还可以用于 GS (Khatkar et al., 2012)。 对基因组预测来说，最重要的因素是样本数还是SNPs数？更多的记录和更紧密的标记间距提高了准确性。但是，当 SNP 的密度很小时（小于1000个SNPs），在训练集中增加更多的动物对于提高基因组预测的准确性变得更加重要。 是否有可能预测不同品种/群体？是否有可能预测不同品种/群体—也就是说，使用一个品种的训练动物为另一个品种做预测。GS已成功地应用于群体内育种值的预测。然而，GS 在杂交预测中的成功是有限的。例如，当计算不属于训练组的品种的基因组预测时，精确度接近于零，或非常低 (Kachman et al., 2013)。在这种情况下，使用几个具有中/大效应的 SNP 并采用贝叶斯方法可能会有帮助，因为这可能只包括那些跨品系/品种分离的 QTL。 我们需要有关基因和基因功能的知识吗？在实践中，大多数GS方法主要使用(非线性)线性模型估计的 SNP 效应，或者仅仅使用 GRM，因此它们没有利用基因、基因功能的特定知识，甚至没有基因组中的SNPs的精确定位。 准确性会随着世代下降吗？一般，基因组选择中GBVs的预测是利用标记与数量性状位点（QTL）之间的 LD 来实现的。然而，在实践中，尤其是在种内预测，基因组预测的可靠性取决于候选/测试中动物与训练/参考动物之间遗传关系的强弱。因此，当应用于下一代时，来自固定训练集的预测方程的精度会迅速下降。这通常意味着预测方程需要通过添加来自更近几代的动物来更新。 使用GS会增加近亲繁殖吗？较短的世代间隔和较高的选择强度将使近交系的年率提高。特别是在选择的标记附近近亲繁殖。这将需要在管理种群近亲繁殖方面给予更多的关注。利用基因组关系信息进行仔细的交配选择，如最小同祖先交配和最优贡献选择，可以控制近交系。 水产养殖中的基因组选择水产养殖物种繁多，由于生命周期、繁殖力、有效种群规模和育种目标的不同，GS 在水产养殖中的潜力将因品种不同而有所不同。目前，以水产养殖品种最多的传统养殖项目主要采用群体选择和家系选择两种方式。同胞系测试是针对那些不能直接在候选样本上测量的性状进行的(例如，在生命后期或屠宰时记录到的性状、抗病性)。以家系为基础的选择只使用了一小部分(一半)的遗传变异，并导致近亲育种的增加。GS 还可以预测家族内部的遗传差异，因此可以利用所有的遗传变异。由于只使用很少的动物/家系而导致近亲繁殖的增加是水产养殖中的一个主要问题。此外，在传统的选择方案中，个体的标记、谱系的记录和单独家系的饲养也是困难和昂贵的。遗传标记的信息可以帮助应对其中的一些挑战。随着测序和基因分型成本的降低，遗传标记和 GS 在水产养殖中的应用越来越受到人们的重视。然而，SNP 集的开发和基因组资源的应用直到最近才在几个水产养殖物种中进行，以此，在实际 GS 实施结果的信息是不全面的。已发表的报告主要是基于 GS 使用模拟数据的效率。所有的模拟研究表明，使用 GS 来进行水产养殖育种可以提高在生产(连续型)和疾病(二分型)位点的选择和遗传增益的准确性。(Sonesson &amp; Meuwissen, 2009)；(Nielsen, Sonesson &amp; Meuwissen, 2011)； (Lillehammer,Meuwissen &amp; Sonesson, 2013)，与传统的同胞系检测相比，近亲繁殖减少了81%(Sonesson &amp; Meuwissen, 2009)。通过结合传统的 BLUP 家系育种值和基于低密度基因分型而不影响遗传增益的家系内育种值，也可以降低基因分型的成本(Lillehammer et al., 2013)。 尽管基因分型和测序成本下降，由于大量样本的养殖和基因分型，基因组选择策略仍然需要很多的成本。然而，部分额外成本可以从增加的遗传收益中弥补而且与传统的选择方案相比，对同胞系样本测试的依赖更少。大多数水产养殖物种中个体动物的价值一般较低（与牛相比）。然而，由于大多数水产养殖物种的繁殖力很高，遗传增益的影响因遗传收益从繁殖核心立即转移到商业池塘而扩大。然而，利用GS优化育种方案还需要进一步的研究，比如在训练和选择中确定最佳的动物数量，标记密度，最小化近亲繁殖最大化长期遗传收益，与传统方案的比较的经济分析。GS 除了用于性状外，遗传标记在水产养殖中还可以提供比传统方法更多的优势，例如，性别相关标记的使用可以生产单性别的后代(Robinson et al., 2014)；基因渗入可以用来在种群中引入理想的基因，例如，通过反复回交和标记辅助选择，将对特定疾病的抗性从本地菌株转移到商品菌株(Odegard et al., 2009)；标记可用于监测和繁殖来自群体的任何有害突变；遗传物质的可操纵性来保护育种者。 总之，随着基因组技术的快速发展和基因组信息获取成本的不断降低，全基因组选择在提高盈利能力和在个体数增加依旧保持遗传变异性的同时，对提高水产养殖品种的食品生产水平具有很大的潜力。 致谢十分感谢 Dr. Gerhard Moser 对此手稿的建议。 参考Calus, M.P., Meuwissen, T.H., de Roos, A.P. and Veerkamp, R.F. (2008) Accuracy of genomic selection using different methods to define haplotypes. Genetics, 178,553–561.de Los Campos, G., Hickey, J.M., Pong-Wong, R. et al. (2013) Whole-genome regression and prediction methods applied to plant and animal breeding. Genetics, 193, 327–345.de Los Campos, G., Naya, H., Gianola, D. et al. (2009) Predicting quantitative traits with regression models for dense molecular markers and pedigree. Genetics, 182, 375–385.Endelman, J.B. (2011) Ridge regression and other kernels for genomic selection with R package rrBLUP. Plant Genome, 4, 250–255.Erbe, M., Hayes, B.J., Matukumalli, L.K. et al. (2012) Improving accuracy of genomic predictions within and between dairy cattle breeds with imputed high-density singlenucleotide polymorphism panels. Journal of Dairy Science, 95, 4114–4129.Falconer, D.S. and Mackay, T.F.C. (1996) Introduction to quantitative genetics, 4th edn, Longman, Essex, UK.Gianola, D. and van Kaam, J.B. (2008) Reproducing kernel Hilbert spaces regression methods for genomic assisted prediction of quantitative traits. Genetics, 178,2289–2303.Goddard, M.E. and Hayes, B.J. (2009) Mapping genes for complex traits in domestic animals and their use in breeding programmes. Nature Reviews Genetics, 10, 381–391.Gonzalez-Camacho, J.M., de Los Campos, G., Perez, P. et al. (2012) Genome-enabled prediction of genetic values using radial basis function neural networks. Theoretical and Applied Genetics, 125, 759–771.Gonzalez-Recio, O., Weigel, K.A., Gianola, D. et al. (2010) L2-Boosting algorithm applied to high-dimensional problems in genomic selection. Genetics Research (Cambridge), 92,227–237.Habier, D., Fernando, R.L. and Dekkers, J.C. (2007) The impact of genetic relationship information on genome-assisted breeding values. Genetics, 177, 2389–2397 Habier, D., Fernando, R.L., Kizilkaya, K. and Garrick, D.J. (2011) Extension of the Bayesian alphabet for genomic selection. BMC Bioinformatics, 12, 186. Heslot, N., Yang, H.P., Sorrells, M.E. and Jannink, J.L. (2012) Genomic selection in plant breeding: a comparison of models. Crop Science, 52 (1), 146–160. Jannink, J.L., Lorenz, A.J. and Iwata, H. (2010) Genomic selection in plant breeding: from theory to practice. Briefings in Functional Genomics, 9, 166–177.Kachman, S.D., Spangler, M.L., Bennett, G.L. et al. (2013) Comparison of molecular breeding values based on within- and across-breed training in beef cattle. Genetics Selection Evolution, 45, 30.Khatkar, M.S., Moser, G., Hayes, B.J. and Raadsma, H.W. (2012) Strategies and utility of imputed SNP genotypes for genomic analysis in dairy cattle. BMC Genomics, 13, 538.Khatkar, M.S., Zenger, K.R., Hobbs, M. et al. (2007) A primary assembly of a bovine haplotype block map based on a 15,036-single-nucleotide polymorphism panel genotyped in Holstein-Friesian cattle. Genetics, 176, 763–772.Lillehammer, M., Meuwissen, T.H. and Sonesson, A.K. (2013) A low-marker density implementation of genomic selection in aquaculture using within-family genomic breeding values. Genetics Selection Evolution, 45, 39.Maenhout, S., De Baets, B. and Haesaert, G. (2010) Prediction of maize single-cross hybrid performance: support vector machine regression versus best linear prediction. Theoretical and Applied Genetics, 120, 415–427.Meuwissen, T.H., Hayes, B.J. and Goddard, M.E. (2001) Prediction of total genetic value using genome-wide dense marker maps. Genetics, 157, 1819–1829.Moser, G., Tier, B., Crump, R.E. et al. (2009) A comparison of five methods to predictgenomic breeding values of dairy bulls from genome-wide SNP markers. Genetics Selection Evolution, 41, 56.Nejati-Javaremi, A., Smith, C. and Gibson, J.P. (1997) Effect of total allelic relationship on accuracy of evaluation and response to selection. Journal of Animal Science, 75,1738–1745.Nielsen, H.M., Sonesson, A.K. and Meuwissen, T.H. (2011) Optimum contribution selection using traditional best linear unbiased prediction and genomic breeding values in aquaculture breeding schemes. Journal of Animal Science, 89, 630–638.Odegard, J., Yazdi, M.H., Sonesson, A.K. and Meuwissen, T.H. (2009) Incorporating desirable genetic characteristics from an inferior into a superior population using genomic selection. Genetics, 181, 737–745.Ogutu, J.O., Piepho, H.P. and Schulz-Streeck, T. (2011) A comparison of random forests, boosting and support vector machines for genomic selection. BMC Proceedings, 5 (3), S11.Pryce, J.E., Goddard, M.E., Raadsma, H.W. and Hayes, B.J. (2010) Deterministic models of breeding scheme designs that incorporate genomic selection. Journal of Dairy Science,93, 5455–5466.Robinson, N.A., Gopikrishna, G., Baranski, M. et al. (2014) QTL for white spot syndrome virus resistance and the sex-determining locus in the Indian black tiger shrimp (Penaeus monodon). BMC Genomics, 15, 731.Schaeffer, L.R. (2006) Strategy for applying genome-wide selection in dairy cattle. Journal of Animal Breeding and Genetics, 123, 218–223.Sonesson, A.K. and Meuwissen, T.H.E. (2009) Testing strategies for genomic selection in aquaculture breeding programs. Genetics Selection Evolution, 41, 1. Torfs, P. and Brauer, C. (2014). “A (very) short introduction to R,” Hydrology and Quantitative Water Management Group, Wageningen University, The Netherlands,available at https://cran.r-project.org/doc/contrib/Torfs+Brauer-Short-R-Intro.pdf Tsai, H.Y., Hamilton, A., Tinch, A.E. et al. (2015) Genome wide association and genomic prediction for growth traits in juvenile farmed Atlantic salmon using a high density SNP array. BMC Genomics, 16, 969.VanRaden, P.M. (2008) Efficient methods to compute genomic predictions. Journal of Dairy Science, 91，4414–4423.VanRaden, P.M., Van Tassell, C.P., Wiggans, G.R. et al. (2009) Invited review: reliability of genomic predictions for North American Holstein bulls. Journal of Dairy Science, 92,16–24.Vazquez, A.I., de los Campos, G., Klimentidis, Y.C. et al. (2012) A comprehensive genetic approach for improving prediction of skin cancer risk in humans. Genetics, 192,1493–1502.Venables, W.N., Smith, D.M. and the R Core Team. (2016). An introduction to R.Notes on R: A programming environment for data analysis and graphics. Version 3.2.4 (2016-03-10) Available: https://cran.r-project.org/doc/manuals/R-intro.pdf. Accessed 15 April 2016.]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[水产育种中的基因组选择（翻译）(一)]]></title>
    <url>%2F2018%2F11%2F08%2FGenomic-Selection-in-Aquaculture-Breeding-Programs%2F</url>
    <content type="text"><![CDATA[​ 作者：Mehar S. Khatkar ​ 翻译：轩暮 介绍基因组选择最初在2001年被提出，如今已经在家畜中广泛应用。但是迄今为止在水产物种中的应用却十分受限。在这一章我们介绍了基因组选择的基本原理，并给出如何进行基因组选择的示例，在结尾提出了一些对水产物种的看法。 最新研究的分子技术为获得成千上万的遗传标记提供了可能（典型的标记 SNPs），即使在没有基因组组装或遗传图谱的物种中也是如此。基因分型和测序的成本不断下降，使得在许多水产物种中使用这些工具成为可能。来自家系或非家系群体的基因型数据可以提供对种群结构、基因关联和选择育种信息的见解。事实上，随着分子数据的容易获得性，就不再需要记录家系信息，这可以大大简化育种计划，特别是在水产物种中。本章首先介绍了基因组选择的概念，即遗传标记在选择育种中的应用，然后介绍了遗传标记在水产育种中的应用前景。 基因组选择分子标记在动物育种中的应用已经有很长一段时间了。然而，直到近些年可以大量地获得全基因组的分子标记，才使得使用DNA检测来直接预测动物的育种特性成为可能。通过模拟， Meuwissen,Hayes, and Goddard (2001)表明动物的遗传值可以不使用其表型或家系信息，只使用在染色体上高密度标记的基因型被准确地估计出来。在这种方法中，最著名的就是基因组选择（genomic selection，GS）或称为全基因组选择（whole genome selection），选择决定是根据基因组育种值(GBVs)作出的，基因组育种值一般使用高密度标记（SNPs）计算得出。基因组选择依赖于如下假设：全基因组标记的密度足够高、至少一个数量性状位点处于高度连锁不平衡区间。整个基因组标记效应的估计将为某一个性状的遗传价值提供准确的预测。 基因组选择的实现非常简单。它使用“训练群体”（多个个体样本）来建立预测方程，该群体既有基因型也有表型。然后，将此方程应用于“测试群体”（另外多个个体样本）的基因型，以计算分子或基因组的育种值(GBVs)。使用育种值排序和选择样本来作为下一代的父母本。（Figure 1）如果需要，可以再使用传统的估计育种值(EBVs)来生成基因组估计育种值(GEBV)。 ​ Figure 1 基因组选择概述（改编自Goddard &amp; Hayes, 2009） 与传统育种方案相比，基因组选择可以提高遗传增益率，因为它可以大幅缩短世代间隔和增加选择强度(Schaeffer, 2006)。例如，绵羊和奶牛的遗传改良率分别提高了25%和100%。基因组选择对于昂贵且难以测定表型的性状尤其有吸引力，这些性状仅以是否死亡来衡量，或在生命后期表达出来(Pryce et al., 2010)。基因组选择可能会增加难以记录性状的遗传增益，例如水产养殖和家禽品种的抗病性，猪的肉质和绵羊一生的羊毛产量及抗虫性。基因组选择技术被认为是动植物育种计划中的一个巨大里程碑。在家畜中，SNP 芯片首次应用在牛中(Khatkar et al., 2007)，这使得基因组选择首次成功地应用于奶牛(Moser et al., 2009)；(VanRaden et al., 2009)。基因组选择已经取代了许多国家的后裔测定方法，或者正应用于后裔测定的幼年公牛预选。目前，基因组选择正被应用于许多其他动植物物种中(Goddard &amp; Hayes,2009)，都具有不同程度的成功。基因组预测的原理甚至在人类研究中得到了应用，特别是对于高危人群的识别进行了最佳干预和个性化治疗。例如，一种基因组预测方法被用来预测人类患皮肤癌的可能性，并取得了很好的结果(Vazquez et al., 2012)。然而，应该注意到在使用“基因组选择”和“基因组预测”这两个术语方面的区别。“基因组选择”已被用于动物育种计划，这包括通过基因组预测鉴定遗传优越的动物选择和选择动物交配以生产下一代。相比之下，“基因组预测”只涉及从基因组信息中预测个体的遗传值，并且更适合于人类研究。 基因组选择的步骤图 1 已经概述了基因组选择，包括以下主要步骤。 参考群体的准备构建参考种群需要大量的动物样本。这些动物被用来测定所感兴趣的性状，并对基因组范围的标记进行基因分型，通常使用 snp 芯片来获得大量的 snp 标记。为了进行统计分析，SNP 基因型通常编码为数值变量，取值为0、1或2，分别对应于一个纯合子、杂合子、另一个纯合子。参考群体通常分为训练集和验证集。 预测方程每个 SNP 标记（编码为0，1和2，一个等位基因的数目或拷贝）效果估计(W)的训练结果统计分析以及所有标记的基因型效应联合生成一个预测方程来估计每种动物的育种值。 ​ GBV = w1∗ SNP1 + w2∗ SNP2 + w3∗ SNP3 +…+ Wn∗ SNPn w是偏回归系数或称为某个SNP的效应值大小，SNP是基因型（数值型）的载体。 下节提到的许多分析方法的其中一种可以用来建立这样的方程。 预测方程的验证验证集中的动物也有基因型和表型的数据。将上述预测方程应用于验证集动物的基因型，用以估计 GBVs。预测方程的准确性是通过将估计的 GBVs 值与实际的表型信息进行比较来评估的。连续性状的预测精度可以用性状的预测值和实际值的方差或相关系数来衡量。此步骤是可选的，但提供了关于基因组选择准确性的重要信息，因此建议进行。 测试集个体育种值的计算测试集个体只需有基因型，不需要表型信息。将预测方程应用于这些动物的基因型计算中。 选择与交配测试集个体根据GBVs进行排序，而排在顶端的动物则被挑选和交配来生产下一代。 基因组预测模型几种分析方法已被应用于全基因组的遗传优势预测。可大致分为三大类： 回归方法：基因组选择最简单的形式就是估计每个标记或者数量性状位点（QTL）的效应，然后对每个个体在基因组中的所有位点进行总结。这必须假定有效应的标记处于 LD 区间或者他们本身就是 QTL 位点，而且大部分的遗传变异都是可加的。然而，在使用高密度 SNP 集情况下，标记的数量（p）远大于在训练集中做训练的动物数量（n）。这使得使用简单回归模型估计所有标记的效应具有挑战性。为了解决这个大 p 与小 n 回归问题，提出几种变量选择和收缩估计方法来解决具表型的全基因组预测。Meuwissen et al. (2001)提出三种方法：BLUP (rrBLUP)，Bayes A 和 Bayes B 可以在预测模型中容纳大量的遗传标记。这些方法和其他后续贝叶斯方法：Bayes C𝜋 (Habier et al.， 2011)，Bayesian LASSO (de Los Campos et al.， 2009)， Bayes-R (Erbe et al.，2012) 的不同之处在于SNP/QTL效应的先验分布的定义。关于这些办法的详细介绍和比较，请看 de Los Campos et al. (2013)。偏最小二乘回归 (PLSR) 和主成分回归（PC）通过计算隐变量来降维从而进行预测。(Jannink,，Lorenz &amp; Iwata，2010；Moser et al.，2009)。 基因组关系法：基因组关系法也被称为“gBLUP”。在此方法中，需要计算个体间的基因组关系矩阵(GRM)。然后使用 GRM 计算所有动物的育种值。这种方法与传统的“动物模型”是等价的，主要的区别在于用 GRM 替换基于家系的关系矩阵。同样的模型可以用来估计方差分量和遗传参数。这个框架可以很容易地扩展到多重性状的分析中。这在水产养殖中的应用是非常具有吸引力的，因为水产养殖记录家系信息是非常困难和昂贵的，而 gBLUP 不需要记录谱系信息直接应用于育种。GRM 是基于实际的基因组相似性估计实际亲缘关系。因此，与基于家系的预期关系相比它更准确。比如，减数分裂时染色体的随机分离会导致全同胞家系样本间实际基因组相似性的变化 (Nejati-Javaremi, Smith &amp; Gibson, 1997)。GRM 可以从标记的基因型矩阵构建。VanRaden (2008) 描述了3种构建 GRM 的方法并评价了它们的实际表现。 半参数和机器学习方法：在大多数实践应用中，GBVs只包括加性效应(传递给下一代遗传优势)。然而，在一些育种计划中，发掘显性基因和上位效应会是可取的。例如，通过选配和杂种优势生产具有最高遗传价值的(杂交)后代 (Falconer &amp; Mackay, 1996)。Gianola and van Kaam (2008) 提出非参数方法可以在不显式建模的情况下解释复杂的上位性效应。基因组选择的半参数和非参数程序，如再生核希尔伯特空间 (Gianola &amp; van Kaam, 2008)，径向基函数神经网络(Gonzalez-Camacho et al., 2012)，支持向量机(SVM) (Maenhout,DeBaets &amp; Haesaert, 2010; Moser et al., 2009)，惩罚支持向量机 ，随机森林(Ogutu, Piepho &amp; Schulz-Streeck, 2011)，boosting (Gonzalez-Recio et al.,2010)可以潜在地利用数千个标记之间的交互作用。 Heslot et al. (2012) 在植物基因组选择中比较了包括机器学习在内的10种不同的方法。]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[biopython NCBI]]></title>
    <url>%2F2018%2F11%2F08%2Fbiopython-NCBI%2F</url>
    <content type="text"><![CDATA[访问NCBI Entrez数据库Entrez (http://www.ncbi.nlm.nih.gov/Entrez) 是一个给客户提供 NCBI 各个数据库（如PubMed, GeneBank, GEO等等）访问的检索系统。 用户可以通过浏览器手动输入查询条目访问Entrez，也可以使用 Biopython 的 Bio.Entrez 模块以编程方式访问来访问 Entrez。 如果使用第二种方法，用户用一个 Python 脚本就可以实现在PubMed 里面搜索或者从 GenBank 下载数据。 1&gt; pip install Bio #安装模块 ESearch: 搜索Entrez数据库esearch 会根据参数得出所需文献、序列等的 ID 号。 检索文献： 123456&gt;&gt;&gt; from Bio import Entrez&gt;&gt;&gt; Entrez.email = "1009133184@qq.com" #Always tell NCBI who you are&gt;&gt;&gt; handle = Entrez.esearch(db="pubmed", term="biopython")&gt;&gt;&gt; record = Entrez.read(handle)&gt;&gt;&gt; record["IdList"]['19304878', '18606172', '16403221', '16377612', '14871861', '14630660', '12230038'] 检索序列： 1234567&gt;&gt;&gt; from Bio import Entrez&gt;&gt;&gt; handle = Entrez.esearch(db="nucleotide",term="Cypripedioideae[Orgn] AND matK[Gene]")&gt;&gt;&gt; record = Entrez.read(handle)&gt;&gt;&gt; record["Count"]'25'&gt;&gt;&gt; record["IdList"]['126789333', '37222967', '37222966', '37222965', ..., '61585492'] 常用db参数：pubmed nucleotide protein gene snp unigene ，默认为pubmed term参数：在检索文献时，term 就是关键词。在检索序列时，ncbi 自己有一套规则，如Cypripedioideae[Orgn] AND matK[Gene]的意思是 拖鞋兰物种中的 matK 基因序列 biomol_mrna[properties] AND Osteichthyes[organism] 只要 mRNA 序列在硬骨鱼纲中 其他关键词还有：OR NOT EFetch: 从Entrez下载数据123456789101112&gt;&gt;&gt; from Bio import Entrez&gt;&gt;&gt; Entrez.email = "1009133184@qq.com" # Always tell NCBI who you are&gt;&gt;&gt; handle = Entrez.efetch(db="nucleotide", id="186972394", rettype="fasta", retmode="text")&gt;&gt;&gt; print (handle.read())&gt;EU490707.1 Selenipedium aequinoctiale maturase K (matK) gene, partial cds; chloroplastATTTTTTACGAACCTGTGGAAATTTTTGGTTATGACAATAAATCTAGTTTAGTACTTGTGAAACGTTTAATTACTCGAATGTATCAACAGAATTTTTTGATTTCTTCGGTTAATGATTCTAACCAAAAAGGATTTTGGGGGCACAAGCATTTTTTTTCTTCTCATTTTTCTTCTCAAATGGTATCAGAAGGTTTTGGAGTCATTCTGGAAATTCCATTCTCGTCGCAATTAGTATCTTCTCTTGAAGAAAAAAAAATACCAAAATATCAGAATTTACGATCTATTCATTCAATATTTCCCTTTTTAGAAGACAAATTTTTACATTTGAATTATGTGTCAGATCTACTAATACCCCATCCCATCCATCTGGAAATCTTGGTTCAAATCCTTCAATGCCGGATCAAGGATGTTCCTTCTTTG……………… id 参数 ：ncbi 为每一条序列标识的ID号，可以使用 Entrez.esearch 获得。 rettype 参数：常用的有 fasta gb retmode 参数：数据的组织形式有 text xml 1234567891011&gt;&gt;&gt; from Bio import Entrez, SeqIO&gt;&gt;&gt; handle = Entrez.efetch(db="nucleotide", id="186972394",rettype="gb", retmode="text")&gt;&gt;&gt; record = SeqIO.read(handle, "genbank") #使用SeqIO 读入genbank序列&gt;&gt;&gt; handle.close()&gt;&gt;&gt; print (record)ID: EU490707.1Name: EU490707Description: Selenipedium aequinoctiale maturase K (matK) gene, partial cds; chloroplast.Number of features: 3...Seq('ATTTTTTACGAACCTGTGGAAATTTTTGGTTATGACAATAAATCTAGTTTAGTA...GAA', IUPACAmbiguousDNA()) EGQuery: 全局搜索- 统计搜索的条目EGQuery提供搜索字段在每个Entrez数据库中的数目。当我们只需要知道在每个数据库中能找到的条目的个数， 而不需要知道具体搜索结果的时候，这个非常的有用。 12345678910&gt;&gt;&gt; from Bio import Entrez&gt;&gt;&gt; Entrez.email = "1009133184@qq.com" # Always tell NCBI who you are&gt;&gt;&gt; handle = Entrez.egquery(term="biopython")&gt;&gt;&gt; record = Entrez.read(handle)&gt;&gt;&gt; for row in record["eGQueryResult"]: print row["DbName"], row["Count"]...pubmed 6pmc 62journals 0... 搜索，下载，和解析Entrez核酸记录获取 Cypripedioideae 在 gene 库的条目数： 12345678&gt;&gt;&gt; from Bio import Entrez&gt;&gt;&gt; Entrez.email = "A.N.Other@example.com" # Always tell NCBI who you are&gt;&gt;&gt; handle = Entrez.egquery(term="Cypripedioideae")&gt;&gt;&gt; record = Entrez.read(handle)&gt;&gt;&gt; for row in record["eGQueryResult"]:... if row["DbName"]=="gene":... print row["Count"]376 获得这376条数据的ID： 123&gt;&gt;&gt; from Bio import Entrez&gt;&gt;&gt; handle = Entrez.esearch(db="nucleotide", term="Cypripedioideae", retmax=376)&gt;&gt;&gt; record = Entrez.read(handle) 使用 efetch 来下载这些结果的前5条： 1234567&gt;&gt;&gt; idlist = ",".join(record["IdList"][:5])&gt;&gt;&gt; print (idlist)187237168,187372713,187372690,187372688,187372686&gt;&gt;&gt; handle = Entrez.efetch(db="nucleotide", id=idlist, retmode="xml")&gt;&gt;&gt; records = Entrez.read(handle) #解析xml文件&gt;&gt;&gt; print (len(records))5]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python和R中的数据合并]]></title>
    <url>%2F2018%2F10%2F31%2Fmerge-in-python-and-R%2F</url>
    <content type="text"><![CDATA[pandas（python）pandas.merge可根据一个或多个键将不同 DataFrame 中的行连接起来。 123456789101112131415161718192021222324252627282930313233import pandas as pddf1 = pd.DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],'data1': range(7)&#125;)df2 = pd.DataFrame(&#123;'key': ['a', 'b', 'd'],'data2': range(3)&#125;)df1Out[5]: data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 a6 6 bdf2Out[6]: data2 key0 0 a1 1 b2 2 dpd.merge(df1, df2)Out[7]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 注意，我并没有指明要用哪个列进行连接。如果没有指定，merge就会将重叠列的列名当做键。不过，最好明确指定一下： 123456789In [40]: pd.merge(df1, df2, on='key')Out[40]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 如果两个对象的列名不同，也可以分别进行指定： 123456789101112131415In [41]: df3 = pd.DataFrame(&#123;'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], ....: 'data1': range(7)&#125;)In [42]: df4 = pd.DataFrame(&#123;'rkey': ['a', 'b', 'd'], ....: 'data2': range(3)&#125;)In [43]: pd.merge(df3, df4, left_on='lkey', right_on='rkey')Out[43]: data1 lkey data2 rkey0 0 b 1 b1 1 b 1 b2 6 b 1 b3 2 a 0 a4 4 a 0 a5 5 a 0 a 结果里面c和d以及与之相关的数据消失了。默认情况下，merge做的是“内连接”how=&#39;outer&#39;；结果中的键是交集。其他方式还有”left”、”right”以及”outer”。 要根据多个键进行合并，传入一个由列名组成的列表即可： 12345678910111213141516In [51]: left = pd.DataFrame(&#123;'key1': ['foo', 'foo', 'bar'], ....: 'key2': ['one', 'two', 'one'], ....: 'lval': [1, 2, 3]&#125;)In [52]: right = pd.DataFrame(&#123;'key1': ['foo', 'foo', 'bar', 'bar'], ....: 'key2': ['one', 'one', 'one', 'two'], ....: 'rval': [4, 5, 6, 7]&#125;)In [53]: pd.merge(left, right, on=['key1', 'key2'], how='outer')Out[53]: key1 key2 lval rval0 foo one 1.0 4.01 foo one 1.0 5.02 foo two 2.0 NaN3 bar one 3.0 6.04 bar two NaN 7.0 dplyr（R）12345innner_join(x, y, by = "z") #内连接left_join(x, y, by = "z") #左连接right_join(x, y, by = "z") #右连接full_join(x, y, by = "z") #全连接innner_join(x, y, by = c("q","j")) #按多键连接]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SNP位点对基因的影响]]></title>
    <url>%2F2018%2F10%2F30%2FThe-effect-of-snp-on-genes%2F</url>
    <content type="text"><![CDATA[snp 是 single-nucleotide polymorphism 的简称，中文为单核苷酸多态性。snp 是如今遗传研究中最热门的一个研究对象，第一是因为他在生物中大量存在，分布广泛。第二他是引起人类（也包含其他生物）疾病的重要原因。 简单来说，snp 就是一个碱基发生了改变。snp 导致基因的转录、翻译受到影响，蛋白质序列、结构的变化使其无法实行原来特定的功能，人因此患上了某一种疾病。那么 snp 有哪些种类？是否所有的 snp 都能够造成基因的变异呢？ snp 的种类 非同义编码 SNP（non-synonymous coding SNPs，nscSNPs） 同义编码 SNP（synonymous coding SNPs，scSNPs） 内含子区 SNP 基因调控区域 SNP 不同snp对基因的影响非同义编码 SNP，顾名思义就是碱基的改变导致翻译出的蛋白质和原来不同，他会直接改变基因编码蛋白的氨基酸组成，其功能影响取决于变异氨基酸位点是否对蛋白结构或功能起到至关重要的作用。 同义编码 SNP，本身并不改变蛋白质序列，但由于蛋白质翻译存在密码子偏好性，从常用的密码子转到发生多态性之后的不常用密码子，这个过程会导致核糖体通过 SNP 周围 mRNA 片段时速度发生改变，而细胞内的蛋白质折叠过程一般被认为是与翻译过程同步进行的，因此这些 scSNP 会影响 P-gp 折叠和其转移到细胞膜的时间，因此会改变底物和抑制物的作用位点的结构 。 内含子区 SNP，内含子在真核生物基因组中占有很大比重，因此分布在内含子区域的 SNP 也很多， 但分析表明内含子区域的 SNP 的致病风险明显低于编码区和基因调控区，但位于第一个内含子的 SNP 比其他内含子中 SNP 有更大的致病风险 。与内含子相关的大多数引起疾病的突变集中在内含子—外显子连接处（GU或AG）。每个外显子和内含子连接区具有高度保守和一致的序列，即大多数内含子5’末端以GT开始，大多数内含子3’末端与AG开始，称为GT-AG规则。 基因调控区域 SNP ，基因的调控区域主要包括启动子、5’UTR 和 3’UTR，这些区域有很多基因表达调控序列元件，如转录因子结合位点（Transcription factor binding site, TFBS）、 miRNA 结合位点等。这些序列元件与调控因子（转录因子、miRNA 等）的结合都需要特定的序列组成，这些位点发生的 SNP 可能会导致调控因子的结合能力发生改变，从 而影响正常的基因表达调控事件。 深入基因调控区域的snp在 GWAS 研究中会发现绝大多数的 snp 位点都定位到了非编码区。编码区的snp位点很好理解：哪个基因编码发生改变，那对应的蛋白也不会行使正常的功能。而基因调控区的snp位点要明白到底如何影响基因功能需要花上不少精力。 上面已经说过，基因调控区域 SNP会影响转录因子结合位点、miRNA 结合位点等。基因表达模式主要是靠各种启动子和增强子来决定的，这些调控元件将各种信号因子和转录因子信息整合起来对基因表达进行调控。 转录因子通过结合在DNA的增强子或启动子区域上调节相应基因，提高或降低基因的转录水平。，如果基因上游调控区发生变异就可能影响基因的转录激活。 为了得到确切的结果，我们可以将 snp 集进行注释，注释工具主流的有 snpEFF，Annovar。最终会得到每一个 snp 的变异类型和影响区域。 如果一个 snp 对基因的表达产生了影响，那么这个 snp 可称为 eQTL位点（expression Quantitative Trait Loci,eQTL ）。可以提取样本的 RNA 进行 qRT-PCR 得到某几个候选基因的表达量作为表型，snp 集作为基因型进行 GWAS 分析。最终得到显著 snp 位点，如果显著 snp 位点恰好在候选基因周围的调控区域，那么这个 snp 位点就是一个 cis-eQTL （顺式eQTL），不在候选基因周围的调控区域，就要进一步研究是否对候选基因有作用，如果成立，那这个 snp 位点就是一个 trans-eQTL （反式eQTL）。研究发现顺式 eQTL 的数量要远多于反式 eQTL。 现在对于非编码区的碱基效应研究还没有编码区那么明了，但是以后一定会将整个基因组研究透彻。总之，一个碱基或者多个碱基的改变会从不同层面，不同量级去影响基因的表达。就像是蝴蝶效应：一个小小的改变终将会影响这个世界，条件只是时间在时间轴的运行的时间。 参考文献 [1] 许超. SNP对基因表达影响的生物信息学模型[D]. 苏州大学, 2012. [2] 卢坤, 曲存民, 李莎,等. 甘蓝型油菜BnTT3基因的表达与eQTL定位分析[J]. 作物学报, 2015, 41(11):1758-1766. [3] Chen R, Davydov E V, Sirota M, et al. Non-Synonymous and Synonymous Coding SNPs Show Similar Likelihood and Effect Size of Human Disease Association[J]. Plos One, 2010, 5(10):e13574. [4] Westra H J, Franke L. From genome to function by studying eQTLs.[J]. BBA - Molecular Basis of Disease, 2014, 1842(10):1896-1902.]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R 字符串处理]]></title>
    <url>%2F2018%2F10%2F18%2FR-stringr%2F</url>
    <content type="text"><![CDATA[使用R包stringr进行R语言的字符串处理。 str_count()计算字符串中字符个数。 1234&gt; library(stringr)&gt; fruit &lt;- c("apple", "banana", "pear", "pineapple")&gt; str_count(fruit, "a")#&gt; [1] 1 3 1 1 str_detect()检查字符是否在字符串中。 12345&gt; fruit &lt;- c("apple", "banana", "pear", "pinapple")&gt; str_detect(fruit, "a")#&gt; [1] TRUE TRUE TRUE TRUE&gt; str_detect(fruit, "^a")#&gt; [1] TRUE FALSE FALSE FALSE str_extract()模式匹配。 123456789&gt; shopping_list &lt;- c("apples x4", "bag of flour", "bag of sugar", "milk x2")&gt; str_extract_all(shopping_list, "[a-z]+\\b", simplify = TRUE) #simplify = TRUE 以矩阵的方式展示结果#&gt; [,1] [,2] [,3] #&gt; [1,] "apples" "" "" #&gt; [2,] "bag" "of" "flour"#&gt; [3,] "bag" "of" "sugar"#&gt; [4,] "milk" "" "" &gt; str_extract(shopping_list, "[a-z]+\\b")[1] "apples" "bag" "bag" "milk" str_match ()模式匹配，进行分组匹配。 12345&gt; fruit &lt;- c("apple12345679!123")&gt; str_match_all(fruit,"([a-z]+).*?(!)") #分2个组，结果第一个为全部匹配的结果[[1]] [,1] [,2] [,3][1,] "apple12345679!" "apple" "!" str_locate()模式匹配位置。 12345678910111213141516171819202122232425262728293031&gt; fruit &lt;- c("apple", "banana", "pear", "pineapple") &gt; str_locate(fruit, "a")#&gt; start end#&gt; [1,] 1 1#&gt; [2,] 2 2#&gt; [3,] 3 3#&gt; [4,] 5 5&gt; str_locate(fruit, c("a", "b", "p", "p"))#&gt; start end#&gt; [1,] 1 1#&gt; [2,] 1 1#&gt; [3,] 1 1#&gt; [4,] 1 1&gt; str_locate_all(fruit, "a")[[1]] start end[1,] 1 1[[2]] start end[1,] 2 2[2,] 4 4[3,] 6 6[[3]] start end[1,] 3 3[[4]] start end[1,] 5 5 str_subset()功能如 linux下grep。 1234567&gt; fruit &lt;- c("apple", "banana", "pear", "pinapple")&gt; str_subset(fruit, "a")#&gt; [1] "apple" "banana" "pear" "pinapple"&gt; str_subset(fruit, "^a")#&gt; [1] "apple"&gt; str_subset(fruit, "[aeiou]")#&gt; [1] "apple" "banana" "pear" "pinapple" str_replace()替换字符。 12345&gt; fruits &lt;- c("one apple", "two pears", "three bananas")&gt; str_replace(fruits, "[aeiou]", "-")#&gt; [1] "-ne apple" "tw- pears" "thr-e bananas"&gt; str_replace_all(fruits, "[aeiou]", "-")#&gt; [1] "-n- -ppl-" "tw- p--rs" "thr-- b-n-n-s" str_split()分割字符串。 12345678910111213141516171819&gt; fruits &lt;- c( "apples and oranges and pears and bananas", "pineapples and mangos and guavas")&gt; str_split(fruits, " and ")#&gt; [[1]]#&gt; [1] "apples" "oranges" "pears" "bananas" #&gt; [[2]]#&gt; [1] "pineapples" "mangos" "guavas" &gt; str_split(fruits, " and ", simplify = TRUE)#&gt; [,1] [,2] [,3] [,4] #&gt; [1,] "apples" "oranges" "pears" "bananas"#&gt; [2,] "pineapples" "mangos" "guavas" "" &gt; str_split(fruits, " and ", n = 3)#&gt; [[1]]#&gt; [1] "apples" "oranges" "pears and bananas" #&gt; [[2]]#&gt; [1] "pineapples" "mangos" "guavas" str_sort()字符串排序。 12345678910&gt; letter&lt;-c('an apple','two oranges','three bananas','four tomatoes')&gt; str_sort(letter)[1] "an apple" "four tomatoes" "three bananas" "two oranges"&gt; str_sort(letter,decreasing = TRUE)[1] "two oranges" "three bananas" "four tomatoes" "an apple"x &lt;- c("100a10", "100a5", "2b", "2a")str_sort(x)#&gt; [1] "100a10" "100a5" "2a" "2b" str_sort(x, numeric = TRUE)#&gt; [1] "2a" "2b" "100a5" "100a10" str_to_upper() str_to_lower() str_to_title()改变字符大小写。 str_length()字符串长度。 str_c()拼接字符串。 1234&gt; str_c('I have',letter,sep = ' ') #单字符串加字符[1] "I have an apple" "I have two oranges" "I have three bananas" "I have four tomatoes"&gt; str_c(letter,collapse = ',')[1] "an apple,two oranges,three bananas,four tomatoes" #拼接所有字符串]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R 数据处理]]></title>
    <url>%2F2018%2F10%2F02%2FR-data-progress%2F</url>
    <content type="text"><![CDATA[本节函数都来自于dplyr包，都可以和group_by函数联合起来处理复杂的数据。 filterfilter()函数用来根据列的具体数据选择行。 12345678910111213141516171819&gt; library(dplyr)&gt; names(mpg) "manufacturer" "model" "displ" "year" "cyl" "trans" "drv" "cty" "hwy" "fl" "class"&gt; filter(mpg,year==1999) #选择year==1999的数据# A tibble: 117 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 audi a4 1.80 1999 4 auto(l5) f 18 29 p compact 2 audi a4 1.80 1999 4 manual(m5) f 21 29 p compact 3 audi a4 2.80 1999 6 auto(l5) f 16 26 p compact&gt; filter(mpg,year==1999,displ&gt;5) #选择year==1999且displ&gt;5的数据# A tibble: 16 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 chevrolet c1500 suburban 2wd 5.70 1999 8 auto(l4) r 13 17 r suv 2 chevrolet corvette 5.70 1999 8 manual(m6) r 16 26 p 2seater 3 chevrolet corvette 5.70 1999 8 auto(l4) r 15 23 p 2seater 4 chevrolet k1500 tahoe 4wd 5.70 1999 8 auto(l4) 4 11 15 r suv 5 chevrolet k1500 tahoe 4wd 6.50 1999 8 auto(l4) 4 14 17 d suv arrangearrange()函数用来排列行的顺序。 12345678910111213141516&gt; arrange(mpg,year,cty, hwy) #根据year,cty, hwy进行行排序# A tibble: 234 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 chevrolet k1500 tahoe 4wd 5.70 1999 8 auto(l4) 4 11 15 r suv 2 dodge dakota pickup 4wd 5.20 1999 8 auto(l4) 4 11 15 r pickup 3 dodge durango 4wd 5.90 1999 8 auto(l4) 4 11 15 r suv &gt; arrange(mpg,year,cty, hwy) #根据year,cty, hwy进行行排序&gt; arrange(mpg,desc(year)) #根据year进行行降序排序# A tibble: 234 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 audi a4 2.00 2008 4 manual(m6) f 20 31 p compact 2 audi a4 2.00 2008 4 auto(av) f 21 30 p compact 3 audi a4 3.10 2008 6 auto(av) f 18 27 p compact 4 audi a4 quattro 2.00 2008 4 manual(m6) 4 20 28 p compact selectselect()函数用来选择列。 1234567&gt; select(mpg,year,cty, hwy) #只显示year,cty, hwy三列# A tibble: 234 x 3 year cty hwy &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1999 18 29 2 1999 21 29 3 2008 20 31 select()函数的辅助函数： start_with(&quot;abc&quot;) ：匹配以 “abc” 开头的变量名。 ends_with(&quot;xyz&quot;) ：以 “xyz” 结尾。 contains(&quot;ijk&quot;) ：包含 “ijk”。 match(&quot;(.)\\\1&quot;) ：正则匹配重复字符。 num_range(&quot;x&quot;,1:3) ：匹配x1，x2和x3。 renamerename()用来改变变量名。 1&gt; rename(mpg,YEAR=year) #将year重命名YEAR mutatemutate()函数用来对数据进行增加新列。 1234567891011121314&gt; mutate(mpg,cty2=cty*2) #数值型# A tibble: 234 x 12 manufacturer model displ year cyl trans drv cty hwy fl class cty2 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 audi a4 1.80 1999 4 auto(l5) f 18 29 p compact 36. 2 audi a4 1.80 1999 4 manual(m5) f 21 29 p compact 42. 3 audi a4 2.00 2008 4 manual(m6) f 20 31 p compact 40.&gt; mutate(mpg,cty2=paste(drv,cty,sep = "")) #字符型# A tibble: 234 x 12 manufacturer model displ year cyl trans drv cty hwy fl class cty2 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 audi a4 1.80 1999 4 auto(l5) f 18 29 p compact f18 2 audi a4 1.80 1999 4 manual(m5) f 21 29 p compact f21 3 audi a4 2.00 2008 4 manual(m6) f 20 31 p compact f20]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R 分面]]></title>
    <url>%2F2018%2F10%2F02%2FR-facet%2F</url>
    <content type="text"><![CDATA[有时候会有需求需要把一个数据框的数据作图，再按照不同的分类将图形分开绘制。或者，只是需要把2个或者多个图简单地放在一个画布上，R图的分面将会作用与此。 facet_gridggplot2的facet_grid()函数按分类条件将图形在行或者列上分面绘图。 1234567&gt; p &lt;- ggplot(mpg, aes(displ, cty)) + geom_point()&gt; p + facet_grid(rows = vars(drv)) #按行分面&gt; p + facet_grid(cols = vars(cyl)) #按列分面&gt; p + facet_grid(vars(drv), vars(cyl)) #使用2个变量同时在行列分面&gt; mt &lt;- ggplot(mtcars, aes(mpg, wt, colour = factor(cyl))) +geom_point() #按cyl填充颜色&gt; mt + facet_grid(cols = vars(cyl), scales = "free") #scales = "free"刻度在在分面上可自适应 facet_wrapggplot2的facet_wrap()函数按分类条件将图形2d(默认n x n)形式的铺在画布上。 123456&gt; p &lt;- ggplot(mpg, aes(displ, hwy)) + geom_point()&gt; p + facet_wrap(vars(class)) #按class分面&gt; p + facet_wrap(vars(class), nrow = 4) #强制分为4行&gt; p + facet_wrap(vars(class), ncol = 4) #强制分为4列&gt; p + facet_wrap(vars(cyl, drv)) #使用2个变量进行分类&gt; p + facet_wrap(c("cyl", "drv"), labeller = "label_both") #友好的显示分类变量名 grid.arrangegrid.arrange()是gridExtra包的一个函数，可以将多个图放入一个画布中。 12345678&gt; install.packages("gridExtra")&gt; library(gridExtra)&gt; p1&lt;-ggplot(mpg, aes(displ, hwy)) + geom_point()&gt; p2&lt;-ggplot(mpg, aes(displ, hwy)) + geom_point()&gt; p3&lt;-ggplot(mpg, aes(displ, hwy)) + geom_point()&gt; grid.arrange(p1,p2,p3, nrow=2) #设定为2行图形&gt; grid.arrange(p1,p2,p3, nrow=2,top = textGrob("xxx",gp=gpar(col="red",fontsize=20，font=2))) #设置总标题，并修改颜色大小，字体]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R 回归分析]]></title>
    <url>%2F2018%2F09%2F29%2FR-regression-analysis%2F</url>
    <content type="text"><![CDATA[逐步回归逐步回归分析是以AIC信息统计量为准则，通过选择最小的AIC信息统计量，来达到删除或增加变量的目的。 AIC : 赤池信息准则(Akaike Information Criterion) ,k是模型中估计参数的数量。L是模型的似然函数的最大值。 AICc：当样本量很小时，AIC很可能会选择具有太多参数的模型，即AIC会过度拟合。为了解决这种潜在的过度拟合问题，AICc可以对小样本进行校正。其中n表示样本大小，k表示参数的数量。 向前逐步回归首先模型中只有一个单独解释因变量变异最大的自变量，之后尝试将加入另一自变量，看加入后整个模型所能解释的因变量变异是否显著增加（这里需要进行检验，可以用 F-test， t-test 等等）。这一过程反复迭代，直到没有自变量再符合加入模型的条件。 MASS包中的stepAIC()函数给予AIC准则实现了逐步回归模型（向前、向后和双向）。 123&gt; library(MASS)&gt; fit &lt;- lm(xx ~ x+y+z,data=datas)&gt; stepAIC(fit, direction="forward") 向后逐步回归向后逐步回归与向前逐步回归相反，此时，所有变量均放入模型，之后尝试将其中一个自变量从模型中剔除，看整个模型解释因变量的变异是否有显著变化，之后将使解释量减少最少的变量剔除；此过程不断迭代，直到没有自变量符合剔除的条件。 123&gt; library(MASS)&gt; fit &lt;- lm(xx ~ x+y+z,data=datas)&gt; stepAIC(fit, direction="backward") 向前向后逐步回归向前向后逐步回归，这种方法相当于将前两种结合起来。可以想象，如果采用第一种方法，每加入一个自变量，可能会使已存在于模型中的变量单独对因变量的解释度减小，当其的作用很小（不显著）时，则可将其从模型中剔除。而第三种方法就做了这么一件事，不是一味的增加变量，而是增加一个后，对整个模型中的所有变量进行检验，剔除作用不显著的变量。最终尽可能得到一个最优的变量组合。 123&gt; library(MASS)&gt; fit &lt;- lm(xx ~ x+y+z,data=datas)&gt; stepAIC(fit, direction="both") Forward、Backward、Stepwise的侧重点有所不同，三种方法的选择取决于你的研究目的，如果是进行预测，在预测效果差不多的情况下，一般选择自变量最少的方法。当自变量间不存在多重共线性时，三种方法的计算结果基本一致。当自变量间存在多重共线性时，Forward侧重于引入单独作用较强的变量，Backward侧重于引入联合作用较强的变量，Stepwise介于两者之间。 全子集回归全子集回归克服了逐步回归的缺点，即所有可能的模型都会被检验，评判准则可以是R平方、修正R平方、BIC或者Mallows Cp统计量。 R平方： 修正R平方 : 我们知道在其他变量不变的情况下，引入新的变量，总能提高模型的R2。修正R2就是相当于给变量的个数加惩罚项。换句话说，如果两个模型，样本数一样，R2一样，那么从修正R2的角度看，使用变量个数少的那个模型更优。其中n是样本数量，p是模型中变量的个数。 当p/n值很小时，如小于0.05，修正R平方将失去修正作用。 BIC：贝叶斯信息准则 ，BIC的惩罚项比AIC的大，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高。 Mallows Cp：马洛斯的Cp值 与AIC等效。 以下为R中ISLR包的Hitters数据集为例，构建棒球运动员的多元线性模型 。 12345678910111213141516171819202122232425&gt; library(ISLR)&gt; library(leaps) #使用leaps做全子集回归&gt; Hitters &lt;- na.omit(Hitters)&gt; dim(Hitters) #除去Salary做为因变量，还剩下19个特征[1] 263 20&gt; regfit.full = regsubsets(Salary~.,Hitters,nvmax = 19) #选择最大19个特征的全子集选择模型&gt; reg.summary = summary(regfit.full) # 可看到不同数量下的特征选择&gt; plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted R2",type = "l")&gt; points(which.max(reg.summary$adjr2),reg.summary$adjr2[11],col="red",cex=2,pch=20) #11个特征时，Adjusted R2最大&gt; plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type = "l")&gt; points(which.min(reg.summary$cp),reg.summary$cp[10],col="red",cex=2,pch=20) # 10个特征时，Cp最小&gt; plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type = "l")&gt; points(which.min(reg.summary$bic),reg.summary$bic[6],col="red",cex=2,pch=20) # 6个特征时，BIC最小&gt; plot(regfit.full,scale = "r2") #特征越多，R2越大，这不意外，默认scale是bic。&gt; plot(regfit.full,scale = "adjr2") #下图&gt; coef(regfit.full,11) #查看模型的系数 (Intercept) AtBat Hits Walks CAtBat 135.7512195 -2.1277482 6.9236994 5.6202755 -0.1389914 CRuns CRBI CWalks LeagueN DivisionW 1.4553310 0.7852528 -0.8228559 43.1116152 -111.1460252 PutOuts Assists 0.2894087 0.2688277 交叉验证交叉验证是在机器学习建立模型和验证模型参数时常用的办法，一般被用于评估一个机器学习模型的表现。更多的情况下，也用交叉验证来进行模型选择(model selection)。 k重交叉验证中，样本被分为k个子样本，轮流将k-1个子样本组合作为训练集，另外1个子样本作为测试集，这样会获得k个预测方程，记录k个测试样本的预测表现结果，然后求其平均值。测试集的目的简单来说就相当于一个游戏的内测，内部评估。进行交叉验证后得到了分数来评估你建模的准确率是高是低。最后的目的是为了哪天来了新的数据，你也可以用你的模型去预测他，相当于游戏公测。 bootstrap包中的crossval() 函数可实现k重交叉验证 ： 1234567891011121314151617181920212223&gt; install.packages("bootstrap") &gt; library(bootstrap) &gt; shrinkage&lt;-function(fit,k=10)&#123; require(bootstrap) theta.fit&lt;-function(x,y)&#123;lsfit(x,y)&#125; theta.predict&lt;-function(fit,x)&#123;cbind(1,x)%*%fit$coef&#125; x&lt;-fit$model[,2:ncol(fit$model)] y&lt;-fit$model[,1] results&lt;-crossval(x,y,theta.fit,theta.predict,ngroup=k) r2&lt;-cor(y,fit$fitted.values)^2 r2cv&lt;-cor(y,results$cv.fit)^2 cat("Original R-square=",r2,"n") cat(k,"Fold Cross-Validated R-square=",r2cv,"n") cat("Change=",r2-r2cv,"n") &#125; &gt; fit&lt;-lm(Murder ~ Population+Income+Illiteracy+Frost,data=states) &gt; shrinkage(fit) &gt; fit2&lt;-lm(Murder ~ Population+Illiteracy,data=states) &gt; shrinkage(fit2) Original R-square=0.566832710 Fold Cross-Validated R-square=0.5193801Change=0.04745256]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R 数据预处理]]></title>
    <url>%2F2018%2F09%2F28%2Fdeal-with-r-data%2F</url>
    <content type="text"><![CDATA[将若干向量组合为数据框12345678&gt; g &lt;- c("a","b","c")&gt; x &lt;- 1:3&gt; data &lt;- data.frame(g,x)&gt; data g x1 a 12 b 23 c 3 从数据框中提取摘要信息1234&gt; str(data)'data.frame': 3 obs. of 2 variables: $ g: Factor w/ 3 levels "a","b","c": 1 2 3 $ x: int 1 2 3 向数据框添加列12345&gt; data$new &lt;- data$x * 2 g x new1 a 1 22 b 2 43 c 3 6 从数据框删除列12345&gt; data &lt;- subset(data,select = c(-x,-new)) g1 a2 b3 c 重排序列12345&gt; data2 &lt;-data[c(1,3,2)] g new x1 a 2 12 b 4 23 c 6 3 选取某几列12345&gt; data3 &lt;-data[c("x","new")] x new1 1 22 2 43 3 6 选取某几行1234&gt; data3 &lt;-data[c(1,2),] g x new1 a 1 22 b 2 unite 4 连续变量转换为分类变量12345&gt; data$class &lt;- cut(data$new,breaks=c(0,4,8,Inf)) g x new class1 a 1 2 (0,4]2 b 2 4 (0,4]3 c 3 6 (4,8] 宽数据《=》长数据长数据有一列数据是变量的类型，有一列是变量的值，但不一定只有两列。ggplot2需要长类型的数据，dplyr也需要长类型的数据，大多数的模型(比如lm(), glm()以及gam())也需要长数据。 使用tidyr 包的gather()函数转换到长数据： 123456789101112131415&gt; library(tidyr)&gt; data g x new1 a 1 22 b 2 43 c 3 6&gt; data2&lt;-gather(data,key='new_one',value ='count',x,new,-g)&gt; data2 g new_one count1 a x 12 b x 23 c x 34 a new 25 b new 46 c new 6 使用tidyr 包的spread()函数转换到宽数据： 12345&gt; spread(data2,key='new_one',value='count') g x new1 a 1 22 b 2 43 c 3 6 多列《=》一列123456789101112131415161718&gt; data g x new1 a 1 22 b 2 43 c 3 6&gt; data3&lt;-unite(data,x_new,x,new,sep = '_')&gt; data3 g x_new1 a 1_22 b 2_43 c 3_6&gt; data4&lt;- separate(data3,x_new,c('x','new'),sep = '_')&gt; data4 g x new1 a 1 22 b 2 43 c 3 6]]></content>
      <categories>
        <category>R</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python Type Hint]]></title>
    <url>%2F2018%2F09%2F19%2Fpython-Type-Hint%2F</url>
    <content type="text"><![CDATA[引入​ 在C语言中每使用一个变量前要先将其声明，如 int a =1,float b =3.14。这么做的好处有2个，一是在之后代码中无需“猜测”变量到底是什么类型，声明是什么就是什么，这就是静态变量。二是少了“猜测”这一步，代码执行效率会有所提升。python默认所有的变量都是动态的，所以你无需提前声明类型，写出了一个变量a，赋值给他一个常数、列表、字典等等都可以。当然动态所带来的问题就是，在代码执行的时候需要去“猜测”变量的类型，从而降低了运行效率。 ​ 在python3.5及之后版本，python加入了模块typing，它允许在你命名变量的时候设定它的类型，抽象出来它长这个样子：variable : type，在设定一个变量类型为常量（int）之后并不会影响你给他赋值为字符串（str），这种类型设定如名字一样是类型暗示（Type Hint），而不是决定。 ​ 有一个名为mypy的包可以显式的帮你找出你在类型上的问题： 12pip install mypy$ mypy program.py 介绍可供使用的类型： Type Description int integer float floating point number bool boolean value str string (unicode) bytes 8-bit string object an arbitrary object (object is the common base class) List[str] list of str objects Tuple[int, int] tuple of two int objects (Tuple[()] is the empty tuple) Tuple[int, ...] tuple of an arbitrary number of int objects Dict[str, int] dictionary from str keys to int values Iterable[int] iterable object containing ints Sequence[bool] sequence of booleans (read-only) Mapping[str, int] mapping from str keys to int values (read-only) Any dynamically typed value with an arbitrary type Union[T1, ..., Tn] Union[int, str]both integers and strings are valid argument values. 方法12345678910111213141516171819202122from typing import List, Set, Dict, Tuple, Optional, Union, Anyx: int = 1x: float = 1.0x: bool = Truex: str = "test"x: bytes = b"test"x: List[int] = [1]x: Set[int] = &#123;6, 7&#125;x: Dict[str, float] = &#123;'field': 2.0&#125;x: Tuple[int, str, float] = (3, "yes", 7.5)x: List[Union[int, str]] = [3, 5, "test", "fun"] def plus(num1: int, num2: float) -&gt; float: return num1 + num2def f(x: Union[int, str]) -&gt; Any: if isinstance(x, int): # Here type of x is int. return (x + 1) # OK else: # Here type of x is str. return (x + 'a') # OK 123456def join(string_list): #定义一个方法，不做类型设定 return ', '.join(string_list)&gt;&gt;&gt; join('hello')'h, e, l, l, o'&gt;&gt;&gt; join(['hello','world'])'hello, world' 对于这个函数，期望的结果就是会把[‘hello’, ‘world’]变成’hello, world’。 但是如果不小心没有传list而是传了一个字符串’hello’，这段代码也不会报错，只是会返回’h, e, l, l, o’这个并不期望的结果。 1234567891011from typing import Listdef join(string_list: List[str]) -&gt; str: return ', '.join(string_list) #string_list 局部变量名， List[str] 局部变量类型， str 返回结果类型&gt;&gt;&gt; join('hello') #虽然不期待，但是仍可以运行'h, e, l, l, o'&gt;&gt;&gt; join(['hello','world'])'hello, world'&gt;&gt;&gt; join.__annotations__&#123;'string_list': typing.List[str], 'return': &lt;class 'str'&gt;&#125; 这样声明函数有一个好处，就是不需要在注释里面说明变量类型，更加直观。Python把这种类型的声明看成是一种对函数的注解(annotation)，而注解本身并不具有任何的意义，也不影响运行的过程。与没有注解的版本差别就是多了一个annotations的字段 。 123456from typing import Listdef join(string_list: List[str]) -&gt; str: return ', '.join(string_list)print(join('hello')) 运行以及使用mypy检查，代码可以运行，mypy也可以给出提示 1234$ python3 test_mypy.pyh, e, l, l, o$ mypy test_mypy.pytest_mypy.py:6: error: Argument 1 to "join" has incompatible type "str"; expected List[str]]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas和R中的groupby]]></title>
    <url>%2F2018%2F09%2F18%2Fgroupby-in-Python-and-R%2F</url>
    <content type="text"><![CDATA[groupby 可以根据一个数据框的部分数据，将整个数据框进行分组，简称：聚合。聚合之后，可以按组进行对数据进行统计分析。 pandas12345678910111213In [10]: df = pd.DataFrame(&#123;'key1' : ['a', 'a', 'b', 'b', 'a'], ....: 'key2' : ['one', 'two', 'one', 'two', 'one'], ....: 'data1' : np.random.randn(5), ....: 'data2' : np.random.randn(5)&#125;)In [11]: df #建立一个Dataframe数据框Out[11]: data1 data2 key1 key20 -0.204708 1.393406 a one1 0.478943 0.092908 a two2 -0.519439 0.281746 b one3 -0.555730 0.769023 b two4 1.965781 1.246435 a one 1234In [12]: grouped = df.groupby('key1') #按key1进行分组In [13]: grouped #得到的grouped一个简单处理的可迭代对象（GroupBy），实际上还没有进行任何计算Out[13]: &lt;pandas.core.groupby.DataframeGroupBy object at 0x0000000000FF07EB8&gt; 123456In [14]: grouped.mean() #调用GroupBy的mean方法来计算分组平均值：Out[14]: data1 data2key1a 0.114474 0.714913b 0.300820 -1.160967 12345678910In [15]: means = df.groupby(['key1', 'key2']).mean()['data1'] #按key1和key2分组，选出data1数据In [16]: meansOut[16]: key1 key2a one 0.880536 two 0.478943b one -0.519439 two -0.555730Name: data1, dtype: float64 1234567891011121314In [35]: people = pd.DataFrame(np.random.randn(5, 5), ....: columns=['a', 'b', 'c', 'd', 'e'], ....: index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])In [36]: people.iloc[2:3, [1, 2]] = np.nan # Add a few NA valuesIn [37]: people #另建一个数据框，拥有index名Out[37]: a b c d eJoe 1.007189 -1.296221 0.274992 0.228913 1.352917Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570Wes -0.539741 -2.001001 -2.001002 -1.021228 -0.577087Jim 0.124121 0.302614 0.523772 0.000940 1.343810Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 123456In [44]: people.groupby(len).sum() #用python函数分组将使用indexOut[44]: a b c d e3 0.591569 -0.993608 0.798764 -0.791374 2.1196395 0.886429 -2.001637 -0.371843 1.669025 -0.4385706 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 聚合运算方法 12345678In [54]: def peak_to_peak(arr): ....: return arr.max() - arr.min()In [55]: grouped.agg(peak_to_peak) #使用自己的聚合函数，将其传入aggregate或agg方法Out[55]: data1 data2key1 a 2.170488 1.300498b 0.036292 0.487276 1234567In [69]: grouped.agg(['mean','size']) #使用多个统计方法In [69]: ftuples = [('fuc1', 'mean'),('fuc2', np.var)] #使用自定义的名字In [70]: grouped.agg(ftuples)In [71]: grouped.agg(&#123;'data1' : ['min', 'max', 'mean', 'std'], #不同列可以指定不同的统计方法 ....: 'data2' : ['sum']&#125;) R123456789101112131415161718192021222324252627library(dplyr) #group_by和summarise都在dplyr包中data &lt;- data.frame(year = rep(2016:2017,6),month = seq(1:12),sales=rep(c(10,20,30,40),3)) year month sales1 2016 1 102 2017 2 203 2016 3 304 2017 4 405 2016 5 106 2017 6 207 2016 7 308 2017 8 409 2016 9 1010 2017 10 2011 2016 11 3012 2017 12 40planes &lt;- group_by(data, year) #按年分组delay &lt;- summarise(planes, count = n(), #个数 max_mon = max(month), #最大值 min_mon = min(month), #最小值 avg_sales = mean(sales), #平均值 sum_sales = sum(sales)) #求和# A tibble: 2 x 6 year count max_mon min_mon avg_sales sum_sales &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;1 2016 6 11 1 20 1202 2017 6 12 2 30 180 常用的摘要函数123mean() sd() min() max() first() last() n() sum()median() #中位数quantile() #分位数，quantile(x，0.25)将会找出x从小到大排列，在25%时的数 聚合函数和逻辑筛选结合12345678910delay &lt;- summarise(planes, avg_sales_1 = mean(sales,na.rm=TRUE), #na.rm=TRUE,遇到NA值时不处理，因为统计时R默认NA值会传播，python不会 avg_sales_2 = mean(sales[sales&gt;10],na.rm=TRUE) #加入逻辑判断planes &lt;- group_by(data, year) #按年分组# A tibble: 2 x 3 year avg_sales_1 avg_sales_2 &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;1 2016 20. 30.2 2017 30. 30. 123456In [53]: grouped = df[df.data1&gt;0].groupby('key1') #逻辑筛选后按key1进行分组Out [53]: data1 data2key1a 0.679126 1.013678b 0.300820 -1.160967]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python标准库学习（二）]]></title>
    <url>%2F2018%2F09%2F18%2Fpython-standard-library-2%2F</url>
    <content type="text"><![CDATA[可迭代对象与迭代器可迭代对象与迭代器概念不同。可迭代对象在内部实现了__iter__，所以可以进行迭代，迭代器在内部实现了__iter__和__next__,所以是个迭代器也可以进行迭代。 举例： 1234&gt;&gt;&gt; mylist = [x for x in range(10)] #mylist是个列表，可以进行迭代&gt;&gt;&gt; mylist = （x for x in range(10)） #生成器表达式，结果同iter(mylist) #现在mylist是个迭代器，可以使用.next()方法 生成器生成器本质就是一个迭代器，自带了iter方法和next方法。迭代器是用来迭代可迭代对象的，而生成器是用来迭代方法的。调用函数的之后函数不执行，返回一个生成器每次调用next方法的时候会取到一个值直到取完最后一个，再执行next会报错。 生成器表达式类似于列表推导，但是，生成器返回按需产生结果的一个对象，而不是一次构建一个结果列表。 比较： 列表表达式在生成后可以按序列取值，排序，切片等，但是占用内存大。 生成器表达式只是产生一个可迭代对象，需要时再迭代取值，占用内存小，但是不可进行列表的操作，迭代完后对象清空。 itertoolsitertools.product产生多个列表和迭代器的笛卡尔积，可以用product来改写深度嵌套的列表推导操作。 1234&gt;&gt;&gt; x = itertools.product('ABC', range(3))&gt;&gt;&gt;&gt;&gt;&gt; print(list(x))[('A', 0), ('A', 1), ('A', 2), ('B', 0), ('B', 1), ('B', 2), ('C', 0), ('C', 1), ('C', 2)] itertools.accumulate简单来说就是累加。 1234&gt;&gt;&gt; import itertools&gt;&gt;&gt; x = itertools.accumulate(range(10))&gt;&gt;&gt; print(list(x))[0, 1, 3, 6, 10, 15, 21, 28, 36, 45] itertools.chain连接多个列表或者迭代器。 123&gt;&gt;&gt; x = itertools.chain(range(3), range(4), [3,2,1])&gt;&gt;&gt; print(list(x))[0, 1, 2, 0, 1, 2, 3, 3, 2, 1] itertools.combinations求列表或生成器中指定数目的元素不重复的所有组合 123&gt;&gt;&gt; x = itertools.combinations(range(4), 3)&gt;&gt;&gt; print(list(x))[(0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)] itertools.combinations_with_replacement允许重复元素的组合 123&gt;&gt;&gt; x = itertools.combinations_with_replacement('ABC', 2)&gt;&gt;&gt; print(list(x))[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')] itertools.count就是一个计数器,可以指定起始位置和步长 123&gt;&gt;&gt; x = itertools.count(start=20, step=-1)&gt;&gt;&gt; print(list(itertools.islice(x, 0, 10, 1)))[20, 19, 18, 17, 16, 15, 14, 13, 12, 11] itertools.cycle循环指定的列表和迭代器 123&gt;&gt;&gt; x = itertools.cycle('ABC')&gt;&gt;&gt; print(list(itertools.islice(x, 0, 10, 1)))['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'] itertools.dropwhile按照真值函数丢弃掉列表和迭代器前面的元素 123&gt;&gt;&gt; x = itertools.dropwhile(lambda e: e &lt; 5, range(10))&gt;&gt;&gt; print(list(x))[5, 6, 7, 8, 9] itertools.takewhile与dropwhile相反，保留元素直至真值函数值为假。 123&gt;&gt;&gt; x = itertools.takewhile(lambda e: e &lt; 5, range(10))&gt;&gt;&gt; print(list(x))[0, 1, 2, 3, 4] itertools.filterfalse保留对应真值为False的元素 123&gt;&gt;&gt; x = itertools.filterfalse(lambda e: e &lt; 5, (1, 5, 3, 6, 9, 4))&gt;&gt;&gt; print(list(x))[5, 6, 9] itertools.groupby按照分组函数的值对元素进行分组 123456&gt;&gt;&gt; x = itertools.groupby(range(10), lambda x: x &lt; 5 or x &gt; 8) &gt;&gt;&gt; for condition, numbers in x: ... print(condition, list(numbers)) True [0, 1, 2, 3, 4] False [5, 6, 7, 8] True [9] itertools.islice上文使用过的函数，对迭代器进行切片 123&gt;&gt;&gt; x = itertools.islice(range(10), 0, 9, 2)&gt;&gt;&gt; print(list(x))[0, 2, 4, 6, 8] itertools.repeat简单的生成一个拥有指定数目元素的迭代器 123&gt;&gt;&gt; x = itertools.repeat(0, 5)&gt;&gt;&gt; print(list(x))[0, 0, 0, 0, 0] itertools.zip_longest类似于zip，不过已较长的列表和迭代器的长度为准 123456&gt;&gt;&gt; x = itertools.zip_longest(range(3), range(5))&gt;&gt;&gt; y = zip(range(3), range(5))&gt;&gt;&gt; print(list(x))[(0, 0), (1, 1), (2, 2), (None, 3), (None, 4)]&gt;&gt;&gt; print(list(y))[(0, 0), (1, 1), (2, 2)]]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[seqkit 使用说明]]></title>
    <url>%2F2018%2F09%2F13%2Fseqkit-usage%2F</url>
    <content type="text"><![CDATA[seqkit 是 Wei Shen 使用 go 语言编写处理 fa 和 fq 文件的一把利器，当前介绍版本为0.10.1。这里不详细介绍各个函数的参数，官方给出的文档已经足够。 软件地址：https://github.com/shenwei356/seqkit 123456789101112131415161718192021222324252627282930Available Commands: common find common sequences of multiple files by id/name/sequence concat concatenate sequences with same ID from multiple files convert convert FASTQ quality encoding between Sanger, Solexa and Illumina duplicate duplicate sequences N times faidx create FASTA index file and extract subsequence fq2fa convert FASTQ to FASTA fx2tab convert FASTA/Q to tabular format (with length/GC content/GC skew) genautocomplete generate shell autocompletion script grep search sequences by ID/name/sequence/sequence motifs, mismatch allowed head print first N FASTA/Q records help Help about any command locate locate subsequences/motifs, mismatch allowed range print FASTA/Q records in a range (start:end) rename rename duplicated IDs replace replace name/sequence by regular expression restart reset start position for circular genome rmdup remove duplicated sequences by id/name/sequence sample sample sequences by number or proportion seq transform sequences (revserse, complement, extract ID...) shuffle shuffle sequences sliding sliding sequences, circular genome supported sort sort sequences by id/name/sequence/length split split sequences into files by id/seq region/size/parts (mainly for FASTA) split2 split sequences into files by size/parts (FASTA, PE/SE FASTQ) stats simple statistics of FASTA/Q files subseq get subsequences by region/gtf/bed, including flanking sequences tab2fx convert tabular format to FASTA/Q format translate translate DNA/RNA to protein sequence version print version information and check for update seq12345678910$ seqkit seq hairpin.fa.gz #展示fa文件&gt;cel-let-7 MI0000001 Caenorhabditis elegans let-7 stem-loopUACACUGUGGAUCCGGUGAGGUAGUAGGUUGUAUAGUUUGGAAUAUUACCACCGGUGAACUAUGCAAUUUUCUACCUUACCGGAGACAGAACUCUUCGA$ seqkit seq read_1.fq.gz #展示fq文件@HWI-D00523:240:HF3WGBCXX:1:1101:2574:2226 1:N:0:CTGTAGTGAGGAATATTGGTCAATGGGCGCGAGCCTGAACCAGCCAAGTAGCGTGAAGGATGACTGCCCTACGGG+HIHIIIIIHIIHGHHIHHIIIIIIIIIIIIIIIHHIIIIIHHIHIIIIIGIHIIIIHHHHHHGHIHIII 12345678910111213141516171819202122$ seqkit seq hairpin.fa.gz -n #展示序列全名cel-let-7 MI0000001 Caenorhabditis elegans let-7 stem-loopcel-lin-4 MI0000002 Caenorhabditis elegans lin-4 stem-loopcel-mir-1 MI0000003 Caenorhabditis elegans miR-1 stem-loop$ seqkit seq hairpin.fa.gz -n -i #展示序列IDcel-let-7cel-lin-4cel-mir-1$ seqkit seq hairpin.fa.gz -n -i --id-regexp "^[^\s]+\s([^\s]+)\s" #使用正则匹配序列名MI0000001MI0000002MI0000003$ seqkit seq hairpin.fa.gz -s -w 0 #只展示序列 并设置每行碱基数为默认UACACUGUGGAUCCGGUGAGGUAGUAGGUUGUAUAGUUUGGAAUAUUACCACCGGUGAACUAUGCAAUUUUCUACCUUACCGGAGACAGAACUCUUCGAAUGCUUCCGGCCUGUUCCCUGAGACCUCAAGUGUGAGUGUACUAUUGAUGCUUCACACCUGGGCUCUCCGGGUACCAGGACGGUUUGAGCAGAUAAAGUGACCGUACCGAGCUGCAUACUUCCUUACAUGCCCAUACUAUAUCAUAAAUGGAUAUGGAAUGUAAAGAAGUAUGUAGAACGGGGUGGUAGU 1234567891011121314$ seqkit seq hairpin.fa.gz -m 50 -M 150 #过滤fq文件，使序列长度在50-150bp之间。$ seqkit seq hairpin.fa.gz -r -p #反转录序列&gt;cel-let-7 MI0000001 Caenorhabditis elegans let-7 stem-loopUCGAAGAGUUCUGUCUCCGGUAAGGUAGAAAAUUGCAUAGUUCACCGGUGGUAAUAUUCCAAACUAUACAACCUACUACCUCACCGGAUCCACAGUGUA$ echo -e "&gt;seq\nACGT-actgc-ACC" | seqkit seq -g -u #去除序列gap 并大写碱基&gt;seqACGTACTGCACC$ echo -e "&gt;seq\nUCAUAUGCUUGUCUCAAAGAUUA" | seqkit seq --rna2dna #DNA转RNA,--dna2rna亦可&gt;seqTCATATGCTTGTCTCAAAGATTA subseq1234567891011121314151617181920212223242526272829$ zcat hairpin.fa.gz | seqkit subseq -r 1:12 #展示序列前12个碱基$ zcat hairpin.fa.gz | seqkit subseq -r -12:-1 #后12个$ cat t.fa&gt;seqactgACTGactgn$ cat t.gtf #注意gtf文件格式，必须以\t分割。seq test CDS 5 8 . . . gene_id "A"; transcript_id "";seq test CDS 5 8 . - . gene_id "B"; transcript_id "";$ seqkit subseq --gtf t.gtf t.fa #使用gtf位置信息，挑选fa序列&gt;seq_5:8:. AACTG&gt;seq_5:8:- BCAGT$ seqkit subseq --gtf Homo_sapiens.GRCh38.84.gtf.gz --chr 1 --feature cds hsa.fa &gt; chr1.gtf.cds.fa #指定染色体和特征$ seqkit subseq --gtf t.gtf t.fa -u 3 #另加3bp上游序列&gt;seq_5:8:._us:3 ActgACTG&gt;seq_5:8:-_us:3 BagtCAGT$ seqkit subseq --gtf t.gtf t.fa -u 3 -f #只取上游3bp序列&gt;seq_5:8:._usf:3 Actg&gt;seq_5:8:-_usf:3 Bagt gff3 文件第九列格式为ID=XXXXX; gtf 文件第九列格式为 gene_id “A”; transcript_id “” sample12seqkit sample -p 0.1 -o sample.fq.gz #取序列文件的百分之十seqkit sample -n 1000 -o sample.fq.gz #取1000条序列 shuffle12345seqkit shuffle hairpin.fq.gz &gt; shuffled.fq #打乱序列顺序[INFO] read sequences ...[INFO] 28645 sequences loaded[INFO] shuffle ...[INFO] output ... stats12345678910111213$ seqkit stats *.f&#123;a,q&#125;.gz #统计序列信息file format type num_seqs sum_len min_len avg_len max_lenhairpin.fa.gz FASTA RNA 28,645 2,949,871 39 103 2,354mature.fa.gz FASTA RNA 35,828 781,222 15 21.8 34reads_1.fq.gz FASTQ DNA 2,500 567,516 226 227 229reads_2.fq.gz FASTQ DNA 2,500 560,002 223 224 225$ seqkit stats *.f&#123;a,q&#125;.gz -a #列出所有统计结果file format type num_seqs sum_len min_len avg_len max_len Q1 Q2 Q3 sum_gap N50 Q20(%) Q30(%)hairpin.fa.gz FASTA RNA 28,645 2,949,871 39 103 2,354 76 91 111 0 101 0 0mature.fa.gz FASTA RNA 35,828 781,222 15 21.8 34 21 22 22 0 22 0 0Illimina1.8.fq.gz FASTQ DNA 10,000 1,500,000 150 150 150 150 150 150 0 150 96.16 89.71reads_1.fq.gz FASTQ DNA 2,500 567,516 226 227 229 227 227 227 0 227 91.24 86.62reads_2.fq.gz FASTQ DNA 2,500 560,002 223 224 225 224 224 224 0 224 91.06 87.66 faidx1234567$ seqkit faidx hairpin.fa #建立序列索引&gt;hsa-let-7a-1UGGGAUGAGGUAGUAGGUUGUAUAGUUUUAGGGUCACACCCACCACUGGGAGAUAACUAUACAAUCUACUGUCUUUCCUA&gt;hsa-let-7a-2AGGUUGAGGUAGUAGGUUGUAUAGUUUAGAAUUACAUCAAGGGAGAUAACUGUACAGCCUCCUAGCUUUCCU fq2fa1$ seqkit fq2fa reads_1.fq.gz -o reads1_.fa.gz #fq转fa convert123456789101112131415161718192021$ seqkit head -n 1 tests/Illimina1.8.fq.gz...#AAAFAAJFFFJJJ&lt;JJJJJFFFJFJJJJJFJJAJJJFJJFJFJJJJFAFJ&lt;JA&lt;FFJ7FJJFJJAAJJJJ&lt;JJJJJJJFJJJAJJJJJFJJ77&lt;JJJJ-F7A-FJFFJJJJJJ&lt;FFJ-&lt;7FJJJFJJ)A7)7AA&lt;7--)&lt;-7F-A7FA&lt;$ seqkit convert tests/Illimina1.8.fq.gz | seqkit head -n 1 #默认转换fq文件质量值到1.8+[INFO] possible quality encodings: [Illumina-1.8+][INFO] guessed quality encoding: Illumina-1.8+[INFO] converting Illumina-1.8+ -&gt; Sanger[WARN] source and target quality encoding match....#AAAFAAJFFFJJJ&lt;JJJJJFFFJFJJJJJFJJAJJJFJJFJFJJJJFAFJ&lt;JA&lt;FFJ7FJJFJJAAJJJJ&lt;JJJJJJJFJJJAJJJJJFJJ77&lt;JJJJ-F7A-FJFFJJJJJJ&lt;FFJ-&lt;7FJJJFJJ)A7)7AA&lt;7--)&lt;-7F-A7FA&lt;$ seqkit convert tests/Illimina1.8.fq.gz --to Illumina-1.5+ | seqkit head -n 1[INFO] possible quality encodings: [Illumina-1.8+] [INFO] guessed quality encoding: Illumina-1.8+[INFO] converting Illumina-1.8+ -&gt; Illumina-1.5+ #转换 Illumina1.8+ -&gt; Illumina1.5+...B```e``ieeeiii[iiiiieeeieiiiiieii`iiieiieieiiiie`ei[i`[eeiVeiieii``iiii[iiiiiiieiii`iiiiieiiVV[iiiiLeV`Leieeiiiiii[eeiL[VeiiieiiH`VHV``[VLLH[LVeL`Ve`[ grep123456789101112131415$ zcat hairpin.fa.gz | seqkit grep -r -p ^hsa #正则匹配序列名&gt;hsa-let-7a-1 MI0000060 Homo sapiens let-7a-1 stem-loopUGGGAUGAGGUAGUAGGUUGUAUAGUUUUAGGGUCACACCCACCACUGGGAGAUAACUAUACAAUCUACUGUCUUUCCUA&gt;hsa-let-7a-2 MI0000061 Homo sapiens let-7a-2 stem-loopAGGUUGAGGUAGUAGGUUGUAUAGUUUAGAAUUACAUCAAGGGAGAUAACUGUACAGCCUCCUAGCUUUCCU$ zcat hairpin.fa.gz | seqkit grep -r -p ^hsa -p ^mmu -v #2个条件并取反$ zcat hairpin.fa.gz | seqkit grep -f list &gt; new.fa #将需要提取的序列名放在list中$ zcat hairpin.fa.gz | seqkit grep -s -r -i -p ^aggcg #正则匹配序列碱基，-i 忽略大小写$ seqkit grep -s -R 1:30 -i -r -p GCTGG #-R 在前30个碱基中正则匹配 rmdup12345678910$ zcat hairpin.fa.gz | seqkit rmdup -s -o clean.fa.gz #去除重复的序列[INFO] 2226 duplicated records removed$ zcat hairpin.fa.gz | seqkit rmdup -s -i -m -o clean.fa.gz -d duplicated.fa.gz -D duplicated.detail.txt #-d输出重复序列，-D统计重复序列$ cat duplicated.detail.txt # here is not the entire list3 hsa-mir-424, mml-mir-424, ppy-mir-4243 hsa-mir-342, mml-mir-342, ppy-mir-3422 ngi-mir-932, nlo-mir-9322 ssc-mir-9784-1, ssc-mir-9784-2 common123$ seqkit common file*.fa -o common.fasta #通过ID寻找共同序列$ seqkit common file*.fa -n -o common.fasta #通过全名$ seqkit common file*.fa -s -i -o common.fasta #通过序列 split1234567891011121314151617181920212223242526272829303132333435363738394041424344$ seqkit split hairpin.fa.gz -s 10000 #按序列数分割文件[INFO] split into 10000 seqs per file[INFO] write 10000 sequences to file: hairpin.fa.part_001.gz[INFO] write 10000 sequences to file: hairpin.fa.part_002.gz[INFO] write 8645 sequences to file: hairpin.fa.part_003.gz $ seqkit split hairpin.fa.gz -p 4 #按文件个数分割文件[INFO] split into 4 parts[INFO] read sequences ...[INFO] read 28645 sequences[INFO] write 7162 sequences to file: hairpin.fa.part_001.gz[INFO] write 7162 sequences to file: hairpin.fa.part_002.gz[INFO] write 7162 sequences to file: hairpin.fa.part_003.gz[INFO] write 7159 sequences to file: hairpin.fa.part_004.gz$ seqkit split hairpin.fa.gz -p 4 -2 #-2减少内存使用[INFO] split into 4 parts[INFO] read and write sequences to tempory file: hairpin.fa.gz.fa ...[INFO] create and read FASTA index ...[INFO] read sequence IDs from FASTA index ...[INFO] 28645 sequences loaded[INFO] write 7162 sequences to file: hairpin.part_001.fa.gz[INFO] write 7162 sequences to file: hairpin.part_002.fa.gz[INFO] write 7162 sequences to file: hairpin.part_003.fa.gz[INFO] write 7159 sequences to file: hairpin.part_004.fa.gz$ seqkit split hairpin.fa.gz -i --id-regexp "^([\w]+)\-" -2 #按ID[INFO] split by ID. idRegexp: ^([\w]+)\-[INFO] read and write sequences to tempory file: hairpin.fa.gz.fa ...[INFO] create and read FASTA index ...[INFO] create FASTA index for hairpin.fa.gz.fa[INFO] read sequence IDs from FASTA index ...[INFO] 28645 sequences loaded[INFO] write 48 sequences to file: hairpin.id_cca.fa.gz[INFO] write 3 sequences to file: hairpin.id_hci.fa.gz$ seqkit split hairpin.fa.gz -r 1:3 -2 #按前3个碱基[INFO] split by region: 1:3[INFO] read and write sequences to tempory file: hairpin.fa.gz.fa ...[INFO] read sequence IDs and sequence region from FASTA file ...[INFO] create and read FASTA index ...[INFO] write 463 sequences to file: hairpin.region_1:3_AUG.fa.gz[INFO] write 349 sequences to file: hairpin.region_1:3_ACU.fa.gz[INFO] write 311 sequences to file: hairpin.region_1:3_CGG.fa.gz range1$ cat hairpin.fa | seqkit range -r 101:150 #输出范围内的序列（1:12 如同 head -n 12） sort123456789101112131415161718192021222324$ echo -e "&gt;seq1\nACGTNcccc\n&gt;SEQ2\nacgtnAAAA" | seqkit sort --quiet #按ID排序，--quiet不输出提示信息&gt;SEQ2acgtnAAAA&gt;seq1ACGTNcccc$ echo -e "&gt;seq1\nACGTNcccc\n&gt;SEQ2\nacgtnAAAA" | seqkit sort --quiet -i #不区分大小写&gt;seq1ACGTNcccc&gt;SEQ2acgtnAAAA$ echo -e "&gt;seq1\nACGTNcccc\n&gt;SEQ2\nacgtnAAAA" | seqkit sort --quiet -i -r #不区分大小写，反转结果&gt;SEQ2acgtnAAAA&gt;seq1ACGTNcccc$ echo -e "&gt;seq1\nACGTNcccc\n&gt;SEQ2\nacgtnAAAAnnn\n&gt;seq3\nacgt" | seqkit sort --quiet -l #按序列长度&gt;seq3acgt&gt;seq1ACGTNcccc&gt;SEQ2acgtnAAAAnnn translate12345678910111213141516171819$ seqkit translate tests/mouse-p53-cds.fna #将DNA/RNA 翻译为蛋白序列&gt;lcl|AB021961.1_cds_BAA82344.1_1 [gene=p53] [protein=P53] [protein_id=BAA82344.1] [location=101..1273] [gbkey=CDS]MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEALRVSGAPAAQDPVTETPGPVAPAPATPWPLSSFVPSQKTYQGNYGFHLGFLQSGTAKSVMCTYSPPLNKLFCQLAKTCPVQLWVSATPPAGSRVRAMAIYKKSQHMTEVVRRCPHHERCSDGDGLAPPQHRIRVEGNLYPEYLEDRQTFRHSVVVPYEPPEAGSEYTTIHYKYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRDSFEVRVCACPGRDRRTEEENFRKKEVLCPELPPGSAKRALPTCTSASPPQKKKPLDGEYFTLKIRGRKRFEMFRELNEALELKDAHATEESGDSRAHSSYLKTKKGQSTSRHKKTMVKKVGPDSD*$ seqkit translate tests/mouse-p53-cds.fna --trim #去掉 */X（终止密码子）&gt;lcl|AB021961.1_cds_BAA82344.1_1 [gene=p53] [protein=P53] [protein_id=BAA82344.1] [location=101..1273] [gbkey=CDS]MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEALRVSGAPAAQDPVTETPGPVAPAPATPWPLSSFVPSQKTYQGNYGFHLGFLQSGTAKSVMCTYSPPLNKLFCQLAKTCPVQLWVSATPPAGSRVRAMAIYKKSQHMTEVVRRCPHHERCSDGDGLAPPQHRIRVEGNLYPEYLEDRQTFRHSVVVPYEPPEAGSEYTTIHYKYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRDSFEVRVCACPGRDRRTEEENFRKKEVLCPELPPGSAKRALPTCTSASPPQKKKPLDGEYFTLKIRGRKRFEMFRELNEALELKDAHATEESGDSRAHSSYLKTKKGQSTSRHKKTMVKKVGPDSD]]></content>
      <categories>
        <category>bioinfo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python标准库学习（一）]]></title>
    <url>%2F2018%2F09%2F11%2Fpython-standard-library-1%2F</url>
    <content type="text"><![CDATA[collections该模块实现专门的容器数据类型，提供Python通用内置容器（dict，set，list，tuple）的替代方案。 nametuple() 工厂函数用于创建带有命名字段的元组子类 deque() 列表式容器，在两端有快速附加和弹出 ChainMap() 类似于类的类，用于创建多个映射的单个视图 Counter () dict子类用于计算hashable对象 OrderedDict () dict子类，记住已添加的条目的顺序 defaultdict () dict子类调用一个工厂函数时提供缺失值 UserDict () 包装字典对象，更容易dict子类化 UserList () 包装列表对象以方便列表子类化 UserString() 包装字符串对象，更容易字符串子类化 ChainMapChainMap用来将多个dict(字典)组成一个list(只是比喻)，可以理解成合并多个字典，但和update不同，而且效率更高。 ChainMap用来将多个dict组成一个list之后，多个dict之间的键不冲突。当多个dict有重复键时，使用get方法：如chainMap.get(&#39;name&#39;)将会返回第一个dict[‘name’]。 123456789101112131415161718192021222324252627282930313233343536# 新建ChainMap及它的结构In[2]: from collections import ChainMapIn[3]: m1 = &#123;'color': 'red', 'user': 'guest'&#125;In[4]: m2 = &#123;'name': 'drfish', 'age': '18'&#125;In[5]: chainMap = ChainMap(m1, m2)In[6]: print(chainMap.items())ItemsView(ChainMap(&#123;'color': 'red', 'user': 'guest'&#125;, &#123;'name': 'drfish', 'age': '18'&#125;))# 获取ChainMap中的元素In[7]: print(chainMap.get('name'))drfishIn[8]: print(chainMap.get('not'))None# 新增mapIn[9]: m3 = &#123;'name': 'boy'&#125;In[10]: chainMap = chainMap.new_child(m3)In[11]: print(chainMap.items())ItemsView(ChainMap(&#123;'name': 'boy'&#125;, &#123;'color': 'red', 'user': 'guest'&#125;, &#123;'name': 'drfish', 'age': '18'&#125;))In[12]: print(chainMap.get('name'))'boy'#一次遍历多个字典In[13]: for i in chainMap.maps: print("----------------") for mykey in i.keys(): print(mykey,i[mykey]) ----------------name boy----------------color reduser guest----------------name drfishage 18 CounterCounter是一个简单的计数器，使用起来非常方便。 123456789101112131415161718192021&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2) #不大于0的全部忽略&gt;&gt;&gt; sorted(c.elements())['a', 'a', 'a', 'a', 'b', 'b']&gt;&gt;&gt; Counter('abracadabra').most_common(3) #返回出现次数最多的字符[('a', 5), ('r', 2), ('b', 2)]&gt;&gt;&gt; cnt = Counter() # 统计列表中元素出现的个数&gt;&gt;&gt; for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:... cnt[word] += 1...&gt;&gt;&gt; cntCounter(&#123;'blue': 3, 'red': 2, 'green': 1&#125;) &gt;&gt;&gt; cnt = Counter() # 统计字符串中元素出现的个数&gt;&gt;&gt; for ch in 'hello':... cnt[ch] = cnt[ch] + 1...&gt;&gt;&gt; cntCounter(&#123;'l': 2, 'o': 1, 'h': 1, 'e': 1&#125;) dequedeque就是一个双端队列，与list非常相似，不过可以同时在list的左边增删元素，支持线程安全 ，从队列两端添加或弹出元素的复杂度都是O(1)，而从列表的头部插入或移除元素时，列表的复杂度为O(N)。 deque和list从中部处理数据复杂度都为为O(N)。 123456789101112In[72]: dq = deque('abc')In[73]: dqOut[73]: deque(['a', 'b', 'c'])In[74]: dq.append('A')In[75]: dq.appendleft('Z')In[76]: dqOut[76]: deque(['Z', 'a', 'b', 'd', 'A'])In[77]: dq.popleft()Out[77]: 'Z'In[78]: dq.extendleft("hello")In[79]: dqOut[79]: deque(['o', 'l', 'l', 'e', 'h', 'a', 'b', 'c', 'A']) defaultdict使用 list 作为 default_factory，很容易将键值对的序列分组到列表的字典,类似setdefault 当第一次遇到每个键时，它不在映射中；因此使用返回空 list 的 default_factory 函数自动创建一个条目。然后，list.append() 操作将值附加到新列表。当再次遇到键时，查找继续正常（返回该键的列表），list.append() 操作将另一个值添加到列表。这种技术比使用 dict.setdefault() 的等效技术更简单和更快 12345678&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]&gt;&gt;&gt; d = defaultdict(list)&gt;&gt;&gt; for k, v in s:... d[k].append(v)...&gt;&gt;&gt; sorted(d.items())[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])] 1234567&gt;&gt;&gt; d = &#123;&#125;&gt;&gt;&gt; s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]&gt;&gt;&gt; for k, v in s:... d.setdefault(k,[]).append(v)...&gt;&gt;&gt; sorted(d.items())[('blue', [2, 4]), ('red', [1]), ('yellow', [1, 3])] 将 default_factory 设置为int 使 defaultdict 可用于计数,当首次遇到字母时，映射中缺少字母，因此 default_factory 函数调用 int() 以提供默认计数为零。增量操作然后建立每个字母的计数。 1234567&gt;&gt;&gt; s = 'mississippi'&gt;&gt;&gt; d = defaultdict(int)&gt;&gt;&gt; for k in s:... d[k] += 1...&gt;&gt;&gt; sorted(d.items())[('i', 4), ('m', 1), ('p', 2), ('s', 4)] 将 default_factory 设置为 set使得 defaultdict 可用于构建集合的字典 1234567&gt;&gt;&gt; s = [('red', 1), ('blue', 2), ('red', 3), ('blue', 4), ('red', 1), ('blue', 4)]&gt;&gt;&gt; d = defaultdict(set)&gt;&gt;&gt; for k, v in s:... d[k].add(v)...&gt;&gt;&gt; sorted(d.items())[('blue', &#123;2, 4&#125;), ('red', &#123;1, 3&#125;)] OrderedDict有序字典与常规字典类似，但它们记住项目插入的顺序。在对有序字典进行迭代时，项目按它们的键首次添加的顺序返回。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python-underscore]]></title>
    <url>%2F2018%2F09%2F10%2Fpython-underscore%2F</url>
    <content type="text"><![CDATA[_name以单下划线开头，表示这是一个保护成员，只有类对象和子类对象自己能访问到这些变量。以单下划线开头的变量和函数被默认当作是内部函数，使用from module improt *时不会被获取，但是使用import module可以获取 name_以单下划线结尾仅仅是为了区别该名称与关键词 __name双下划线开头，表示为私有成员，只允许类本身访问，子类也不行。在文本上被替换为_class__method name双下划线开头，双下划线结尾。一种约定，Python内部的名字，用来区别其他用户自定义的命名,以防冲突。是一些 Python 的“魔术”对象，表示这是一个特殊成员，例如：定义类的时候，若是添加init方法，那么在创建类的实例的时候，实例会自动调用这个方法，一般用来对实例的属性进行初使化，Python不建议将自己命名的方法写为这种形式]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ruby部分代码风格]]></title>
    <url>%2F2018%2F09%2F10%2Fruby-guide-tips%2F</url>
    <content type="text"><![CDATA[每个缩排层级使用两个空格。不要使用制表符. 123456789# 差 - 四个空格def some_method do_somethingend# 好def some_method do_somethingend 范围的字面量语法中，不要添加任何空格。 1234567# 差1 .. 3'a' ... 'z'# 好1..3'a'...'z' 使用 _ 语法改善大数的数值字面量的可读性。 12345# 差 - 有几个零？num = 1000000# 好 - 方便人脑理解num = 1_000_000 使用 def 定义方法时，如果有参数则使用括号，如果无参数则省略括号。 12345678910111213141516171819# 差def some_method() # 省略主体end# 好def some_method # 省略主体end# 差def some_method_with_parameters param1, param2 # 省略主体end# 好def some_method_with_parameters(param1, param2) # 省略主体end 除非必要，否则避免在并行赋值时使用单字符的 _ 变量。优先考虑前缀形式的下划线变量，而不是直接使用 _，因为前者可以提供一定的语义信息。但当赋值语句左侧出现带 * 操作符的变量时，使用 _ 也是可以接受的。 123456789101112131415foo = 'one,two,three,four,five'# 差 _可有可无，且无任何有用信息first, second, _ = foo.split(',')first, _, _ = foo.split(',')first, *_ = foo.split(',')# 好 _可有可无，但提供了额外信息first, _second = foo.split(',')first, _second, = foo.split(',')first, *_ending = foo.split(',')# 好 _占位符，_ 担当最后一个元素*beginning, _ = foo.split(',')*beginning, something, _ = foo.split(',') 永远不要使用 for， 除非你很清楚为什么。大部分情况下，你应该使用迭代器。for 是由 each 实现的，所以你绕弯了。另外，for 没有引入一个新的作用域 (each 有），因此在它内部定义的变量在外部仍是可见的。 123456789101112131415arr = [1, 2, 3]# 差for elem in arr do puts elemend# 注意，elem 可在 for 循环外部被访问elem # =&gt; 3# 好arr.each &#123; |elem| puts elem &#125;# 注意，elem 不可在 each 块外部被访问elem # =&gt; NameError: undefined local variable or method `elem' 倾向使用三元操作符（?:）而不是 if/then/else/end 结构。前者更为常见且简练。 12345# 差result = if some_condition then something else something_else end# 好result = some_condition ? something : something_else 永远不要使用 if x; ...。使用三元操作符来替代。 12345# 差result = if some_condition; something else something_else end# 好result = some_condition ? something : something_else 使用 ! 而不是 not。 12345# 差 - 因为操作符的优先级，这里必须使用括号x = (not something)# 好x = !something 永远不要使用 and 与 or 关键字。使用 &amp;&amp; 与 || 来替代。 12345678910111213141516# 差# 布尔表达式ok = got_needed_arguments and arguments_are_valid# 流程控制document.save or fail(RuntimeError, "Failed to save document!")# 好# 布尔表达式ok = got_needed_arguments &amp;&amp; arguments_are_valid# 流程控制fail(RuntimeError, "Failed to save document!") unless document.save# 流程控制document.save || fail(RuntimeError, "Failed to save document!") 对于单行主体，倾向使用 {...} 而不是 do...end。对于多行主体，避免使用 {...}。对于“流程控制”或“方法定义”（比如 Rakefile、其他 DSL 构成片段），总是使用 do...end。避免在链式方法调用中使用 do...end. 1234567891011121314151617names = %w[Bozhidar Steve Sarah]# 差names.each do |name| puts nameend# 好names.each &#123; |name| puts name &#125;# 差names.select do |name| name.start_with?('S')end.map &#123; |name| name.upcase &#125;# 好names.select &#123; |name| name.start_with?('S') &#125;.map(&amp;:upcase) 某些人可能会争论在多行链式方法调用时使用 {...} 看起来还可以。但他们应该扪心自问——这样的代码真的可读吗？难道不能把区块内容提取出来放到小巧的方法里吗？ #我觉得写成代码块易读。 避免在不需要流程控制的情况下使用 return。 123456789# 差def some_method(some_arr) return some_arr.sizeend# 好def some_method(some_arr) some_arr.sizeend 嗯。。。很魔幻。 通过使用范围或 Comparable#between? 来替代复杂的比较逻辑。 12345678# 差do_something if x &gt;= 1000 &amp;&amp; x &lt;= 2000# 好do_something if (1000..2000).include?(x)# 好do_something if x.between?(1000, 2000) 当创建一组元素为单词（没有空格或特殊字符）的数组时，倾向使用 %w 而不是 []。此规则只适用于数组元素有两个或以上的时候。 12345# 差STATES = ['draft', 'open', 'closed']# 好STATES = %w[draft open closed] 倾向使用符号而不是字符串作为哈希键。 12345# 差hash = &#123; 'one' =&gt; 1, 'two' =&gt; 2, 'three' =&gt; 3 &#125;# 好hash = &#123; 'one': 1, 'two': 2, 'three': 3 &#125; 倾向使用 Hash#each_key 而不是 Hash#keys.each，使用 Hash#each_value 而不是 Hash#values.each。 123456789# 差hash.keys.each &#123; |k| p k &#125;hash.values.each &#123; |v| p v &#125;hash.each &#123; |k, _v| p k &#125;hash.each &#123; |_k, v| p v &#125;# 好hash.each_key &#123; |k| p k &#125;hash.each_value &#123; |v| p v &#125;]]></content>
      <categories>
        <category>ruby</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas学习（二）]]></title>
    <url>%2F2018%2F09%2F06%2Fpandas-2%2F</url>
    <content type="text"><![CDATA[算术运算和数据对齐pandas最重要的一个功能是，它可以对不同索引的对象进行算术运算。在将对象相加时，如果存在不同的索引对，则结果的索引就是该索引对的并集。对于有数据库经验的用户，这就像在索引标签上进行自动外连接。看一个简单的例子： 123456789101112131415161718192021In [150]: s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])In [151]: s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], .....: index=['a', 'c', 'e', 'f', 'g'])In [152]: s1Out[152]: a 7.3c -2.5d 3.4e 1.5dtype: float64In [153]: s2Out[153]: a -2.1c 3.6e -1.5f 4.0g 3.1dtype: float64 将它们相加就会产生： 123456789In [154]: s1 + s2Out[154]: a 5.2c 1.1d NaNe 0.0f NaNg NaNdtype: float64 自动的数据对齐操作在不重叠的索引处引入了NA值。缺失值会在算术运算过程中传播。 对于DataFrame，对齐操作会同时发生在行和列上： 1234567891011121314151617181920In [155]: df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), .....: index=['Ohio', 'Texas', 'Colorado'])In [156]: df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), .....: index=['Utah', 'Ohio', 'Texas', 'Oregon'])In [157]: df1Out[157]: b c dOhio 0.0 1.0 2.0Texas 3.0 4.0 5.0Colorado 6.0 7.0 8.0In [158]: df2Out[158]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0 把它们相加后将会返回一个新的DataFrame，其索引和列为原来那两个DataFrame的并集： 12345678In [159]: df1 + df2Out[159]: b c d eColorado NaN NaN NaN NaNOhio 3.0 NaN 6.0 NaNOregon NaN NaN NaN NaNTexas 9.0 NaN 12.0 NaNUtah NaN NaN NaN NaN 因为’c’和’e’列均不在两个DataFrame对象中，在结果中以缺省值呈现。行也是同样。 在算术方法中填充值在对不同索引的对象进行算术运算时，你可能希望当一个对象中某个轴标签在另一个对象中找不到时填充一个特殊值（比如0）： 12345678910111213141516171819202122In [165]: df1 = pd.DataFrame(np.arange(12.).reshape((3, 4)), .....: columns=list('abcd'))In [166]: df2 = pd.DataFrame(np.arange(20.).reshape((4, 5)), .....: columns=list('abcde'))In [167]: df2.loc[1, 'b'] = np.nanIn [168]: df1Out[168]: a b c d0 0.0 1.0 2.0 3.01 4.0 5.0 6.0 7.02 8.0 9.0 10.0 11.0In [169]: df2Out[169]: a b c d e0 0.0 1.0 2.0 3.0 4.01 5.0 NaN 7.0 8.0 9.02 10.0 11.0 12.0 13.0 14.03 15.0 16.0 17.0 18.0 19.0 将它们相加时，没有重叠的位置就会产生NA值： 1234567In [170]: df1 + df2Out[170]: a b c d e0 0.0 2.0 4.0 6.0 NaN1 9.0 NaN 13.0 15.0 NaN2 18.0 20.0 22.0 24.0 NaN3 NaN NaN NaN NaN NaN 使用df1的add方法，传入df2以及一个fill_value参数： 1234567In [171]: df1.add(df2, fill_value=0)Out[171]: a b c d e0 0.0 2.0 4.0 6.0 4.01 9.0 5.0 13.0 15.0 9.02 18.0 20.0 22.0 24.0 14.03 15.0 16.0 17.0 18.0 19.0 与此类似，在对Series或DataFrame重新索引时，也可以指定一个填充值： 123456In [174]: df1.reindex(columns=df2.columns, fill_value=0)Out[174]: a b c d e0 0.0 1.0 2.0 3.0 01 4.0 5.0 6.0 7.0 02 8.0 9.0 10.0 11.0 0 DataFrame和Series之间的运算跟不同维度的NumPy数组一样，DataFrame和Series之间算术运算也是有明确规定的。先来看一个具有启发性的例子，计算一个二维数组与其某行之间的差： 12345678910111213141516In [175]: arr = np.arange(12.).reshape((3, 4))In [176]: arrOut[176]: array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])In [177]: arr[0]Out[177]: array([ 0., 1., 2., 3.])In [178]: arr - arr[0]Out[178]: array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) 当我们从arr减去arr[0]，每一行都会执行这个操作。这就叫做广播（broadcasting），附录A将对此进行详细讲解。DataFrame和Series之间的运算差不多也是如此： 1234567891011121314151617181920In [179]: frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), .....: columns=list('bde'), .....: index=['Utah', 'Ohio', 'Texas', 'Oregon'])In [180]: series = frame.iloc[0]In [181]: frameOut[181]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0In [182]: seriesOut[182]: b 0.0d 1.0e 2.0Name: Utah, dtype: float64 默认情况下，DataFrame和Series之间的算术运算会将Series的索引匹配到DataFrame的列，然后沿着行一直向下广播： 1234567In [183]: frame - seriesOut[183]: b d eUtah 0.0 0.0 0.0Ohio 3.0 3.0 3.0Texas 6.0 6.0 6.0Oregon 9.0 9.0 9.0 如果某个索引值在DataFrame的列或Series的索引中找不到，则参与运算的两个对象就会被重新索引以形成并集： 123456789In [184]: series2 = pd.Series(range(3), index=['b', 'e', 'f'])In [185]: frame + series2Out[185]: b d e fUtah 0.0 NaN 3.0 NaNOhio 3.0 NaN 6.0 NaNTexas 6.0 NaN 9.0 NaNOregon 9.0 NaN 12.0 NaN 如果你希望匹配行且在列上广播，则必须使用算术运算方法。例如： 12345678910111213141516171819202122232425In [186]: series3 = frame['d']In [187]: frameOut[187]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0In [188]: series3Out[188]: Utah 1.0Ohio 4.0Texas 7.0Oregon 10.0Name: d, dtype: float64In [189]: frame.sub(series3, axis='index')Out[189]: b d eUtah -1.0 0.0 1.0Ohio -1.0 0.0 1.0Texas -1.0 0.0 1.0Oregon -1.0 0.0 1.0 传入的轴号就是希望匹配的轴。在本例中，我们的目的是匹配DataFrame的行索引（axis=’index’ or axis=0）并进行广播。 函数应用和映射NumPy的ufuncs（元素级数组方法）也可用于操作pandas对象： 123456789101112131415161718In [190]: frame = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'), .....: index=['Utah', 'Ohio', 'Texas', 'Oregon'])In [191]: frameOut[191]: b d eUtah -0.204708 0.478943 -0.519439Ohio -0.555730 1.965781 1.393406Texas 0.092908 0.281746 0.769023Oregon 1.246435 1.007189 -1.296221In [192]: np.abs(frame)Out[192]: b d eUtah 0.204708 0.478943 0.519439Ohio 0.555730 1.965781 1.393406Texas 0.092908 0.281746 0.769023Oregon 1.246435 1.007189 1.296221 另一个常见的操作是，将函数应用到由各列或行所形成的一维数组上。DataFrame的apply方法即可实现此功能： 12345678In [193]: f = lambda x: x.max() - x.min()In [194]: frame.apply(f)Out[194]: b 1.802165d 1.684034e 2.689627dtype: float64 这里的函数f，计算了一个Series的最大值和最小值的差，在frame的每列都执行了一次。结果是一个Series，使用frame的列作为索引。 如果传递axis=’columns’到apply，这个函数会在每行执行： 1234567In [195]: frame.apply(f, axis='columns')Out[195]:Utah 0.998382Ohio 2.521511Texas 0.676115Oregon 2.542656dtype: float64 许多最为常见的数组统计功能都被实现成DataFrame的方法（如sum和mean），因此无需使用apply方法。 传递到apply的函数不是必须返回一个标量，还可以返回由多个值组成的Series： 12345678In [196]: def f(x): .....: return pd.Series([x.min(), x.max()], index=['min', 'max'])In [197]: frame.apply(f)Out[197]: b d emin -0.555730 0.281746 -1.296221max 1.246435 1.965781 1.393406 汇总和计算描述统计pandas对象拥有一组常用的数学和统计方法。它们大部分都属于约简和汇总统计，用于从Series中提取单个值（如sum或mean）或从DataFrame的行或列中提取一个Series。跟对应的NumPy数组方法相比，它们都是基于没有缺失数据的假设而构建的。看一个简单的DataFrame： 123456789101112In [230]: df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5], .....: [np.nan, np.nan], [0.75, -1.3]], .....: index=['a', 'b', 'c', 'd'], .....: columns=['one', 'two'])In [231]: dfOut[231]: one twoa 1.40 NaNb 7.10 -4.5c NaN NaNd 0.75 -1.3 调用DataFrame的sum方法将会返回一个含有列的和的Series： 12345In [232]: df.sum()Out[232]: one 9.25two -5.80dtype: float64 传入axis=’columns’或axis=1将会按行进行求和运算： 123456In [233]: df.sum(axis=1)Out[233]:a 1.40b 2.60c NaNd -0.55 NA值会自动被排除，除非整个切片（这里指的是行或列）都是NA。通过skipna选项可以禁用该功能： 1234567In [234]: df.mean(axis='columns', skipna=False)Out[234]: a NaNb 1.300c NaNd -0.275dtype: float64]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas学习（一）]]></title>
    <url>%2F2018%2F09%2F05%2Fpandas-1%2F</url>
    <content type="text"><![CDATA[导入 pandas、numpy、matplotlib 12345In [1]: import pandas as pdIn [2]: import numpy as npIn [3]: import matplotlib.pyplot as plt 创造对象Series 是一个值的序列，它只有一个列，以及索引。下面的例子中，就用默认的整数索引 1234567891011In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]: 0 11 32 53 NaN4 65 8dtype: float64 DataFrame 是有多个列的数据表，每个列拥有一个 label，当然，DataFrame 也有索引 1234567891011121314151617181920In [6]: dates = pd.date_range('20130101', periods=6)In [7]: datesOut[7]: DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')In [8]: df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))#index 行名，columns，列名。都需要提供一个列表对象In [9]: dfOut[9]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 如果参数是一个 dict，每个 dict 的 value 会被转化成一个 Series 123456789101112131415In [10]: df2 = pd.DataFrame(&#123; 'A' : 1., ....: 'B' : pd.Timestamp('20130102'), ....: 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), ....: 'D' : np.array([3] * 4,dtype='int32'), ....: 'E' : pd.Categorical(["test","train","test","train"]), ....: 'F' : 'foo' &#125;) ....: In [11]: df2Out[11]: A B C D E F0 1 2013-01-02 1 3 test foo1 1 2013-01-02 1 3 train foo2 1 2013-01-02 1 3 test foo3 1 2013-01-02 1 3 train foo 每列的格式用 dtypes 查看 123456789In [12]: df2.dtypesOut[12]: A float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 你可以认为，DataFrame 是由 Series 组成的 1234567In [13]: df2.AOut[13]: 0 11 12 13 1Name: A, dtype: float64 查看数据用 head 和 tail 查看顶端和底端的几列 123456789101112131415In [14]: df.head()Out[14]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.087401In [15]: df.tail(3)Out[15]: A B C D2013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 实际上，DataFrame 内部用 numpy 格式存储数据。你也可以单独查看 index 和 columns 1234567891011121314151617In [16]: df.indexOut[16]: DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')In [17]: df.columnsOut[17]: Index(['A', 'B', 'C', 'D'], dtype='object')In [18]: df.valuesOut[18]: array([[ 0.4691, -0.2829, -1.5091, -1.1356], [ 1.2121, -0.1732, 0.1192, -1.0442], [-0.8618, -2.1046, -0.4949, 1.0718], [ 0.7216, -0.7068, -1.0396, 0.2719], [-0.425 , 0.567 , 0.2762, -1.0874], [-0.6737, 0.1136, -1.4784, 0.525 ]]) describe() 显示数据的概要。 1234567891011In [19]: df.describe()Out[19]: A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.073711 -0.431125 -0.687758 -0.233103std 0.843157 0.922818 0.779887 0.973118min -0.861849 -2.104569 -1.509059 -1.13563225% -0.611510 -0.600794 -1.368714 -1.07661050% 0.022070 -0.228039 -0.767252 -0.38618875% 0.658444 0.041933 -0.034326 0.461706max 1.212112 0.567020 0.276232 1.071804 和 numpy 一样，可以方便的得到转置 1234567In [20]: df.TOut[20]: 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 0.469112 1.212112 -0.861849 0.721555 -0.424972 -0.673690B -0.282863 -0.173215 -2.104569 -0.706771 0.567020 0.113648C -1.509059 0.119209 -0.494929 -1.039575 0.276232 -1.478427D -1.135632 -1.044236 1.071804 0.271860 -1.087401 0.524988 对 axis 按照 index 排序（axis=1 是指第二个维度，即：列，axis=0 是指第一个维度，即：行） 123456789In [21]: df.sort_index(axis=1, ascending=False) #ascending=False 默认为True，升序Out[21]: D C B A2013-01-01 -1.135632 -1.509059 -0.282863 0.4691122013-01-02 -1.044236 0.119209 -0.173215 1.2121122013-01-03 1.071804 -0.494929 -2.104569 -0.8618492013-01-04 0.271860 -1.039575 -0.706771 0.7215552013-01-05 -1.087401 0.276232 0.567020 -0.4249722013-01-06 0.524988 -1.478427 0.113648 -0.673690 按值排序 123456789In [22]: df.sort_values(by='B') #也可以是by=['A','B']，按2列排序Out[22]: A B C D2013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-06 -0.673690 0.113648 -1.478427 0.5249882013-01-05 -0.424972 0.567020 0.276232 -1.087401 选择 注意，以下这些对交互式环境很友好，但是作为 production code 请用优化过的 .at, .iat, .loc, .iloc 和 .ix 获取行/列从 DataFrame 选择一个列，就得到了 Series 123456789In [23]: df['A']Out[23]: 2013-01-01 0.4691122013-01-02 1.2121122013-01-03 -0.8618492013-01-04 0.7215552013-01-05 -0.4249722013-01-06 -0.673690Freq: D, Name: A, dtype: float64 和 numpy 类似，这里也能用 []选择行 12345678910111213In [24]: df[0:3]Out[24]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.071804In [25]: df['20130102':'20130104']Out[25]: A B C D2013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.271860 通过 label 选择还可以多选 123456789In [27]: df.loc[:,['A','B']] #所有行，AB列Out[27]: A B2013-01-01 0.469112 -0.2828632013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.5670202013-01-06 -0.673690 0.113648 注意那个冒号，用法和 MATLAB 或 NumPy 是一样的！所以也可以这样 123456In [28]: df.loc['20130102':'20130104',['A','B']] Out[28]: A B2013-01-02 1.212112 -0.1732152013-01-03 -0.861849 -2.1045692013-01-04 0.721555 -0.706771 12345In [29]: df.loc['20130102',['A','B']]Out[29]: A 1.212112B -0.173215Name: 2013-01-02 00:00:00, dtype: float64 如果对所有的维度都写了标量，不就是选出一个元素吗？ 如果对所有的维度都写了标量，不就是选出一个元素吗？ 12In [30]: df.loc[dates[0],'A']Out[30]: 0.46911229990718628 这种情况通常用 at ，速度更快 12In [31]: df.at[dates[0],'A']Out[31]: 0.46911229990718628 通过整数下标选择这个就和数组类似啦，直接看例子。选出第3行：(注意有第0行) 1234567In [32]: df.iloc[3]Out[32]: A 0.721555B -0.706771C -1.039575D 0.271860Name: 2013-01-04 00:00:00, dtype: float64 选出3~4行，0~1列： 12345In [33]: df.iloc[3:5,0:2] #注意 3:5 是第3行到第4行，尾数在python中是不选择的（有第0行）Out[33]: A B2013-01-04 0.721555 -0.7067712013-01-05 -0.424972 0.567020 也能用 list 选择 123456In [34]: df.iloc[[1,2,4],[0,2]]Out[34]: A C2013-01-02 1.212112 0.1192092013-01-03 -0.861849 -0.4949292013-01-05 -0.424972 0.276232 对应单个元素 1234In [37]: df.iloc[1,1]Out[37]: -0.17321464905330858In [38]: df.iat[1,1]Out[38]: -0.17321464905330858 总结：df.icol 是按下标选择，df.col是按标签选择。 通过布尔值下标基本用法 123456In [39]: df[df.A &gt; 0]Out[39]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-04 0.721555 -0.706771 -1.039575 0.271860 多条件 12345In [39]: df[(df.A &gt; 0)&amp;(df.A &lt;1)]Out[39]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-04 0.721555 -0.706771 -1.039575 0.271860 不满足条件的填充为： NaN 123456789In [40]: df[df &gt; 0]Out[40]: A B C D2013-01-01 0.469112 NaN NaN NaN2013-01-02 1.212112 NaN 0.119209 NaN2013-01-03 NaN NaN NaN 1.0718042013-01-04 0.721555 NaN NaN 0.2718602013-01-05 NaN 0.567020 0.276232 NaN2013-01-06 NaN 0.113648 NaN 0.524988 isin() 函数：是否在集合中 12345678910111213141516171819In [41]: df2 = df.copy()In [42]: df2['E'] = ['one', 'one','two','three','four','three']In [43]: df2Out[43]: A B C D E2013-01-01 0.469112 -0.282863 -1.509059 -1.135632 one2013-01-02 1.212112 -0.173215 0.119209 -1.044236 one2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-04 0.721555 -0.706771 -1.039575 0.271860 three2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four2013-01-06 -0.673690 0.113648 -1.478427 0.524988 threeIn [44]: df2[df2['E'].isin(['two','four'])]Out[44]: A B C D E2013-01-03 -0.861849 -2.104569 -0.494929 1.071804 two2013-01-05 -0.424972 0.567020 0.276232 -1.087401 four contains()函数：是否包含特定字符 12345678910111213141516In [44]:tipsOut[44]: total_bill tip sex smoker day time size0 16.99 1.01 Female No Sun Dinner 21 10.34 1.66 Male No Sun Dinner 32 21.01 3.50 Male No Sun Dinner 33 23.68 3.31 Male No Sun Dinner 24 24.59 3.61 Female No Sun Dinner 4In [45]: tipsoutput = tips[tips.sex.str.contains('Fe*')]Out[45]: total_bill tip sex smoker day time size0 16.99 1.01 Female No Sun Dinner 24 24.59 3.61 Female No Sun Dinner 4 删除数据丢弃某条轴上的一个或多个项很简单，只要有一个索引数组或列表即可。由于需要执行一些数据整理和集合逻辑，所以drop方法返回的是一个在指定轴上删除了指定值的新对象： 1234567891011In [110]: data = pd.DataFrame(np.arange(16).reshape((4, 4)), .....: index=['Ohio', 'Colorado', 'Utah', 'New York'], .....: columns=['one', 'two', 'three', 'four'])In [111]: dataOut[111]: one two three fourOhio 0 1 2 3Colorado 4 5 6 7Utah 8 9 10 11New York 12 13 14 15 用标签序列调用drop会从行标签（axis 0）删除值： 12345In [112]: data.drop(['Colorado', 'Ohio'])Out[112]: one two three fourUtah 8 9 10 11New York 12 13 14 15 通过传递axis=1或axis=’columns’可以删除列的值： 1234567In [113]: data.drop('two', axis=1)Out[113]: one three fourOhio 0 2 3Colorado 4 6 7Utah 8 10 11New York 12 14 15 读取、写入数据CSV写入 1In [136]: df.to_csv('foo.csv') 读取 12345678In [137]: pd.read_csv('foo.csv')In [137]: pd.read_csv(StringIO(data), names=['foo', 'bar', 'baz'], header=None)# 自定义表头，当name设定的时候，header必须显式None，因为默认为0 foo bar baz0 a b c1 1 2 32 4 5 63 7 8 9 table读取 12data = pd.read_table('example.txt',sep='\t',header=0) # 指定分隔符和表头，默认header=0，把第一行作为表头。分割符默认制表符'/t'，'/s+'，可以匹配任何空格。 excel写入 1In [140]: df.to_excel('foo.xlsx', sheet_name='Sheet1') 读取 12In [141]: pd.read_excel('foo.xlsx', sheet_name='Sheet1', index_col=None, na_values=['NA'])#index_col 行名，na_values 缺失值的形式]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[next主题设置]]></title>
    <url>%2F2018%2F09%2F01%2Fnext-theme%2F</url>
    <content type="text"><![CDATA[主题风格通过修改next主题下的_config.yml的scheme字段，配置不同的风格。 12345# Schemes#scheme: Musescheme: Mist #推荐#scheme: Pisces#scheme: Gemini 菜单通过修改next主题下的_config.yml的menu字段，选定显示的菜单项。 可自己修改字段和目录名，||之后为配套的小图标。我添加了links字段，但是当前语言是简体中文，页面上无法给我翻译出来，去添加language/zh-CN.yml里的对应字段即可。 1234567menu: home: /home/ || home about: /about/ || user #tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive links: /links/ || calendar 头像在主题下的source/images/下替换原有的头像文件avatar.gif，并在_config.yml中查找Sidebar Avatar字段，添加url字段内容： /images/avatar.gif 头像旋转找到位于 source/css/_common/components/sidebar/sidebar-author.syl 模板文件里侧边栏头像的样式 .site-author-image 将内容修改为 123456.site-author-image &#123; border-radius: 50%; -webkit-border-radius: 50%; -moz-border-radius: 50%; transition: 1.4s all;&#125; 然后添加 .site-author-image:hover 样式，由 rotate() 方法实现，旋转 360° 123456.site-author-image:hover &#123; -webkit-transform: rotate(360deg); -moz-transform: rotate(360deg); -ms-transform: rotate(360deg); -transform: rotate(360deg);&#125; 文章代码主题Next主题总共支持5种主题，默认主题是白色的normal。通过修改next主题下的_config.yml的highlight字段，来设置代码主题。 推荐 night 。 标签、分类在存在标签页、分类页的情况下，在写文章的时候，在文章头部添加 tags、categories 字段。 12tags: [npm, hexo, github]categories: 搭建博客 搜索功能安装 hexo-generator-searchdb 1$ npm install hexo-generator-searchdb --save 在站点myBlog/_config.yml中添加search字段，如下 12345search:path: search.xmlfield: postformat: htmllimit: 10000 修改next主题下的_config.yml的Local search字段 1enable: true hexo博客底部页脚找到/themes/next/layout/_partials/footer.swig文件 内容如下 1234567891011121314151617181920&lt;div class="copyright" &gt; &#123;% set current = date(Date.now(), "YYYY") %&#125; © &#123;% if theme.since and theme.since != current %&#125; &#123;&#123; theme.since &#125;&#125; - &#123;% endif %&#125; &lt;span itemprop="copyrightYear"&gt;&#123;&#123; current &#125;&#125;&lt;/span&gt; &lt;span class="with-love"&gt; &lt;i class="fa fa-heart"&gt;&lt;/i&gt; &lt;/span&gt; &lt;span class="author" itemprop="copyrightHolder"&gt;&#123;&#123; config.author &#125;&#125;&lt;/span&gt;&lt;/div&gt;&lt;div class="powered-by"&gt; &#123;&#123; __('footer.powered', '&lt;a class="theme-link" href="https://hexo.io"&gt;Hexo&lt;/a&gt;') &#125;&#125;&lt;/div&gt;&lt;div class="theme-info"&gt; &#123;&#123; __('footer.theme') &#125;&#125; - &lt;a class="theme-link" href="https://github.com/iissnan/hexo-theme-next"&gt; NexT.&#123;&#123; theme.scheme &#125;&#125; &lt;/a&gt;&lt;/div&gt; 删除class 为powered-by的div和theme-info的div。 github标识在网站上选择一个喜欢的标识类型，复制粘贴代码到themes/next/layout/_layout.swig文件中(放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面)，并把href改为个人github地址 。 首页隐藏指定文章有时候我们可能只想在首页显示关于编程之类的内容，而个人日记之类的文章放在其他分类之下而不在首页显示。可以从、分类、标签、归档中查看文章。 自定义front-matter的参数例如，自定义添加一个notshow参数，值为true，用来提供判断 123tags: [npm, hexo, github]categories: 搭建博客notshow: true 修改主题的themes/next/layout/index.swig文件,将 123456789&#123;% block content %&#125; &lt;section id="posts" class="posts-expand"&gt; &#123;% for post in page.posts %&#125; &#123;&#123; post_template.render(post, true) &#125;&#125; &#123;% endfor %&#125; &lt;/section&gt; &#123;% include '_partials/pagination.swig' %&#125;&#123;% endblock % 添加过滤条件 1234567891011&#123;% block content %&#125; &lt;section id="posts" class="posts-expand"&gt; &#123;% for post in page.posts %&#125; &#123;% if post.notshow != true %&#125; &#123;&#123; post_template.render(post, true) &#125;&#125; &#123;% endif %&#125; &#123;% endfor %&#125; &lt;/section&gt; &#123;% include '_partials/pagination.swig' %&#125;&#123;% endblock %&#125;]]></content>
      <categories>
        <category>blog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python-copy]]></title>
    <url>%2F2018%2F08%2F27%2Fpython-copy%2F</url>
    <content type="text"><![CDATA[赋值（assignment）在Python中，用一个变量给另一个变量赋值，其实就是给当前内存中的对象增加一个“标签”而已。 12345&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; print(id(a), id(b), sep='\n') #a和b都只是[1,2,3]的一个标签139701469405552139701469405552 浅拷贝（shallow copy）注意：浅拷贝和深拷贝的不同仅仅是对组合对象来说，所谓的组合对象就是包含了其它对象的对象，如列表，类实例。而对于数字、字符串以及其它“原子”类型，没有拷贝一说，产生的都是原对象的引用。 所谓“浅拷贝”，是指创建一个新的对象，其内容是原对象中元素的引用。（拷贝组合对象，不拷贝子对象） 常见的浅拷贝有：切片操作、工厂函数（如list()，dict()等）、对象的copy()方法、copy模块中的copy函数。 12345678910&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = list(a)&gt;&gt;&gt; print(id(a), id(b)) # a和b身份不同140601785066200 140601784764968&gt;&gt;&gt; for x, y in zip(a, b): # 但它们包含的子对象身份相同... print(id(x), id(y))... 140601911441984 140601911441984140601911442016 140601911442016140601911442048 140601911442048 从上面可以明显的看出来，a 浅拷贝得到 b，a 和 b 指向内存中不同的 list 对象，但它们的元素却指向相同的 int 对象。这就是浅拷贝！ 1234567&gt;&gt;&gt; a = [1, 2, 3，[4,5]]&gt;&gt;&gt; b = list(a)&gt;&gt;&gt; a[-1].append(6)&gt;&gt;&gt; print(a)[1, 2, 3，[4,5,6]]&gt;&gt;&gt; print(b) #当a的子对象发生改变时，b也发生了改变[1, 2, 3，[4,5,6]] 深拷贝（deep copy）所谓“深拷贝”，是指创建一个新的对象，然后递归的拷贝原对象所包含的子对象。深拷贝出来的对象与原对象没有任何关联。 深拷贝只有一种方式：copy模块中的deepcopy函数。 12345678&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [1, 2, 3，[4,5]]&gt;&gt;&gt; b = copy.deepcopy(a)&gt;&gt;&gt; a[-1].append(6)&gt;&gt;&gt; print(a)[1, 2, 3，[4,5,6]]&gt;&gt;&gt; print(b) #当a的子对象发生改变时，b不发生改变[1, 2, 3，[4,5]] 1234567891011&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = copy.deepcopy(a)&gt;&gt;&gt; print(id(a), id(b))140601785065840 140601785066200&gt;&gt;&gt; for x, y in zip(a, b):... print(id(x), id(y))... 140601911441984 140601911441984140601911442016 140601911442016140601911442048 140601911442048 看了上面的例子，有人可能会疑惑： 为什么使用了深拷贝，a和b中元素的id还是一样呢？ 答：这是因为对于不可变对象，当需要一个新的对象时，python可能会返回已经存在的某个类型和值都一致的对象的引用。而且这种机制并不会影响 a 和 b 的相互独立性，因为当两个元素指向同一个不可变对象时，对其中一个赋值不会影响另外一个。 我们可以用一个包含可变对象的列表来确切地展示“浅拷贝”与“深拷贝”的区别： 1234567891011121314151617181920&gt;&gt;&gt; import copy&gt;&gt;&gt; a = [[1, 2],[5, 6], [8, 9]]&gt;&gt;&gt; b = copy.copy(a) # 浅拷贝得到b&gt;&gt;&gt; c = copy.deepcopy(a) # 深拷贝得到c&gt;&gt;&gt; print(id(a), id(b)) # a 和 b 不同139832578518984 139832578335520&gt;&gt;&gt; for x, y in zip(a, b): # a 和 b 的子对象相同... print(id(x), id(y))... 139832578622816 139832578622816139832578622672 139832578622672139832578623104 139832578623104&gt;&gt;&gt; print(id(a), id(c)) # a 和 c 不同139832578518984 139832578622456&gt;&gt;&gt; for x, y in zip(a, c): # a 和 c 的子对象也不同... print(id(x), id(y))... 139832578622816 139832578621520139832578622672 139832578518912139832578623104 139832578623392 总结：1、赋值：简单地拷贝对象的引用，两个对象的id相同。2、浅拷贝：创建一个新的组合对象，这个新对象与原对象共享内存中的子对象。3、深拷贝：创建一个新的组合对象，同时递归地拷贝所有子对象，新的组合对象与原对象没有任何关联。虽然实际上会共享不可变的子对象，但不影响它们的相互独立性。 浅拷贝和深拷贝的不同仅仅是对组合对象来说，所谓的组合对象就是包含了其它对象的对象，如列表，类实例。而对于数字、字符串(如a=1)以及其它“原子”类型，没有拷贝一说，产生的都是原对象的引用。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ruby-re]]></title>
    <url>%2F2018%2F08%2F21%2Fruby-re%2F</url>
    <content type="text"><![CDATA[Ruby中正则表达式的写法主要有三种 在//之间，要进行转义 在%r{}内，不用进行转义 Regexp.new()内，不用进行转义 匹配的两种方法 =~肯定匹配, !~否定匹配。=~表达式返回匹配到的位置索引，失败返回nil，符号左右内容可交换 regexp#match(str)，返回MatchData，一个数组，从0开始，还有match.pre_match返回匹配前内容，match.post_match返回匹配后内容 1234/cat/ =~ "dog and cat" #返回8# 类似python可以将正则放入一个变量，如re = Regexp.new(/cat/)，在后续匹配时直接使用remt = /cat/.match("bigcatcomes") # mt = re.match("bigcatcomes")"#&#123;mt.pre_match&#125;-&gt;#&#123;mt[0]&#125;&lt;-#&#123;mt.post_match&#125;" #返回big-&gt;cat&lt;-comes 替换很多时候匹配是为了替换，Ruby中进行正则替换非常简单，两个方法即可搞定，sub()+gsub()。sub只替换第一次匹配，gsub（g:global）会替换所有的匹配，没有匹配到返回原字符串的copy 123str = "ABDADA"new_str = str.sub(/A/, "a") #返回"aBDADA"new_str2 = str.gsub(/A/, "a") #返回"aBDaDa" 分组匹配Ruby的分组匹配与其它语言差别不大，分组匹配表达式是对要进行分组的内容加()。对于匹配到的结果，可以用系统变量$1，$2…索引，也可用matchData数组来索引 123md = /(\d\d):(\d\d)(..)/.match("12:50am") # md为一个MatchData对象puts "Hour is #&#123;$1&#125;, minute #&#123;$2&#125;"puts "Hour is #&#123;md[1]&#125;, minute #&#123;md[2]&#125;" 匹配所有regexp#match()只能匹配一次，如果想匹配所有要用regexp#scan()用法示例： 1"abcabcqwe".scan(%r&#123;abc&#125;).each &#123;|x| puts x&#125; # 输出2行abc]]></content>
      <categories>
        <category>ruby</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo+github搭建博客]]></title>
    <url>%2F2018%2F08%2F09%2Fhexo%2Bgithub%2F</url>
    <content type="text"><![CDATA[准备node.js和git node.js : 直接官网下载 git : 直接官网下载 验证安装结果 : 1234$ node -v$ npm -v#npm是随同NodeJS一起安装的包管理工具,类似python的pip$ git --version Github账户注册和新建项目项目必须要遵守格式：uername.github.io，如下： 安装Hexo Hexo 是一个快速、简洁且高效的博客框架。 12345$ mkdir blog &amp;&amp; cd - #创建个人目录$ npm install -g hexo-cli #安装hexo$ hexo -v #检查hexo$ hexo init #初始化hexo$ npm install #安装所需包 Hexo与Github page关联 设置Git的user name和email（如果是第一次的话） 12git config --global user.name "xxx" git用户名git config --global user.email "xxx@xx.com" git邮箱 生成密钥、公钥 1234$ ssh-add -D$ rm -r ~/.ssh #删除存在的密钥、公钥$ ssh-keygen -t rsa -C "xxx@xx.com" #生成新密钥、公钥对应git邮箱$ cat ~/.ssh/id_rsa.pub #查看公钥内容 添加公钥到github 登陆github帐户，点击头像，然后 Settings -&gt; 左栏点击 SSH and GPG keys -&gt; 点击 New SSH key。然后复制上面的公钥内容，粘贴进“Key”文本域内。 title随便起个名字，点击 Add key完成。 确认成功 1234$ ssh -T xxx@github.com$ git remote -vorigin https://github.com/someaccount/someproject.git (fetch)origin https://github.com/someaccount/someproject.git (push) git使用https协议，每次pull, push都会提示要输入密码，使用git协议，然后使用ssh密钥，这样免去每次都输密码的麻烦。 SSH地址 HTTPS地址 在_config.yml 进行基础配置回到创建hexo的文件夹找到_config.yml，并编辑最后的信息: 12345deploy： type： git repository ：git@github.com:flystar233/flystar233.github.io.git #发布不再需要密码 repository ：https://github.com/flystar233/flystar233.github.io.git #发布需要密码 branch ：master 让博客能加载图片12post_asset_folder: true #在_config.yml中将false改为true$ npm install hexo-asset-image --save #在命令行中执行 这样之后，在运行hexo n &quot;xxxx&quot;来生成md博客时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹。将博客所需图片放入此文件夹中，在md文件中（博客内容）插入图片时，使用命令 来插入图片，xxxx是文件名，路径不可有中文。 发布博客在生成以及部署文章之前，需要安装一个扩展： 1$ npm install hexo-deployer-git --save 发布相关命令 123456$ hexo clean # 清除全部文章$ hexo generate == hexo g #生成静态文件$ hexo deploy == hexo d #部署文件到github$ hexo new "文件名" #创建新文章$ hexo new page "页面名" #创建新页面$ hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server） 查看博客 部署成功后访问博客地址，如：flystar233.github.io]]></content>
      <categories>
        <category>blog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[awk]]></title>
    <url>%2F2018%2F08%2F08%2Fawk%2F</url>
    <content type="text"><![CDATA[使用awk 取文件第一列是以数字开头的数据： 1$ awk '$1~/^[0-9]+/ &#123;print $0&#125;' infile &gt; outfile 使用awk 取文件第一列包含chr或者包含sca： 1$ awk '$1~/chr|sca/ &#123;print $0&#125;' infile &gt; outfile 使用awk 取文件第一列大于0.1且小于0.5的数据： 1$ awk '$1&gt;0.1 &amp;&amp; $1&lt;0.5 &#123;print $0&#125;' infile &gt; outfile 使用awk 取文件第一列是chr： 12$ awk '$1~/^chr$/ &#123;print $0&#125;' infile &gt; outfile$ awk '$1=="chr" &#123;print $0&#125;' infile &gt; outfile 使用awk 进行字符串捕获： 1234$ cat fileaaaaBASE_DDDD123$ awk '&#123;match($0,/(BASE_[a-zA-Z0-9]+)/,a);print a[1]&#125;' fileBASE_DDDD123 split 函数： 1234$ echo "12:00:00"|awk '&#123;split($0,time,":" ); for (i in time) print time[i]&#125;'# split()是awk内置函数，还有length()# $0 指代前面的字符，time为存放数据的数组名，":" 为分隔符 使用awk 进行格式化输出： 12# %c 字符 %s 字符串 %d 十进制整数 %f 浮点数$ printf ("%s\t%s\t%s",$1,$2,$3)]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[幻城]]></title>
    <url>%2F2018%2F08%2F07%2Fworld%2F</url>
    <content type="text"><![CDATA[游云月半圆望时，秋地枯荑春未迟 身藏幻城鸿去近，城南回水树新枝 远帆排浪遮日月，孤翁楫舟度岁年 一海破残何殆尽，镜中白鬓拆青丝 ---------------------------------------------------------------- **兰花草** --胡适 我从山中来，带着兰花草。种在小园中，希望花开早 一日看三回，看得花时过。兰花却依然，苞也无一个 转眼秋天到，移兰入暖房。朝朝频顾惜，夜夜不相忘 期待春花开，能将夙愿偿。满庭花簇簇，添得许多香]]></content>
      <categories>
        <category>thought</category>
      </categories>
  </entry>
</search>
